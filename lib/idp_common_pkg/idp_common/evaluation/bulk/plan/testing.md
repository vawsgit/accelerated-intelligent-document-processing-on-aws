# Testing & Verification Plan

This document details how to verify the bulk evaluation feature works end-to-end. It is designed for a **human reviewer** approving the implementation PR.

---

## Test Layers

| Layer | What | How | Who |
|-------|------|-----|-----|
| 1. Unit tests | Lambda shim `BulkEvaluationAggregator` logic + parity vs Stickler | `pytest` ‚Äî automated | CI |
| 2. Notebook verification | Stickler `aggregate_from_comparisons()` with real confusion matrices | Jupyter notebook ‚Äî manual | Reviewer |
| 3. Integration test | Full pipeline: upload ‚Üí evaluate ‚Üí aggregate ‚Üí UI | IDP CLI + Test Studio UI ‚Äî manual | Reviewer |

---

## Layer 1: Unit Tests (Automated)

File: `lib/idp_common_pkg/tests/unit/evaluation/test_bulk_aggregator.py`

These run in CI. Tests cover the Lambda shim's `BulkEvaluationAggregator` and include a parity test against Stickler's `aggregate_from_comparisons()`.

### Test Cases

| Test | Input | Expected Output |
|------|-------|-----------------|
| Empty input | `aggregator.update({})` √ó 3, then `compute()` | `document_count: 0`, empty fields |
| Single flat doc | 1 doc with 3 flat fields (invoice_id, date, total) | Field-level P/R/F1 match hand-calculated values |
| Multiple flat docs | 3 docs with same fields, varying TP/FP/FN | Counts sum correctly, derived metrics are micro-averaged |
| Nested fields | Doc with `line_items.description`, `line_items.amount` via `nested_fields` | Dot-path fields appear in output |
| Hierarchical fields | Doc with `address.overall` + `address.fields.street/city` | Both parent and child paths accumulated |
| Mixed docs | 5 docs: 3 with confusion_matrix, 1 empty `{}`, 1 `None` | `document_count: 3`, empty/None skipped |
| Reset | `update()` √ó 3, `reset()`, `update()` √ó 1 | Only last doc counted |
| Zero denominators | All FN, no TP or FP | `precision: 0.0`, `recall: 0.0`, `f1: 0.0` |
| **Parity vs Stickler** | Same input to both Lambda shim and `aggregate_from_comparisons()` | Identical metric values |

### Sample Test Code

> **Note:** Tests import from the Lambda shim's `bulk_aggregator` module. The parity test also imports from Stickler.

```python
import pytest
import sys
import os

# Add Lambda shim to path for testing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../../../nested/appsync/src/lambda/test_results_resolver'))
from bulk_aggregator import BulkEvaluationAggregator


class TestBulkEvaluationAggregator:

    def test_single_document_flat_fields(self):
        agg = BulkEvaluationAggregator()
        agg.update({
            "overall": {"tp": 2, "fp": 1, "fn": 0, "tn": 0},
            "fields": {
                "invoice_id": {"tp": 1, "fp": 0, "fn": 0, "tn": 0},
                "total": {"tp": 1, "fp": 1, "fn": 0, "tn": 0},
            }
        })
        result = agg.compute()
        assert result["document_count"] == 1
        assert result["overall"]["tp"] == 2
        assert result["overall"]["fp"] == 1
        assert result["fields"]["invoice_id"]["precision"] == 1.0
        assert result["fields"]["total"]["precision"] == pytest.approx(0.5)

    def test_multiple_documents_accumulate(self):
        agg = BulkEvaluationAggregator()
        for _ in range(3):
            agg.update({
                "overall": {"tp": 1, "fp": 0, "fn": 1, "tn": 0},
                "fields": {
                    "name": {"tp": 1, "fp": 0, "fn": 0, "tn": 0},
                    "date": {"tp": 0, "fp": 0, "fn": 1, "tn": 0},
                }
            })
        result = agg.compute()
        assert result["document_count"] == 3
        assert result["overall"]["tp"] == 3
        assert result["overall"]["fn"] == 3
        assert result["fields"]["name"]["tp"] == 3
        assert result["fields"]["name"]["f1"] == 1.0
        assert result["fields"]["date"]["fn"] == 3
        assert result["fields"]["date"]["recall"] == 0.0

    def test_nested_fields(self):
        agg = BulkEvaluationAggregator()
        agg.update({
            "overall": {"tp": 3, "fp": 0, "fn": 0, "tn": 0},
            "fields": {
                "line_items": {
                    "overall": {"tp": 3, "fp": 0, "fn": 0, "tn": 0},
                    "nested_fields": {
                        "description": {"tp": 1, "fp": 0, "fn": 0, "tn": 0},
                        "amount": {"tp": 1, "fp": 0, "fn": 0, "tn": 0},
                        "quantity": {"tp": 1, "fp": 0, "fn": 0, "tn": 0},
                    }
                }
            }
        })
        result = agg.compute()
        assert "line_items" in result["fields"]
        assert "line_items.description" in result["fields"]
        assert "line_items.amount" in result["fields"]

    def test_empty_and_none_skipped(self):
        agg = BulkEvaluationAggregator()
        agg.update({"overall": {"tp": 1, "fp": 0, "fn": 0, "tn": 0}, "fields": {}})
        agg.update({})
        agg.update(None)
        assert agg.compute()["document_count"] == 1
```

### Running

```bash
cd lib/idp_common_pkg
pytest tests/unit/evaluation/test_bulk_aggregator.py -v
```

### Sync Check

> **üîÑ Updated 2026-02-12:** With PR #74, the aggregator no longer exists in two places. `idp_common` uses Stickler's `aggregate_from_comparisons()` directly. The standalone shim only exists in the Lambda directory.

Instead of a file diff, verify **parity** between the Lambda shim and Stickler:

```python
# In test_bulk_aggregator.py
def test_lambda_shim_parity_with_stickler():
    """Verify Lambda shim produces identical output to Stickler's aggregate_from_comparisons()."""
    from stickler import aggregate_from_comparisons
    from bulk_aggregator import BulkEvaluationAggregator  # Lambda shim

    # Same input data
    comparison_results = [...]  # synthetic compare_with() results

    # Stickler path
    stickler_result = aggregate_from_comparisons(comparison_results)

    # Lambda shim path
    agg = BulkEvaluationAggregator()
    for r in comparison_results:
        agg.update(r.get("confusion_matrix", {}))
    shim_result = agg.compute()

    # Verify parity
    assert shim_result["overall"]["tp"] == stickler_result.metrics["tp"]
    assert shim_result["overall"]["precision"] == pytest.approx(stickler_result.metrics["cm_precision"])
    # ... etc for all metric keys
```

---

## Layer 2: Notebook Verification (Manual)

File: `notebooks/examples/step7_bulk_evaluation.ipynb`

This notebook lets the reviewer run the aggregator locally against synthetic and/or real evaluation data without needing a deployed stack.

### Prerequisites

```bash
cd lib/idp_common_pkg && pip install -e ".[evaluation]"
```

### Notebook Cells

#### Cell 1: Setup

```python
import json
from stickler import aggregate_from_comparisons
import pandas as pd

print("Stickler aggregate_from_comparisons imported successfully ‚úÖ")
```

#### Cell 2: Synthetic test data (3 invoice documents)

This is the core verification fixture. The reviewer can hand-verify the math.

```python
# 3 synthetic documents with known confusion matrices
synthetic_docs = [
    {
        "doc_id": "invoice_001",
        "confusion_matrix": {
            "overall": {"tp": 4, "fp": 1, "fn": 0, "tn": 0},
            "fields": {
                "invoice_number": {"tp": 1, "fp": 0, "fn": 0, "tn": 0},
                "invoice_date": {"tp": 1, "fp": 0, "fn": 0, "tn": 0},
                "total_amount": {"tp": 1, "fp": 0, "fn": 0, "tn": 0},
                "vendor_name": {"tp": 1, "fp": 1, "fn": 0, "tn": 0},
            }
        }
    },
    {
        "doc_id": "invoice_002",
        "confusion_matrix": {
            "overall": {"tp": 3, "fp": 0, "fn": 1, "tn": 0},
            "fields": {
                "invoice_number": {"tp": 1, "fp": 0, "fn": 0, "tn": 0},
                "invoice_date": {"tp": 1, "fp": 0, "fn": 0, "tn": 0},
                "total_amount": {"tp": 1, "fp": 0, "fn": 0, "tn": 0},
                "vendor_name": {"tp": 0, "fp": 0, "fn": 1, "tn": 0},
            }
        }
    },
    {
        "doc_id": "invoice_003",
        "confusion_matrix": {
            "overall": {"tp": 2, "fp": 1, "fn": 1, "tn": 0},
            "fields": {
                "invoice_number": {"tp": 1, "fp": 0, "fn": 0, "tn": 0},
                "invoice_date": {"tp": 0, "fp": 0, "fn": 1, "tn": 0},
                "total_amount": {"tp": 1, "fp": 0, "fn": 0, "tn": 0},
                "vendor_name": {"tp": 0, "fp": 1, "fn": 0, "tn": 0},
            }
        }
    },
]
print(f"Created {len(synthetic_docs)} synthetic documents")
```

#### Cell 3: Run aggregation

```python
# Build list of compare_with() result dicts (each must have "confusion_matrix" key)
comparison_results = [{"confusion_matrix": doc["confusion_matrix"]} for doc in synthetic_docs]

result = aggregate_from_comparisons(comparison_results)
print(f"Documents aggregated: {result.document_count}")
print(f"Overall: P={result.metrics.get('cm_precision', 0):.3f} R={result.metrics.get('cm_recall', 0):.3f} F1={result.metrics.get('cm_f1', 0):.3f}")
```

#### Cell 4: Display as table (sorted by worst F1)

```python
df = pd.DataFrame([
    {"field": k, **v} for k, v in result.field_metrics.items()
]).sort_values("cm_f1")

# Expected results for hand-verification:
# invoice_number: TP=3, FP=0, FN=0 ‚Üí P=1.000, R=1.000, F1=1.000
# total_amount:   TP=3, FP=0, FN=0 ‚Üí P=1.000, R=1.000, F1=1.000
# invoice_date:   TP=2, FP=0, FN=1 ‚Üí P=1.000, R=0.667, F1=0.800
# vendor_name:    TP=1, FP=2, FN=1 ‚Üí P=0.333, R=0.500, F1=0.400

display(df[["field", "cm_precision", "cm_recall", "cm_f1", "tp", "fp", "fn"]])
```

#### Cell 5: Verify against hand-calculated expected values

```python
# Automated assertions the reviewer can run
fields = result.field_metrics

assert fields["invoice_number"]["tp"] == 3
assert fields["invoice_number"]["cm_f1"] == 1.0

assert fields["vendor_name"]["tp"] == 1
assert fields["vendor_name"]["fp"] == 2
assert fields["vendor_name"]["fn"] == 1
assert abs(fields["vendor_name"]["cm_precision"] - 1/3) < 0.001
assert abs(fields["vendor_name"]["cm_f1"] - 0.4) < 0.001

assert fields["invoice_date"]["fn"] == 1
assert abs(fields["invoice_date"]["cm_recall"] - 2/3) < 0.001

print("All assertions passed ‚úÖ")
```

#### Cell 6 (Optional): Load real eval results from S3

```python
# Only works with a deployed stack
import boto3

STACK_NAME = "IDP"  # ‚Üê reviewer sets this
OUTPUT_BUCKET = "..."  # ‚Üê from CloudFormation outputs
TEST_RUN_PREFIX = "..."  # ‚Üê from a completed test run

s3 = boto3.client("s3")
comparison_results = []

# List eval result files
paginator = s3.get_paginator("list_objects_v2")
for page in paginator.paginate(Bucket=OUTPUT_BUCKET, Prefix=TEST_RUN_PREFIX):
    for obj in page.get("Contents", []):
        if obj["Key"].endswith("/evaluation/results.json"):
            response = s3.get_object(Bucket=OUTPUT_BUCKET, Key=obj["Key"])
            eval_result = json.loads(response["Body"].read())
            # Extract confusion matrix from section metrics
            for section in eval_result.get("section_results", []):
                cm = section.get("metrics", {}).get("confusion_matrix", {})
                if cm:
                    comparison_results.append({"confusion_matrix": cm})

result = aggregate_from_comparisons(comparison_results)
print(f"Aggregated {result.document_count} documents from S3")

df = pd.DataFrame([
    {"field": k, **v} for k, v in result.field_metrics.items()
]).sort_values("cm_f1")
display(df[["field", "cm_precision", "cm_recall", "cm_f1", "tp", "fp", "fn"]])
```

### What the Reviewer Should Verify

| Check | How |
|-------|-----|
| Aggregator imports cleanly | Cell 1 runs without error |
| Counts sum correctly | Cell 4 table matches hand-calculated values in comments |
| Assertions pass | Cell 5 prints "All assertions passed ‚úÖ" |
| Nested fields work | If testing with real data that has list fields, dot-paths appear |
| Empty docs handled | Add `aggregator.update({})` before Cell 3 ‚Äî count should not change |

---

## Layer 3: Integration Test (Manual, Deployed Stack)

This verifies the full pipeline: document upload ‚Üí evaluation ‚Üí bulk aggregation ‚Üí Test Studio UI.

### Prerequisites

- Deployed IDP stack (Pattern 2 recommended ‚Äî has evaluation built in)
- IDP CLI installed: `cd lib/idp_cli_pkg && pip install -e .`
- Bedrock model access enabled (Claude 3.x/4.x, Nova models)
- A test set with baselines configured

### Step 1: Run a test set with evaluation

```bash
idp-cli run-inference \
    --stack-name IDP \
    --test-set fcc-example-test \
    --number-of-files 5 \
    --context "Bulk eval verification" \
    --monitor
```

Wait for all documents to complete (status: COMPLETED).

### Step 2: Verify confusion matrix in eval results JSON

Pick one completed document and check its eval results in S3:

```bash
# Get the output bucket from stack outputs
OUTPUT_BUCKET=$(aws cloudformation describe-stacks \
    --stack-name IDP \
    --query "Stacks[0].Outputs[?OutputKey=='OutputBucketName'].OutputValue" \
    --output text)

# List eval results for the test run
aws s3 ls s3://$OUTPUT_BUCKET/ --recursive | grep "evaluation/results.json" | head -3

# Download one and check for confusion_matrix
aws s3 cp s3://$OUTPUT_BUCKET/<path>/evaluation/results.json /tmp/eval_result.json
python3 -c "
import json
r = json.load(open('/tmp/eval_result.json'))
for s in r.get('section_results', []):
    cm = s.get('metrics', {}).get('confusion_matrix')
    print(f\"Section {s['section_id']}: confusion_matrix={'present' if cm else 'MISSING'}\")
    if cm:
        print(f\"  overall: {cm.get('overall', {})}\")
        print(f\"  fields: {list(cm.get('fields', {}).keys())}\")
"
```

**Expected**: Each section's `metrics` dict contains a `confusion_matrix` key with `overall` and `fields`.

**If missing**: Phase 1 (`include_confusion_matrix=True` in `compare_with()`) was not implemented correctly.

### Step 3: Verify field-level metrics in Test Studio UI

1. Open the Test Studio UI (URL from CloudFormation outputs)
2. Navigate to the test run from Step 1
3. Look for the **"Field-Level Metrics (Aggregated)"** section

**Expected**:
- A table showing each extraction field with P/R/F1/TP/FP/FN columns
- Sorted by F1 ascending (worst fields first)
- Color-coded F1 badges (red < 0.5, yellow 0.5-0.8, green > 0.8)
- An overall summary showing aggregate P/R/F1

**If missing**: Check that `fieldLevelMetrics` is in the GraphQL query response (browser dev tools ‚Üí Network ‚Üí look for `getTestRun` query).

### Step 4: Verify GraphQL response

In browser dev tools (Network tab), find the `getTestRun` GraphQL request and check the response:

```json
{
  "data": {
    "getTestRun": {
      "fieldLevelMetrics": "{\"document_count\":5,\"overall\":{...},\"fields\":{...}}"
    }
  }
}
```

**Expected**: `fieldLevelMetrics` is a non-null JSON string. Parse it and verify `document_count` matches the number of successfully evaluated documents.

### Step 5: Cross-check notebook vs UI

Run the notebook (Layer 2, Cell 6) against the same test run's S3 data. The field-level metrics from the notebook should match what the UI displays.

| Field | Notebook F1 | UI F1 | Match? |
|-------|-------------|-------|--------|
| field_a | ... | ... | ‚úÖ/‚ùå |
| field_b | ... | ... | ‚úÖ/‚ùå |

---

## Test Data Requirements

| Data | Source | Purpose |
|------|--------|---------|
| Synthetic confusion matrices | Hardcoded in unit tests + notebook | Verify aggregation math |
| `samples/lending_package.pdf` | Already in repo | Integration test input (Pattern 2) |
| Test set with baselines | `fcc-example-test` (pre-configured) | End-to-end evaluation pipeline |

No new test data files need to be created. The synthetic fixtures in the unit tests and notebook are sufficient for verifying aggregation logic. Real evaluation data comes from running the existing test set.

---

## Reviewer Checklist

```
‚ñ° Unit tests pass: pytest tests/unit/evaluation/test_bulk_aggregator.py -v
‚ñ° Parity test passes: Lambda shim output matches stickler.aggregate_from_comparisons()
‚ñ° Notebook Cell 5 assertions pass (synthetic data)
‚ñ° Notebook Cell 6 loads real S3 data (if stack available)
‚ñ° Eval results JSON contains confusion_matrix in metrics
‚ñ° Test Studio UI shows Field-Level Metrics section
‚ñ° UI field metrics match notebook output for same test run
‚ñ° GraphQL response includes non-null fieldLevelMetrics
‚ñ° Existing metrics (overallAccuracy, costBreakdown, etc.) still work
‚ñ° Test runs without baselines gracefully show no field-level metrics
‚ñ° stickler-eval version bumped to release containing PR #74
```
