# KISS Alternative: Direct Stickler Import

## Overview

This document evaluates the alternative approach of directly importing Stickler's `BulkStructuredModelEvaluator` class in the `test_results_resolver` Lambda instead of reimplementing the accumulation logic in a custom `BulkEvaluationAggregator` class.

---

## Key Finding: Stickler Is Already an IDP Dependency

Stickler (`stickler-eval==0.1.4`) is **already a core dependency** of the IDP Accelerator:

| Where | How |
|-------|-----|
| `pyproject.toml` ‚Üí `[evaluation]` extra | `stickler-eval==0.1.4` |
| `pyproject.toml` ‚Üí `[test]` extra | `stickler-eval==0.1.4` |
| `pyproject.toml` ‚Üí `[all]` extra | `stickler-eval==0.1.4` |
| `evaluation/service.py` | `from stickler import StructuredModel` |
| `evaluation/llm_comparator.py` | `from stickler.structured_object_evaluator.models.comparator_registry import ...` |
| `evaluation/stickler_mapper.py` | Maps IDP config ‚Üí Stickler config format |
| `evaluation/stickler_version.py` | Tracks Stickler version/commit |
| Evaluation Lambda (all patterns) | Docker image with `idp_common[evaluation,docs_service]` |

The class we want already exists at: `stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator`

---

## The Packaging Problem

The `test_results_resolver` Lambda is where bulk aggregation needs to run. Here's its current packaging:

```
TestResultsResolverFunction:
  PackageType: Zip              # ‚Üê NOT Docker
  CodeUri: ./src/lambda/test_results_resolver
  Layers: (none)                # ‚Üê NO Lambda layers
  Runtime: python3.12
  MemorySize: 512
  Timeout: 300
```

It's a **bare Zip Lambda** with a single `index.py` file and **no dependencies** ‚Äî it only uses `boto3` (provided by the Lambda runtime). It has no access to `idp_common` or `stickler`.

Compare with the Evaluation Lambda (where Stickler currently runs):

```
EvaluationFunction:
  PackageType: Image            # ‚Üê Docker image
  ImageUri: ...evaluation-function-${ImageVersion}
  requirements.txt: ../../lib/idp_common_pkg[evaluation,docs_service]
  MemorySize: 1024
  Timeout: 900
```

---

## What It Would Take to Import Stickler Directly

### Option A: Add `IDPCommonBaseLayer` + evaluation deps to test_results_resolver

```yaml
# nested/appsync/template.yaml
TestResultsResolverFunction:
  Properties:
    Layers:
      - !Ref IDPCommonBaseLayerArn    # ‚Üê Add base layer
    # But base layer doesn't include evaluation deps...
```

The existing Lambda layers are:

| Layer | Contents | Includes Stickler? |
|-------|----------|-------------------|
| `IDPCommonBaseLayer` | core + docs_service + image | ‚ùå No |
| `IDPCommonReportingLayer` | core + reporting | ‚ùå No |
| `IDPCommonAgentsLayer` | core + agents | ‚ùå No |

**None of the existing layers include Stickler.** We'd need to either:

1. Create a **new** `IDPCommonEvaluationLayer` (core + evaluation deps), or
2. Add evaluation deps to the base layer (bloats all Lambdas that use it), or
3. Convert `test_results_resolver` to a Docker image Lambda

### Option B: Create a new `IDPCommonEvaluationLayer`

```yaml
# template.yaml
IDPCommonEvaluationLayer:
  Type: AWS::Lambda::LayerVersion
  Properties:
    Description: "IDP Common evaluation layer (core + evaluation)"
    Content:
      S3Bucket: ...
      S3Key: ".../layers/<IDP_COMMON_EVALUATION_LAYER_ZIP>"
    CompatibleRuntimes:
      - python3.12
```

Build config addition in `publish.py`:

```python
layers_config = {
    "base": ["docs_service", "image"],
    "reporting": ["reporting"],
    "agents": ["agents"],
    "evaluation": ["evaluation"],    # ‚Üê NEW
}
```

### Option C: Convert test_results_resolver to Docker image

```yaml
TestResultsResolverFunction:
  PackageType: Image
  ImageUri: !Sub "${ECRRepository.RepositoryUri}:test-results-resolver-${ImageVersion}"
```

With a `requirements.txt`:
```
../../lib/idp_common_pkg[evaluation]
```

---

## Size Analysis

### Stickler + Transitive Dependencies

| Package | Size | Required by Stickler? |
|---------|------|-----------------------|
| `scipy` | **97 MB** | ‚úÖ Direct dependency |
| `pandas` | **70 MB** | ‚úÖ Direct dependency |
| `numpy` | **33 MB** | ‚úÖ Direct dependency |
| `rapidfuzz` | 5.0 MB | ‚úÖ Direct dependency |
| `pydantic` + `pydantic_core` | 8.3 MB | ‚úÖ Direct dependency |
| `psutil` | 848 KB | ‚úÖ Direct dependency |
| `munkres` | ~100 KB | ‚úÖ Direct dependency |
| `jsonschema` + specs | 1.2 MB | ‚úÖ Direct dependency |
| `stickler` itself | 1.4 MB | ‚Äî |
| **Total** | **~221 MB** | |

### Lambda Layer Size Limits

| Limit | Value | Stickler Fits? |
|-------|-------|----------------|
| Single layer (zipped) | 50 MB | ‚ùå Unlikely (221 MB unzipped) |
| All layers combined (unzipped) | 250 MB | ‚ö†Ô∏è Tight ‚Äî leaves ~29 MB for other layers |
| Docker image | 10 GB | ‚úÖ Easily |

A Lambda layer with Stickler + deps would likely **exceed the 50 MB zipped limit** due to scipy (97 MB) and pandas (70 MB). This means:

- **Option A (layer)**: Probably won't work without stripping scipy/pandas
- **Option B (new evaluation layer)**: Same problem
- **Option C (Docker image)**: Works, but changes the Lambda packaging model

### What BulkStructuredModelEvaluator Actually Needs

Looking at the source, `BulkStructuredModelEvaluator` imports:

```python
from stickler.structured_object_evaluator.models.structured_model import StructuredModel
from stickler.utils.process_evaluation import ProcessEvaluation
```

It needs `StructuredModel` because its `update()` method takes `(gt_model: StructuredModel, pred_model: StructuredModel)` ‚Äî it calls `gt_model.compare_with(pred_model)` internally.

**This is the fundamental mismatch**: `BulkStructuredModelEvaluator` is designed to take raw model instances and run comparisons. But in our use case, the per-document comparisons have **already happened** in the Evaluation Lambda. We just need to aggregate the pre-computed confusion matrices.

---

## Implementation: Direct Stickler Import

If we went with Option C (Docker image), here's what the implementation looks like:

### 1. Convert test_results_resolver to Docker image

```yaml
# nested/appsync/template.yaml
TestResultsResolverFunction:
  Type: AWS::Serverless::Function
  DependsOn: DockerBuildRun
  Metadata:
    SkipBuild: True
  Properties:
    PackageType: Image
    ImageUri: !Sub "${ECRRepository.RepositoryUri}:test-results-resolver-${ImageVersion}"
    Architectures:
      - arm64
    ImageConfig:
      Command:
        - "index.handler"
    MemorySize: 512
    Timeout: 300
```

New `requirements.txt`:
```
../../lib/idp_common_pkg[evaluation]
```

### 2. Use BulkStructuredModelEvaluator... but we can't

The `BulkStructuredModelEvaluator.update()` signature is:

```python
def update(self, gt_model: StructuredModel, pred_model: StructuredModel, doc_id: str = None):
```

It takes **StructuredModel instances**, not pre-computed confusion matrices. To use it, we'd need to:

1. Load the ground truth JSON from S3
2. Load the prediction JSON from S3
3. Reconstruct the Stickler schema (from the config)
4. Create `StructuredModel` instances from both
5. Call `update()` which internally calls `compare_with()` **again**

This means **re-running all per-document evaluations** in the test_results_resolver Lambda ‚Äî duplicating work already done by the Evaluation Lambda.

### 3. Alternative: Use only the accumulation internals

We could import Stickler but only use its internal accumulation methods:

```python
from stickler.structured_object_evaluator.bulk_structured_model_evaluator import BulkStructuredModelEvaluator
from stickler.utils.process_evaluation import ProcessEvaluation

# Can't use update() ‚Äî it needs StructuredModel instances
# Would need to call internal methods directly:
evaluator = BulkStructuredModelEvaluator.__new__(BulkStructuredModelEvaluator)
evaluator.reset()

for doc_result in eval_results:
    cm = doc_result.get("confusion_matrix", {})
    evaluator._accumulate_confusion_matrix(cm)  # ‚Üê Private method
    evaluator._processed_count += 1

result = evaluator.compute()
```

This works but:
- Depends on **private methods** (`_accumulate_confusion_matrix`, `_processed_count`)
- Could break on any Stickler version update
- Requires the full 221 MB dependency chain just to call a few accumulation functions

---

## Side-by-Side Comparison

| Aspect | Current Plan: Custom Aggregator | KISS Alt: Direct Stickler Import |
|--------|--------------------------------|----------------------------------|
| **New code** | ~80 lines (`aggregator.py`) | ~20 lines (import + wire up) |
| **Dependency size** | 0 MB (pure Python, no deps) | ~221 MB (stickler + scipy + pandas + numpy) |
| **Lambda packaging change** | None | Convert to Docker image OR create new layer |
| **Build pipeline change** | None | Add Docker build step for test_results_resolver |
| **CloudFormation changes** | Add `fieldLevelMetrics` to schema | Add `fieldLevelMetrics` + change Lambda to Image type |
| **Cold start impact** | None | +2-5s (loading scipy/numpy) |
| **Memory impact** | Negligible | +100-200 MB runtime |
| **API compatibility** | Operates on pre-computed confusion matrix dicts | `update()` needs StructuredModel instances (mismatch) |
| **Version coupling** | None ‚Äî standalone code | Tied to Stickler version; private API risk |
| **Re-evaluation** | No ‚Äî uses pre-computed results | Yes ‚Äî would re-run `compare_with()` per doc |
| **Maintenance** | Own code to maintain (~80 lines) | Stickler maintains the logic |
| **Test complexity** | Unit test with dict fixtures | Need Stickler + StructuredModel in test env |
| **Deploy time impact** | None | +30-60s (Docker build + push) |

---

## Recommendation

**Stick with the custom `BulkEvaluationAggregator` approach.** Here's why:

### 1. Fundamental API mismatch
`BulkStructuredModelEvaluator.update()` takes `StructuredModel` instances and runs `compare_with()` internally. Our use case has **already-computed** confusion matrices. Using Stickler directly would mean either:
- Re-running all evaluations (wasteful, slow)
- Calling private methods (fragile)

### 2. Massive dependency overhead for minimal logic
The accumulation logic is ~80 lines of straightforward Python (sum counts, derive P/R/F1). Importing Stickler brings in **221 MB** of dependencies (scipy, pandas, numpy) that are never used by the accumulation code.

### 3. No packaging changes needed
The test_results_resolver is currently a simple Zip Lambda with no dependencies. Keeping it that way avoids:
- Docker image build pipeline changes
- New Lambda layer creation
- Increased cold start times
- Higher memory consumption

### 4. Stickler is still the source of truth
The per-document evaluation still uses Stickler's `compare_with()` in the Evaluation Lambda. The custom aggregator just sums up the confusion matrices that Stickler produced. If Stickler changes its confusion matrix format, we update the aggregator ‚Äî same as we'd update any consumer.

### 5. The accumulation logic is stable
Summing TP/FP/FN/TN counts and deriving P/R/F1 is textbook ML metrics. This logic won't change. The risk of "getting it wrong" is minimal compared to the operational complexity of adding Stickler to the resolver Lambda.

---

## When Direct Import WOULD Make Sense

The Stickler import approach would be better if:

- We needed to **re-evaluate** documents with different thresholds/comparators at query time
- We needed Stickler's `merge_state()` for distributed evaluation across multiple Lambdas
- The test_results_resolver was **already** a Docker image with heavy dependencies
- ~~Stickler offered a `update_from_confusion_matrix(cm_dict)` public method (it doesn't today)~~ ‚Üí **Now it does ‚Äî see addendum below**

---

## üîÑ Addendum (2026-02-12): Stickler PR #74 Resolves the API Mismatch

[Stickler PR #74](https://github.com/awslabs/stickler/pull/74) merged to `dev` on 2026-02-12, shipping:

- `update_from_comparison_result(comparison_result, doc_id)` ‚Äî instance method accepting pre-computed `compare_with()` result dicts
- `aggregate_from_comparisons(comparison_results)` ‚Äî standalone function, top-level import: `from stickler import aggregate_from_comparisons`
- `target_schema` is now optional ‚Äî enables schema-less aggregation

**This resolves the fundamental API mismatch** that was the primary reason for the custom aggregator recommendation. The updated decision is:

| Context | Approach |
|---------|----------|
| `idp_common` (notebooks, CLI, Docker Lambdas) | **Use Stickler directly:** `from stickler import aggregate_from_comparisons` |
| `test_results_resolver` Lambda (bare Zip) | **Keep standalone shim** ‚Äî packaging constraint (221 MB) unchanged |

The custom `BulkEvaluationAggregator` class is no longer needed in `idp_common`. It only exists as a Lambda-only shim in `test_results_resolver/bulk_aggregator.py`.

**Blocking:** PR #74 is on Stickler's `dev` branch, not released. IDP pins `stickler-eval==0.1.4`. Implementation blocked until a Stickler release includes these changes.

See [.working/pr-74-integration/research.md](../.working/pr-74-integration/research.md) for full analysis and [future-stickler-refactor.md](../future-stickler-refactor.md) for updated migration path.
