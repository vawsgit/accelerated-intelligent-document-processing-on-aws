# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0

"""
Deployer Module

Handles CloudFormation stack deployment from CLI.
"""

import logging
import os
import random
import string
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

import boto3

logger = logging.getLogger(__name__)


class StackDeployer:
    """Manages CloudFormation stack deployment"""

    def __init__(self, region: Optional[str] = None):
        """
        Initialize stack deployer

        Args:
            region: AWS region (optional)
        """
        self.region = region
        self.cfn = boto3.client("cloudformation", region_name=region)

    def deploy_stack(
        self,
        stack_name: str,
        template_path: Optional[str] = None,
        template_url: Optional[str] = None,
        parameters: Dict[str, str] = None,
        wait: bool = False,
    ) -> Dict:
        """
        Deploy CloudFormation stack

        Args:
            stack_name: Name for the stack
            template_path: Path to local CloudFormation template (optional)
            template_url: URL to CloudFormation template in S3 (optional)
            parameters: Stack parameters
            wait: Whether to wait for stack creation to complete

        Returns:
            Dictionary with deployment result
        """
        logger.info(f"Deploying stack: {stack_name}")

        if not template_path and not template_url:
            raise ValueError("Either template_path or template_url must be provided")

        # Determine template source
        if template_url:
            logger.info(f"Using template URL: {template_url}")
            template_param = {"TemplateURL": template_url}
        else:
            # Read template from local file
            template_body = self._read_template(template_path)
            template_param = {"TemplateBody": template_body}

        # Convert parameters dict to CloudFormation format
        cfn_parameters = [
            {"ParameterKey": k, "ParameterValue": v}
            for k, v in (parameters or {}).items()
        ]

        # Check if stack exists
        stack_exists = self._stack_exists(stack_name)

        try:
            if stack_exists:
                logger.info(f"Stack {stack_name} exists - updating")
                response = self.cfn.update_stack(
                    StackName=stack_name,
                    **template_param,
                    Parameters=cfn_parameters,
                    Capabilities=[
                        "CAPABILITY_IAM",
                        "CAPABILITY_NAMED_IAM",
                        "CAPABILITY_AUTO_EXPAND",
                    ],
                )
                operation = "UPDATE"
            else:
                logger.info(f"Creating new stack: {stack_name}")
                response = self.cfn.create_stack(
                    StackName=stack_name,
                    **template_param,
                    Parameters=cfn_parameters,
                    Capabilities=[
                        "CAPABILITY_IAM",
                        "CAPABILITY_NAMED_IAM",
                        "CAPABILITY_AUTO_EXPAND",
                    ],
                    OnFailure="ROLLBACK",
                )
                operation = "CREATE"

            result = {
                "stack_name": stack_name,
                "stack_id": response.get("StackId", ""),
                "operation": operation,
                "status": "INITIATED",
            }

            if wait:
                result = self._wait_for_completion(stack_name, operation)

            return result

        except self.cfn.exceptions.AlreadyExistsException:
            raise ValueError(
                f"Stack {stack_name} already exists. Use --update flag to update."
            )
        except Exception as e:
            logger.error(f"Error deploying stack: {e}")
            raise

    def _read_template(self, template_path: str) -> str:
        """Read CloudFormation template file"""
        template_file = Path(template_path)

        if not template_file.exists():
            raise FileNotFoundError(f"Template not found: {template_path}")

        return template_file.read_text()

    def _stack_exists(self, stack_name: str) -> bool:
        """Check if stack exists"""
        try:
            self.cfn.describe_stacks(StackName=stack_name)
            return True
        except self.cfn.exceptions.ClientError as e:
            if "does not exist" in str(e):
                return False
            raise

    def _wait_for_completion(self, stack_name: str, operation: str) -> Dict:
        """
        Wait for stack operation to complete with progress display

        Args:
            stack_name: Stack name
            operation: CREATE or UPDATE

        Returns:
            Dictionary with final status
        """
        from rich.console import Console
        from rich.progress import Progress, SpinnerColumn, TextColumn

        console = Console()
        logger.info(f"Waiting for {operation} to complete...")

        complete_statuses = {
            "CREATE": [
                "CREATE_COMPLETE",
                "CREATE_FAILED",
                "ROLLBACK_COMPLETE",
                "ROLLBACK_FAILED",
            ],
            "UPDATE": [
                "UPDATE_COMPLETE",
                "UPDATE_FAILED",
                "UPDATE_ROLLBACK_COMPLETE",
                "UPDATE_ROLLBACK_FAILED",
            ],
        }

        success_statuses = {
            "CREATE": ["CREATE_COMPLETE"],
            "UPDATE": ["UPDATE_COMPLETE"],
        }

        target_statuses = complete_statuses[operation]
        success_set = success_statuses[operation]

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task(
                f"[cyan]{operation} stack: {stack_name}", total=None
            )
            last_event_time = None

            while True:
                try:
                    response = self.cfn.describe_stacks(StackName=stack_name)
                    stacks = response.get("Stacks", [])

                    if not stacks:
                        raise ValueError(f"Stack {stack_name} not found")

                    stack = stacks[0]
                    status = stack.get("StackStatus", "")

                    # Get recent events
                    events = self.get_stack_events(stack_name, limit=5)
                    if events and events[0]["timestamp"] != last_event_time:
                        last_event_time = events[0]["timestamp"]
                        # Show most recent event
                        latest = events[0]
                        resource = latest["resource"]
                        resource_status = latest["status"]
                        progress.update(
                            task,
                            description=f"[cyan]{operation}: {resource} - {resource_status}",
                        )

                    if status in target_statuses:
                        # Operation complete
                        is_success = status in success_set

                        result = {
                            "stack_name": stack_name,
                            "operation": operation,
                            "status": status,
                            "success": is_success,
                            "outputs": self._get_stack_outputs(stack),
                        }

                        if not is_success:
                            result["error"] = self._get_stack_failure_reason(stack_name)

                        return result

                    # Wait before next check
                    time.sleep(10)

                except Exception as e:
                    logger.error(f"Error waiting for stack: {e}")
                    raise

    def _get_stack_outputs(self, stack: Dict) -> Dict[str, str]:
        """Extract stack outputs as dictionary"""
        outputs = {}
        for output in stack.get("Outputs", []):
            key = output.get("OutputKey", "")
            value = output.get("OutputValue", "")
            outputs[key] = value
        return outputs

    def _get_stack_failure_reason(self, stack_name: str) -> str:
        """Get failure reason from stack events"""
        try:
            response = self.cfn.describe_stack_events(StackName=stack_name)
            events = response.get("StackEvents", [])

            # Find first failed event
            for event in events:
                status = event.get("ResourceStatus", "")
                if "FAILED" in status:
                    reason = event.get("ResourceStatusReason", "Unknown")
                    resource = event.get("LogicalResourceId", "Unknown")
                    return f"{resource}: {reason}"

            return "Unknown failure reason"
        except Exception as e:
            return str(e)

    def get_stack_events(self, stack_name: str, limit: int = 20) -> List[Dict]:
        """
        Get recent stack events

        Args:
            stack_name: Stack name
            limit: Maximum number of events to return

        Returns:
            List of event dictionaries
        """
        try:
            response = self.cfn.describe_stack_events(StackName=stack_name)
            events = response.get("StackEvents", [])[:limit]

            return [
                {
                    "timestamp": event.get("Timestamp", ""),
                    "resource": event.get("LogicalResourceId", ""),
                    "status": event.get("ResourceStatus", ""),
                    "reason": event.get("ResourceStatusReason", ""),
                }
                for event in events
            ]
        except Exception as e:
            logger.error(f"Error getting stack events: {e}")
            return []

    def delete_stack(
        self,
        stack_name: str,
        empty_buckets: bool = False,
        wait: bool = True,
    ) -> Dict:
        """
        Delete CloudFormation stack

        Args:
            stack_name: Name of stack to delete
            empty_buckets: Whether to empty S3 buckets before deletion
            wait: Whether to wait for deletion to complete

        Returns:
            Dictionary with deletion result including stack_id
        """
        logger.info(f"Deleting stack: {stack_name}")

        # Check if stack exists
        if not self._stack_exists(stack_name):
            raise ValueError(f"Stack '{stack_name}' does not exist")

        # Get stack ID before deletion (needed for querying deleted stacks)
        try:
            response = self.cfn.describe_stacks(StackName=stack_name)
            stack_id = response["Stacks"][0]["StackId"]
        except Exception as e:
            logger.warning(f"Could not get stack ID: {e}")
            stack_id = stack_name  # Fallback to name

        # Get stack resources to find buckets
        bucket_info = self._get_stack_buckets(stack_name)

        # Empty buckets if requested
        if empty_buckets and bucket_info:
            self._empty_buckets(bucket_info)

        # Delete stack
        try:
            self.cfn.delete_stack(StackName=stack_name)
            logger.info(f"Stack deletion initiated: {stack_name}")

            result = {
                "stack_name": stack_name,
                "stack_id": stack_id,
                "operation": "DELETE",
                "status": "INITIATED",
            }

            if wait:
                result = self._wait_for_deletion(stack_name)
                # Ensure stack_id is preserved after wait
                result["stack_id"] = stack_id

            return result

        except Exception as e:
            logger.error(f"Error deleting stack: {e}")
            raise

    def _get_stack_buckets(self, stack_name: str) -> List[Dict]:
        """
        Get S3 buckets from stack

        Args:
            stack_name: Stack name

        Returns:
            List of bucket information dictionaries
        """
        buckets = []

        try:
            # Get stack resources
            paginator = self.cfn.get_paginator("list_stack_resources")
            pages = paginator.paginate(StackName=stack_name)

            for page in pages:
                for resource in page.get("StackResourceSummaries", []):
                    if resource.get("ResourceType") == "AWS::S3::Bucket":
                        bucket_name = resource.get("PhysicalResourceId")
                        if bucket_name:
                            buckets.append(
                                {
                                    "logical_id": resource.get("LogicalResourceId"),
                                    "bucket_name": bucket_name,
                                }
                            )

            return buckets

        except Exception as e:
            logger.error(f"Error getting stack buckets: {e}")
            return []

    def _empty_buckets(self, bucket_info: List[Dict]) -> None:
        """
        Empty S3 buckets

        Args:
            bucket_info: List of bucket information dictionaries
        """
        s3 = boto3.resource("s3", region_name=self.region)

        for bucket_dict in bucket_info:
            bucket_name = bucket_dict["bucket_name"]
            logical_id = bucket_dict["logical_id"]

            try:
                logger.info(f"Emptying bucket {logical_id}: {bucket_name}")
                bucket = s3.Bucket(bucket_name)

                # Delete all objects and versions
                bucket.object_versions.all().delete()
                logger.info(f"Emptied bucket: {bucket_name}")

            except Exception as e:
                logger.error(f"Error emptying bucket {bucket_name}: {e}")
                raise Exception(
                    f"Failed to empty bucket {bucket_name}. You may need to empty it manually."
                )

    def _wait_for_deletion(self, stack_name: str) -> Dict:
        """
        Wait for stack deletion to complete

        Args:
            stack_name: Stack name

        Returns:
            Dictionary with final status
        """
        from rich.console import Console
        from rich.progress import Progress, SpinnerColumn, TextColumn

        console = Console()
        logger.info("Waiting for DELETE to complete...")

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task(f"[cyan]DELETE stack: {stack_name}", total=None)

            while True:
                try:
                    response = self.cfn.describe_stacks(StackName=stack_name)
                    stacks = response.get("Stacks", [])

                    if not stacks:
                        # Stack no longer exists - deletion complete
                        return {
                            "stack_name": stack_name,
                            "operation": "DELETE",
                            "status": "DELETE_COMPLETE",
                            "success": True,
                        }

                    stack = stacks[0]
                    status = stack.get("StackStatus", "")

                    # Update progress with current status
                    progress.update(task, description=f"[cyan]DELETE: {status}")

                    if status == "DELETE_FAILED":
                        return {
                            "stack_name": stack_name,
                            "operation": "DELETE",
                            "status": status,
                            "success": False,
                            "error": self._get_stack_failure_reason(stack_name),
                        }

                    # Wait before next check
                    time.sleep(10)

                except self.cfn.exceptions.ClientError as e:
                    if "does not exist" in str(e):
                        # Stack deleted successfully
                        return {
                            "stack_name": stack_name,
                            "operation": "DELETE",
                            "status": "DELETE_COMPLETE",
                            "success": True,
                        }
                    raise

    def get_bucket_info(self, stack_name: str) -> List[Dict]:
        """
        Get information about S3 buckets in stack

        Args:
            stack_name: Stack name

        Returns:
            List of bucket information with object counts and sizes
        """
        buckets = self._get_stack_buckets(stack_name)
        s3 = boto3.client("s3", region_name=self.region)

        for bucket_dict in buckets:
            bucket_name = bucket_dict["bucket_name"]

            try:
                # Get bucket statistics
                response = s3.list_objects_v2(Bucket=bucket_name)
                objects = response.get("Contents", [])

                bucket_dict["object_count"] = len(objects)
                bucket_dict["total_size"] = sum(obj.get("Size", 0) for obj in objects)

                # Convert size to human-readable format
                size_mb = bucket_dict["total_size"] / (1024 * 1024)
                bucket_dict["size_display"] = f"{size_mb:.2f} MB"

            except Exception as e:
                logger.warning(f"Could not get stats for {bucket_name}: {e}")
                bucket_dict["object_count"] = 0
                bucket_dict["total_size"] = 0
                bucket_dict["size_display"] = "Unknown"

        return buckets

    def get_retained_resources_after_deletion(
        self, stack_name: str, include_nested: bool = True
    ) -> Dict:
        """
        Get resources that CloudFormation didn't delete

        Queries stack after deletion attempt to find resources
        with status DELETE_SKIPPED or still existing.

        Args:
            stack_name: Stack name
            include_nested: Include nested stack resources

        Returns:
            Dictionary with categorized retained resources
        """
        retained_resources = {
            "dynamodb_tables": [],
            "log_groups": [],
            "s3_buckets": [],
            "other": [],
        }

        # Get all stacks to check (main + nested)
        stacks_to_check = [stack_name]

        if include_nested:
            nested_stacks = self._get_nested_stacks(stack_name)
            stacks_to_check.extend(nested_stacks)

        for stack in stacks_to_check:
            try:
                # Query resources for this stack
                paginator = self.cfn.get_paginator("list_stack_resources")
                pages = paginator.paginate(StackName=stack)

                for page in pages:
                    for resource in page.get("StackResourceSummaries", []):
                        status = resource.get("ResourceStatus")
                        resource_type = resource.get("ResourceType")
                        physical_id = resource.get("PhysicalResourceId")

                        # Special handling for log groups - CF may mark DELETE_COMPLETE but they still exist
                        if resource_type == "AWS::Logs::LogGroup":
                            if physical_id and self._log_group_exists(physical_id):
                                resource_info = {
                                    "logical_id": resource.get("LogicalResourceId"),
                                    "physical_id": physical_id,
                                    "type": resource_type,
                                    "status": "EXISTS",  # Override CF status
                                    "status_reason": "Verified to exist in CloudWatch",
                                    "stack": stack,
                                }
                                retained_resources["log_groups"].append(resource_info)
                        # For other resources, use CF status
                        elif status not in ["DELETE_COMPLETE", "DELETE_IN_PROGRESS"]:
                            resource_info = {
                                "logical_id": resource.get("LogicalResourceId"),
                                "physical_id": physical_id,
                                "type": resource_type,
                                "status": status,
                                "status_reason": resource.get(
                                    "ResourceStatusReason", ""
                                ),
                                "stack": stack,
                            }

                            # Categorize by type
                            if resource_type == "AWS::DynamoDB::Table":
                                retained_resources["dynamodb_tables"].append(
                                    resource_info
                                )
                            elif resource_type == "AWS::S3::Bucket":
                                retained_resources["s3_buckets"].append(resource_info)
                            else:
                                retained_resources["other"].append(resource_info)

            except self.cfn.exceptions.ClientError as e:
                # Stack might not exist anymore - that's ok
                if "does not exist" not in str(e):
                    logger.warning(f"Error checking resources for {stack}: {e}")

        return retained_resources

    def _get_nested_stacks(self, stack_name: str) -> List[str]:
        """
        Recursively find all nested stacks

        Args:
            stack_name: Parent stack name

        Returns:
            List of nested stack names
        """
        nested_stacks = []

        try:
            paginator = self.cfn.get_paginator("list_stack_resources")
            pages = paginator.paginate(StackName=stack_name)

            for page in pages:
                for resource in page.get("StackResourceSummaries", []):
                    if resource.get("ResourceType") == "AWS::CloudFormation::Stack":
                        nested_name = resource.get("PhysicalResourceId")
                        if nested_name:
                            nested_stacks.append(nested_name)
                            # Recursively get nested stacks of nested stacks
                            nested_stacks.extend(self._get_nested_stacks(nested_name))

        except Exception as e:
            logger.warning(f"Error getting nested stacks for {stack_name}: {e}")

        return nested_stacks

    def _log_group_exists(self, log_group_name: str) -> bool:
        """
        Check if log group actually exists in CloudWatch

        Args:
            log_group_name: Log group name to check

        Returns:
            True if log group exists, False otherwise
        """
        logs = boto3.client("logs", region_name=self.region)

        try:
            response = logs.describe_log_groups(
                logGroupNamePrefix=log_group_name, limit=1
            )
            # Check if exact match exists
            for group in response.get("logGroups", []):
                if group["logGroupName"] == log_group_name:
                    return True
            return False
        except Exception as e:
            logger.warning(f"Error checking log group {log_group_name}: {e}")
            return False

    def _get_stack_cloudfront_distributions(self, stack_name: str) -> List[Dict]:
        """
        Get CloudFront distributions from stack resources

        Args:
            stack_name: Stack name

        Returns:
            List of CloudFront distribution information
        """
        distributions = []

        try:
            paginator = self.cfn.get_paginator("list_stack_resources")
            pages = paginator.paginate(StackName=stack_name)

            for page in pages:
                for resource in page.get("StackResourceSummaries", []):
                    if resource.get("ResourceType") == "AWS::CloudFront::Distribution":
                        dist_id = resource.get("PhysicalResourceId")
                        if dist_id:
                            distributions.append(
                                {
                                    "logical_id": resource.get("LogicalResourceId"),
                                    "distribution_id": dist_id,
                                    "status": resource.get("ResourceStatus"),
                                }
                            )

            return distributions

        except self.cfn.exceptions.ClientError as e:
            if "does not exist" in str(e):
                # Stack deleted - distributions should be gone
                return []
            logger.warning(f"Error getting CloudFront distributions: {e}")
            return []

    def _verify_cloudfront_distributions_deleted(
        self, stack_name: str, max_wait_seconds: int = 300
    ) -> None:
        """
        Verify CloudFront distributions are deleted before proceeding with S3 deletion

        This prevents orphaned CloudFront distributions pointing to deleted S3 origins.

        Args:
            stack_name: Stack name
            max_wait_seconds: Maximum time to wait for distributions to be deleted

        Raises:
            Exception: If distributions still exist after max wait time
        """
        from rich.console import Console

        console = Console()

        # Get CloudFront distributions from stack
        distributions = self._get_stack_cloudfront_distributions(stack_name)

        if not distributions:
            logger.info("No CloudFront distributions found in stack")
            return

        console.print(
            f"[cyan]Verifying {len(distributions)} CloudFront distribution(s) are deleted...[/cyan]"
        )

        cloudfront = boto3.client("cloudfront")
        start_time = time.time()

        for dist_info in distributions:
            dist_id = dist_info["distribution_id"]
            logical_id = dist_info["logical_id"]

            # Check if distribution still exists
            while True:
                try:
                    response = cloudfront.get_distribution(Id=dist_id)
                    dist_config = response.get("Distribution", {})
                    status = dist_config.get("Status", "")
                    enabled = dist_config.get("DistributionConfig", {}).get(
                        "Enabled", False
                    )

                    elapsed = time.time() - start_time

                    if elapsed > max_wait_seconds:
                        raise Exception(
                            f"CloudFront distribution {logical_id} ({dist_id}) still exists after {max_wait_seconds}s. "
                            f"Status: {status}, Enabled: {enabled}. "
                            f"Cannot proceed with S3 bucket deletion to prevent orphaned distributions. "
                            f"Please disable/delete the distribution manually and retry."
                        )

                    # Distribution still exists - wait for it to be deleted
                    console.print(
                        f"  Waiting for {logical_id} ({dist_id}) to be deleted... "
                        f"Status: {status}, Enabled: {enabled} ({int(elapsed)}s elapsed)"
                    )
                    time.sleep(10)

                except cloudfront.exceptions.NoSuchDistribution:
                    # Distribution deleted - good to proceed
                    console.print(f"  ✓ {logical_id} ({dist_id}) is deleted")
                    break
                except Exception as e:
                    if "NoSuchDistribution" in str(e):
                        # Distribution deleted
                        console.print(f"  ✓ {logical_id} ({dist_id}) is deleted")
                        break
                    else:
                        raise

    def _discover_auto_created_log_groups(self, stack_name: str) -> List[str]:
        """
        Discover auto-created log groups that match stack name patterns

        These are log groups created automatically by AWS services (Lambda, CodeBuild,
        Glue Crawlers, etc.) that are not tracked by CloudFormation.

        Args:
            stack_name: Stack name to match patterns against

        Returns:
            List of log group names matching stack patterns
        """
        logs = boto3.client("logs", region_name=self.region)
        discovered_log_groups = []

        # Define patterns to match - these are auto-created by AWS services
        # Use exact prefixes to avoid inadvertent matches to longer stack names
        # (e.g., "idp1" should not match "idp10")
        patterns_to_check = [
            # Lambda functions - pattern requires hyphen after stack name
            f"/aws/lambda/{stack_name}-DOCUMENTKB",
            f"/aws/lambda/{stack_name}-BDASAMPLEPROJECT",  # BDA sample project
            f"/aws/lambda/{stack_name}-DashboardMergerFunction",
            f"/aws/lambda/{stack_name}-InitializeConcurrencyTableLambda",
            # Nested stacks - pattern requires hyphen after stack name
            f"/{stack_name}-PATTERN1STACK-",  # e.g., /IDPDocker-P1-PATTERN1STACK-ABC123/lambda/...
            f"/{stack_name}-PATTERN2STACK-",
            f"/{stack_name}-PATTERN3STACK-",
            # CodeBuild projects - pattern requires hyphen after stack name
            f"/aws/codebuild/{stack_name}-PATTERN1STACK",  # Nested stack CodeBuild
            f"/aws/codebuild/{stack_name}-PATTERN2STACK",
            f"/aws/codebuild/{stack_name}-PATTERN3STACK",
            f"/aws/codebuild/{stack_name}-webui-build",  # Main stack webui build
            # Glue crawlers - pattern requires hyphen after stack name
            f"/aws-glue/crawlers-role/{stack_name}-DocumentSectionsCrawlerRole",
        ]

        # Also check for explicit log group names (these may or may not be in CFN)
        # These are exact prefixes with hyphens to prevent matching longer stack names
        explicit_patterns = [
            f"{stack_name}-GetDomainLambdaLogGroup-",
            f"{stack_name}-StacknameCheckFunctionLogGroup-",
            f"{stack_name}-ConfigurationCopyFunctionLogGroup-",
            f"{stack_name}-UpdateSettingsFunctionLogGroup-",
        ]

        try:
            # Use paginator to handle large numbers of log groups
            paginator = logs.get_paginator("describe_log_groups")

            # Track matches found per pattern for debugging
            pattern_match_count = {}

            # Check each pattern
            for pattern in patterns_to_check:
                try:
                    matches_for_pattern = 0
                    page_iterator = paginator.paginate(logGroupNamePrefix=pattern)

                    for page in page_iterator:
                        for log_group in page.get("logGroups", []):
                            log_group_name = log_group["logGroupName"]

                            # Additional validation: ensure we don't match longer stack names
                            # The pattern already includes a hyphen, so this should be safe
                            # But we double-check by ensuring the log group starts with exactly our pattern
                            if log_group_name.startswith(pattern):
                                if log_group_name not in discovered_log_groups:
                                    discovered_log_groups.append(log_group_name)
                                    matches_for_pattern += 1
                                    logger.debug(
                                        f"Discovered auto-created log group: {log_group_name}"
                                    )

                    pattern_match_count[pattern] = matches_for_pattern

                except Exception as e:
                    logger.warning(f"Error checking pattern {pattern}: {e}")
                    continue

            # Check explicit patterns
            for pattern in explicit_patterns:
                try:
                    matches_for_pattern = 0
                    response = logs.describe_log_groups(
                        logGroupNamePrefix=pattern,
                        limit=50,  # Should be enough for exact matches
                    )

                    for log_group in response.get("logGroups", []):
                        log_group_name = log_group["logGroupName"]
                        # Verify exact prefix match to avoid matching longer stack names
                        if log_group_name.startswith(pattern):
                            if log_group_name not in discovered_log_groups:
                                discovered_log_groups.append(log_group_name)
                                matches_for_pattern += 1
                                logger.debug(
                                    f"Discovered explicit log group: {log_group_name}"
                                )

                    pattern_match_count[pattern] = matches_for_pattern

                except Exception as e:
                    logger.warning(f"Error checking explicit pattern {pattern}: {e}")
                    continue

            # Log summary with pattern match counts
            if discovered_log_groups:
                logger.info(
                    f"Discovered {len(discovered_log_groups)} auto-created log groups for stack {stack_name}"
                )
                logger.debug(f"Pattern match counts: {pattern_match_count}")
            else:
                logger.info(f"No auto-created log groups found for stack {stack_name}")
                logger.debug(
                    f"Checked {len(patterns_to_check) + len(explicit_patterns)} patterns"
                )

            return discovered_log_groups

        except Exception as e:
            logger.error(f"Error discovering auto-created log groups: {e}")
            return []

    def cleanup_retained_resources(self, stack_identifier: str) -> Dict:
        """
        Delete resources that CloudFormation retained

        Args:
            stack_identifier: Stack name or stack ID (use ID for deleted stacks)

        Returns:
            Cleanup summary with deleted resources and errors
        """
        from rich.console import Console
        from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn

        console = Console()

        console.print("\n[bold blue]Analyzing retained resources...[/bold blue]")

        # Get resources that weren't deleted by CloudFormation
        retained = self.get_retained_resources_after_deletion(stack_identifier)

        # Extract stack name from identifier (handle both stack name and stack ARN/ID)
        # Stack ARN format: arn:aws:cloudformation:region:account:stack/stack-name/guid
        stack_name = stack_identifier
        if stack_identifier.startswith("arn:"):
            try:
                # Extract stack name from ARN
                stack_name = stack_identifier.split("/")[1]
            except IndexError:
                logger.warning(
                    f"Could not extract stack name from ARN: {stack_identifier}"
                )

        # Discover auto-created log groups that CloudFormation doesn't track
        console.print("[cyan]Discovering auto-created log groups...[/cyan]")
        auto_created_log_groups = self._discover_auto_created_log_groups(stack_name)

        # Merge auto-created log groups with CloudFormation-tracked ones
        # Use a set to avoid duplicates
        cfn_log_group_names = {lg["physical_id"] for lg in retained["log_groups"]}

        for log_group_name in auto_created_log_groups:
            if log_group_name not in cfn_log_group_names:
                # Add to retained log groups list
                retained["log_groups"].append(
                    {
                        "logical_id": "Auto-created",
                        "physical_id": log_group_name,
                        "type": "AWS::Logs::LogGroup",
                        "status": "AUTO_CREATED",
                        "status_reason": "Auto-created by AWS service",
                        "stack": stack_name,
                    }
                )

        # Count resources (including newly discovered log groups)
        total = (
            len(retained["dynamodb_tables"])
            + len(retained["log_groups"])
            + len(retained["s3_buckets"])
        )

        if total == 0:
            console.print(
                "[green]✓ No retained resources found - CloudFormation deleted everything![/green]"
            )
            return {"total_deleted": 0, "errors": []}

        console.print(f"Found {total} retained resources:")
        console.print(f"  • DynamoDB Tables: {len(retained['dynamodb_tables'])}")
        console.print(f"  • CloudWatch Logs: {len(retained['log_groups'])}")
        console.print(f"  • S3 Buckets: {len(retained['s3_buckets'])}")

        if retained["other"]:
            console.print(f"  • Other: {len(retained['other'])}")
            console.print(
                "[yellow]Warning: Some resources cannot be auto-deleted[/yellow]"
            )

        results = {
            "dynamodb_deleted": [],
            "logs_deleted": [],
            "buckets_deleted": [],
            "errors": [],
        }

        # Delete each type with progress bar
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        ) as progress:
            # Phase 1: DynamoDB
            if retained["dynamodb_tables"]:
                task = progress.add_task(
                    "Deleting DynamoDB tables...",
                    total=len(retained["dynamodb_tables"]),
                )
                for table in retained["dynamodb_tables"]:
                    try:
                        self._delete_dynamodb_table(table["physical_id"])
                        results["dynamodb_deleted"].append(table["physical_id"])
                    except Exception as e:
                        results["errors"].append(
                            {
                                "resource": table["physical_id"],
                                "type": "DynamoDB",
                                "error": str(e),
                            }
                        )
                    progress.advance(task)

            # Phase 2: Log Groups
            if retained["log_groups"]:
                task = progress.add_task(
                    "Deleting CloudWatch logs...", total=len(retained["log_groups"])
                )
                for log_group in retained["log_groups"]:
                    try:
                        self._delete_log_group(log_group["physical_id"])
                        results["logs_deleted"].append(log_group["physical_id"])
                    except Exception as e:
                        results["errors"].append(
                            {
                                "resource": log_group["physical_id"],
                                "type": "LogGroup",
                                "error": str(e),
                            }
                        )
                    progress.advance(task)

            # Phase 3: Verify CloudFront distributions are deleted before S3 buckets
            # This prevents orphaned CloudFront distributions pointing to deleted S3 origins
            if retained["s3_buckets"]:
                console.print()
                console.print(
                    "[cyan]Verifying CloudFront distributions are deleted...[/cyan]"
                )
                try:
                    self._verify_cloudfront_distributions_deleted(stack_name)
                    console.print(
                        "[green]✓ CloudFront distributions verified as deleted[/green]"
                    )
                except Exception as e:
                    error_msg = str(e)
                    console.print(
                        f"[red]✗ CloudFront verification failed: {error_msg}[/red]"
                    )
                    results["errors"].append(
                        {
                            "resource": "CloudFront Distribution",
                            "type": "CloudFront",
                            "error": error_msg,
                        }
                    )
                    # Do not proceed with S3 deletion if CloudFront still exists
                    console.print(
                        "[yellow]Skipping S3 bucket deletion to prevent orphaned CloudFront distributions[/yellow]"
                    )
                    return results

            # Phase 4: S3 Buckets (LoggingBucket last)
            if retained["s3_buckets"]:
                # Separate LoggingBucket from others
                logging_bucket = None
                regular_buckets = []

                for bucket in retained["s3_buckets"]:
                    if "logging" in bucket["logical_id"].lower():
                        logging_bucket = bucket
                    else:
                        regular_buckets.append(bucket)

                # Delete regular buckets first, then logging bucket
                all_buckets = regular_buckets + (
                    [logging_bucket] if logging_bucket else []
                )

                task = progress.add_task(
                    "Deleting S3 buckets...", total=len(all_buckets)
                )
                for bucket in all_buckets:
                    try:
                        self._empty_and_delete_bucket(bucket["physical_id"])
                        results["buckets_deleted"].append(bucket["physical_id"])
                    except Exception as e:
                        results["errors"].append(
                            {
                                "resource": bucket["physical_id"],
                                "type": "S3Bucket",
                                "error": str(e),
                            }
                        )
                    progress.advance(task)

        return results

    def _delete_dynamodb_table(self, table_name: str) -> None:
        """
        Delete a DynamoDB table

        Args:
            table_name: Table name to delete
        """
        dynamodb = boto3.client("dynamodb", region_name=self.region)

        try:
            # Disable point-in-time recovery if enabled
            try:
                dynamodb.update_continuous_backups(
                    TableName=table_name,
                    PointInTimeRecoverySpecification={
                        "PointInTimeRecoveryEnabled": False
                    },
                )
            except Exception:
                # May not have PITR enabled, continue
                pass

            # Delete the table
            dynamodb.delete_table(TableName=table_name)
            logger.info(f"Deleted DynamoDB table: {table_name}")

        except Exception as e:
            logger.error(f"Error deleting DynamoDB table {table_name}: {e}")
            raise

    def _delete_log_group(self, log_group_name: str) -> None:
        """
        Delete a CloudWatch Log Group

        Args:
            log_group_name: Log group name to delete
        """
        logs = boto3.client("logs", region_name=self.region)

        try:
            logs.delete_log_group(logGroupName=log_group_name)
            logger.info(f"Deleted log group: {log_group_name}")

        except Exception as e:
            logger.error(f"Error deleting log group {log_group_name}: {e}")
            raise

    def _empty_and_delete_bucket(self, bucket_name: str) -> None:
        """
        Empty and delete an S3 bucket

        Args:
            bucket_name: Bucket name to delete
        """
        s3 = boto3.resource("s3", region_name=self.region)

        try:
            bucket = s3.Bucket(bucket_name)

            # Delete all objects and versions
            bucket.object_versions.all().delete()

            # Delete the bucket
            bucket.delete()
            logger.info(f"Deleted S3 bucket: {bucket_name}")

        except Exception as e:
            logger.error(f"Error deleting bucket {bucket_name}: {e}")
            raise


def is_local_file_path(path: str) -> bool:
    """
    Determine if path is a local file vs S3 URI

    Args:
        path: Path to check

    Returns:
        True if local file path, False if S3 URI
    """
    return not path.startswith("s3://")


def validate_s3_uri(uri: str) -> bool:
    """
    Validate S3 URI format

    Args:
        uri: S3 URI to validate

    Returns:
        True if valid S3 URI format
    """
    if not uri.startswith("s3://"):
        return False

    # Remove s3:// prefix and check for bucket/key structure
    path = uri[5:]
    parts = path.split("/", 1)

    # Must have bucket and key
    return len(parts) == 2 and parts[0] and parts[1]


def get_or_create_config_bucket(region: str) -> str:
    """
    Get or create temporary S3 bucket for CLI config uploads

    Args:
        region: AWS region

    Returns:
        Bucket name
    """
    s3 = boto3.client("s3", region_name=region)
    sts = boto3.client("sts")

    try:
        account_id = sts.get_caller_identity()["Account"]
    except Exception as e:
        raise Exception(f"Failed to get AWS account ID: {e}")

    # Normalize region name for bucket (replace hyphens with nothing for cleaner name)
    region_normalized = region.replace("-", "")

    # Check for existing bucket with pattern
    bucket_prefix = f"idp-cli-config-{account_id}-{region_normalized}-"

    try:
        response = s3.list_buckets()
        for bucket in response.get("Buckets", []):
            bucket_name = bucket["Name"]
            if bucket_name.startswith(bucket_prefix):
                logger.info(f"Using existing config bucket: {bucket_name}")
                return bucket_name
    except Exception as e:
        logger.warning(f"Error listing buckets: {e}")

    # Create new bucket with random suffix
    suffix = "".join(random.choices(string.ascii_lowercase + string.digits, k=8))
    bucket_name = f"{bucket_prefix}{suffix}"

    logger.info(f"Creating new config bucket: {bucket_name}")

    try:
        # Create bucket
        if region == "us-east-1":
            s3.create_bucket(Bucket=bucket_name)
        else:
            s3.create_bucket(
                Bucket=bucket_name,
                CreateBucketConfiguration={"LocationConstraint": region},
            )

        # Enable versioning
        s3.put_bucket_versioning(
            Bucket=bucket_name, VersioningConfiguration={"Status": "Enabled"}
        )

        # Enable encryption
        s3.put_bucket_encryption(
            Bucket=bucket_name,
            ServerSideEncryptionConfiguration={
                "Rules": [
                    {"ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"}}
                ]
            },
        )

        # Set lifecycle policy (30-day expiration)
        s3.put_bucket_lifecycle_configuration(
            Bucket=bucket_name,
            LifecycleConfiguration={
                "Rules": [
                    {
                        "ID": "DeleteOldConfigs",
                        "Status": "Enabled",
                        "Prefix": "idp-cli/custom-configurations/",
                        "Expiration": {"Days": 30},
                    }
                ]
            },
        )

        # Add tags
        s3.put_bucket_tagging(
            Bucket=bucket_name,
            Tagging={
                "TagSet": [
                    {"Key": "CreatedBy", "Value": "idp-cli"},
                    {"Key": "Purpose", "Value": "config-staging"},
                ]
            },
        )

        logger.info(f"Successfully created config bucket: {bucket_name}")
        return bucket_name

    except Exception as e:
        raise Exception(f"Failed to create config bucket: {e}")


def upload_local_config(
    file_path: str, region: str, stack_name: Optional[str] = None
) -> str:
    """
    Upload local config file to temporary S3 bucket

    Args:
        file_path: Path to local config file
        region: AWS region
        stack_name: CloudFormation stack name (unused, kept for compatibility)

    Returns:
        S3 URI of uploaded file
    """
    # Validate file exists
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f"Config file not found: {file_path}")

    logger.info(f"Uploading local config file: {file_path}")

    # Always use temp bucket
    bucket_name = get_or_create_config_bucket(region)

    # Generate timestamped key - use underscores instead of hyphens in filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    original_name = os.path.basename(file_path)
    # Sanitize filename - replace hyphens with underscores for maximum compatibility
    safe_name = original_name.replace("-", "_")
    s3_key = f"idp-cli/custom-configurations/config_{timestamp}_{safe_name}"

    # Upload file
    s3 = boto3.client("s3", region_name=region)
    try:
        with open(file_path, "rb") as f:
            s3.put_object(
                Bucket=bucket_name, Key=s3_key, Body=f, ServerSideEncryption="AES256"
            )

        # Return S3 URI
        s3_uri = f"s3://{bucket_name}/{s3_key}"
        logger.info(f"Uploaded config file to: {s3_uri}")
        return s3_uri

    except Exception as e:
        raise Exception(f"Failed to upload config file: {e}")


def build_parameters(
    pattern: Optional[str] = None,
    admin_email: Optional[str] = None,
    max_concurrent: Optional[int] = None,
    log_level: Optional[str] = None,
    enable_hitl: Optional[str] = None,
    pattern_config: Optional[str] = None,
    custom_config: Optional[str] = None,
    additional_params: Optional[Dict[str, str]] = None,
    region: Optional[str] = None,
    stack_name: Optional[str] = None,
) -> Dict[str, str]:
    """
    Build CloudFormation parameters dictionary

    Only includes parameters that are explicitly provided. For stack updates,
    CloudFormation will automatically use previous values for parameters not included.

    If custom_config is a local file path, it will be uploaded to S3:
    - For existing stacks: Uses the stack's ConfigurationBucket
    - For new stacks: Creates a temporary bucket

    Args:
        pattern: IDP pattern (pattern-1, pattern-2, pattern-3) - optional for updates
        admin_email: Admin user email - optional for updates
        max_concurrent: Maximum concurrent workflows - optional
        log_level: Logging level - optional
        enable_hitl: Enable HITL (true/false) - optional
        pattern_config: Pattern configuration preset - optional
        custom_config: Custom configuration (local file path or S3 URI) - optional
        additional_params: Additional parameters as dict - optional
        region: AWS region (auto-detected if not provided)
        stack_name: Stack name (helps determine upload bucket for updates)

    Returns:
        Dictionary of parameter key-value pairs (only includes explicitly provided values)
    """
    parameters = {}

    # Only add parameters if explicitly provided
    if admin_email is not None:
        parameters["AdminEmail"] = admin_email

    if pattern is not None:
        # Map pattern names to CloudFormation values
        pattern_map = {
            "pattern-1": "Pattern1 - Packet or Media processing with Bedrock Data Automation (BDA)",
            "pattern-2": "Pattern2 - Packet processing with Textract and Bedrock",
            "pattern-3": "Pattern3 - Packet processing with Textract, SageMaker(UDOP), and Bedrock",
        }
        parameters["IDPPattern"] = pattern_map.get(pattern, pattern)

    if max_concurrent is not None:
        parameters["MaxConcurrentWorkflows"] = str(max_concurrent)

    if log_level is not None:
        parameters["LogLevel"] = log_level

    if enable_hitl is not None:
        parameters["EnableHITL"] = enable_hitl

    # Add pattern-specific configuration (only if provided)
    if pattern_config is not None:
        if pattern == "pattern-1":
            parameters["Pattern1Configuration"] = pattern_config
        elif pattern == "pattern-2":
            parameters["Pattern2Configuration"] = pattern_config
        elif pattern == "pattern-3":
            parameters["Pattern3Configuration"] = pattern_config

    # Handle custom config - support both local files and S3 URIs
    if custom_config:
        if is_local_file_path(custom_config):
            # Local file - need to upload it
            if not region:
                # Auto-detect region from boto3 session
                import boto3

                session = boto3.session.Session()
                region = session.region_name
                if not region:
                    raise ValueError(
                        "Region could not be determined. Please specify --region or configure AWS_DEFAULT_REGION"
                    )

            logger.info(f"Detected local config file: {custom_config}")
            logger.info(f"Using region: {region}")

            # Upload to S3 bucket (stack's ConfigurationBucket if exists, else temp bucket)
            s3_uri = upload_local_config(custom_config, region, stack_name)
            parameters["CustomConfigPath"] = s3_uri

            logger.info(f"Using uploaded config: {s3_uri}")
        else:
            # Already an S3 URI - validate and use
            if not validate_s3_uri(custom_config):
                raise ValueError(
                    f"Invalid S3 URI format: {custom_config}. Expected format: s3://bucket/key"
                )

            parameters["CustomConfigPath"] = custom_config
            logger.info(f"Using S3 config: {custom_config}")

    # Add any additional parameters
    if additional_params:
        parameters.update(additional_params)

    return parameters
