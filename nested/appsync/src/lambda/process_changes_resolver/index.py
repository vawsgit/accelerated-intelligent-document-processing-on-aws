# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0

import json
import os
import boto3
import logging
from datetime import datetime, timezone

# Import IDP Common modules
from idp_common.models import Document, Section, Status
from idp_common.docs_service import create_document_service

logger = logging.getLogger()
logger.setLevel(os.environ.get('LOG_LEVEL', 'INFO'))

# Initialize AWS clients
s3_client = boto3.client('s3')
sqs_client = boto3.client('sqs')

# Environment variables
QUEUE_URL = os.environ.get('QUEUE_URL')

def handler(event, context):
    logger.info(f"ProcessChanges resolver invoked with event: {json.dumps(event)}")
    
    # Add comprehensive error handling
    try:
        # Extract arguments from the GraphQL event
        args = event.get('arguments', {})
        logger.info(f"Arguments received: {json.dumps(args)}")
        
        object_key = args.get('objectKey')
        modified_sections = args.get('modifiedSections', [])
        modified_pages = args.get('modifiedPages', [])
        
        if not object_key:
            logger.error("objectKey is required but not provided")
            return {
                'success': False,
                'message': 'objectKey is required',
                'processingJobId': None
            }
        
        # Allow empty arrays for evaluation-only reprocessing
        # When both are empty, the document will be resubmitted and the existing
        # Lambda skip logic will bypass OCR/Classification/Extraction/Assessment
        # and proceed directly to Summarization and Evaluation steps
        if not modified_sections and not modified_pages:
            logger.info("No section or page modifications - reprocessing for evaluation/summarization only")

        logger.info(f"Processing changes for document: {object_key}")
        logger.info(f"Modified sections: {json.dumps(modified_sections)}")
        logger.info(f"Modified pages: {json.dumps(modified_pages)}")

        # Use DynamoDB service to get the document (only service that supports get_document)
        try:
            dynamodb_service = create_document_service(mode='dynamodb')
            document = dynamodb_service.get_document(object_key)  # Returns Document object directly
            
            if not document:
                raise ValueError(f"Document {object_key} not found")
            
            # Set bucket names from environment variables (fix for null bucket issue)
            input_bucket = os.environ.get('INPUT_BUCKET')
            output_bucket = os.environ.get('OUTPUT_BUCKET')
            document.input_bucket = input_bucket
            document.output_bucket = output_bucket
            logger.info(f"Set document buckets - input_bucket: {input_bucket}, output_bucket: {output_bucket}")
            
            logger.info(f"Found document: {document.id}")
            
            # Mark HITL review as completed when processing changes
            # This handles the case where user edits data and clicks "Process Changes"
            if document.hitl_status and document.hitl_status not in ['Completed', 'Skipped']:
                identity = event.get('identity', {})
                username = identity.get('username', 'system')
                user_email = identity.get('claims', {}).get('email', '')
                
                # Mark all pending sections as completed
                pending_sections = document.hitl_sections_pending or []
                completed_sections = list(document.hitl_sections_completed or [])
                completed_sections.extend(pending_sections)
                
                document.hitl_status = 'Completed'
                document.hitl_sections_pending = []
                document.hitl_sections_completed = completed_sections
                
                # Update review fields in DynamoDB (not in document model)
                # HITLReviewedBy tracks who completed the review via Process Changes
                tracking_table = os.environ.get('TRACKING_TABLE')
                if tracking_table:
                    dynamodb_resource = boto3.resource('dynamodb')
                    table = dynamodb_resource.Table(tracking_table)
                    table.update_item(
                        Key={"PK": f"doc#{object_key}", "SK": "none"},
                        UpdateExpression="SET HITLReviewedBy = :reviewedBy, HITLReviewedByEmail = :reviewedByEmail, HITLCompleted = :completed",
                        ExpressionAttributeValues={
                            ":reviewedBy": username,
                            ":reviewedByEmail": user_email,
                            ":completed": True,
                        },
                    )
                
                logger.info(f"Marked HITL review as completed by {username} via Process Changes")
            
        except Exception as e:
            logger.error(f"Error retrieving document {object_key}: {str(e)}")
            raise ValueError(f"Document {object_key} not found or error retrieving: {str(e)}")

        # Track modified section IDs for selective processing
        modified_section_ids = []
        
        # Process page-level modifications first (if any)
        if modified_pages:
            process_page_changes(document, modified_pages, modified_section_ids)
        
        # Process each section modification
        for modified_section in modified_sections:
            section_id = modified_section['sectionId']
            classification = modified_section['classification']
            page_ids = [int(pid) for pid in modified_section['pageIds']]  # Ensure integer page IDs
            is_new = modified_section.get('isNew', False)
            is_deleted = modified_section.get('isDeleted', False)
            
            if is_deleted:
                # Find section to delete BEFORE removing it
                section_to_delete = None
                for s in document.sections:
                    if s.section_id == section_id:
                        section_to_delete = s
                        break
                
                if section_to_delete:
                    # Clear S3 extraction data before removing section
                    if section_to_delete.extraction_result_uri:
                        clear_extraction_data(section_to_delete.extraction_result_uri)
                        logger.info(f"Cleared extraction data for deleted section: {section_id}")
                    
                    # Remove section from document
                    document.sections = [s for s in document.sections if s.section_id != section_id]
                    logger.info(f"Deleted section: {section_id}")
                else:
                    logger.warning(f"Section {section_id} marked for deletion but not found")
                        
                continue
            
            elif is_new:
                # Create new section (don't search for existing)
                logger.info(f"Creating new section: {section_id}")
                new_section = Section(
                    section_id=section_id,
                    classification=classification,
                    confidence=1.0,
                    page_ids=[str(pid) for pid in page_ids],
                    extraction_result_uri=None,
                    attributes=None,
                    confidence_threshold_alerts=[]
                )
                document.sections.append(new_section)
                
            else:
                # Update existing section
                existing_section = None
                for section in document.sections:
                    if section.section_id == section_id:
                        existing_section = section
                        break
                
                if existing_section:
                    logger.info(f"Updating existing section: {section_id}")
                    existing_section.classification = classification
                    existing_section.page_ids = [str(pid) for pid in page_ids]
                    
                    # Clear extraction data for reprocessing
                    if existing_section.extraction_result_uri:
                        clear_extraction_data(existing_section.extraction_result_uri)
                        existing_section.extraction_result_uri = None
                        existing_section.attributes = None
                    
                    # Clear confidence threshold alerts for modified sections
                    existing_section.confidence_threshold_alerts = []
                    logger.info(f"Cleared confidence alerts for modified section: {section_id}")
                else:
                    logger.warning(f"Section {section_id} marked as update but not found - treating as new")
                    # Treat as new section if not found
                    new_section = Section(
                        section_id=section_id,
                        classification=classification,
                        confidence=1.0,
                        page_ids=[str(pid) for pid in page_ids],
                        extraction_result_uri=None,
                        attributes=None,
                        confidence_threshold_alerts=[]
                    )
                    document.sections.append(new_section)
            
            # Only add to modified list if not deleted
            modified_section_ids.append(section_id)
            
            # Update page classifications to match section classification (only if not deleted)
            for page_id in page_ids:
                page_id_str = str(page_id)
                if page_id_str in document.pages:
                    document.pages[page_id_str].classification = classification
                    logger.info(f"Updated page {page_id} classification to {classification}")

        # Update document status and timing - reset for reprocessing
        current_time = datetime.now(timezone.utc).isoformat()
        document.status = Status.QUEUED
        document.initial_event_time = document.queued_time or current_time
        document.start_time = None
        document.completion_time = None
        document.workflow_execution_arn = None

        # Sort sections by starting page ID
        document.sections.sort(key=lambda s: min([int(pid) for pid in s.page_ids] + [float('inf')]))

        logger.info(f"Document updated with {len(document.sections)} sections and {len(document.pages)} pages")

        # Log uncompressed document for troubleshooting
        uncompressed_document_json = json.dumps(document.to_dict(), default=str)
        logger.info(f"Uncompressed document (size: {len(uncompressed_document_json)} chars): {uncompressed_document_json}")

        # NOTE: We intentionally do NOT write the document back to the database here.
        # The processing pipeline will handle document updates via AppSync as it processes.
        # This avoids race conditions and ensures consistent state management.

        # Compress document before sending to SQS for large document optimization
        working_bucket = os.environ.get('WORKING_BUCKET')
        if working_bucket:
            # Use document compression (always compress with 0KB threshold)
            sqs_message_content = document.serialize_document(working_bucket, "process_changes", logger)
            logger.info(f"Document compressed for SQS (always compress)")
        else:
            # Fallback to direct document dict if no working bucket
            sqs_message_content = document.to_dict()
            logger.warning("No WORKING_BUCKET configured, sending uncompressed document")

        # Log the SQS message for debugging
        message_body = json.dumps(sqs_message_content, default=str)
        logger.info(f"SQS message prepared (size: {len(message_body)} chars)")
        logger.info(f"SQS message content: {message_body}")
        logger.info(f"Modified sections will be reprocessed: {modified_section_ids}")

        if QUEUE_URL:
            response = sqs_client.send_message(
                QueueUrl=QUEUE_URL,
                MessageBody=message_body
            )
            
            logger.info(f"Sent document to SQS queue. MessageId: {response.get('MessageId')}")
            processing_job_id = response.get('MessageId')
        else:
            logger.warning("QUEUE_URL not configured, skipping SQS message")
            processing_job_id = None

        # Use AppSync service for immediate UI status update
        try:
            appsync_service = create_document_service(mode='appsync')
            document.status = Status.QUEUED  # Ensure status is QUEUED for UI
            updated_document = appsync_service.update_document(document)
            logger.info(f"Updated document status to QUEUED in AppSync for immediate UI feedback")
        except Exception as e:
            logger.warning(f"Failed to update document status in AppSync: {str(e)}")
            # Don't fail the entire operation if AppSync update fails

        # Log successful completion
        logger.info(f"Successfully processed changes for {len(modified_sections)} sections")
        
        response = {
            'success': True,
            'message': f'Successfully processed changes for {len(modified_sections)} sections',
            'processingJobId': processing_job_id
        }
        
        logger.info(f"Returning response: {json.dumps(response)}")
        return response

    except Exception as e:
        logger.error(f"Error processing changes: {str(e)}", exc_info=True)
        
        error_response = {
            'success': False,
            'message': f'Error processing changes: {str(e)}',
            'processingJobId': None
        }
        
        logger.error(f"Returning error response: {json.dumps(error_response)}")
        return error_response

def process_page_changes(document, modified_pages, modified_section_ids):
    """
    Process page-level modifications
    
    Args:
        document: Document object to modify
        modified_pages: List of modified page dictionaries
        modified_section_ids: List to track which sections need reprocessing
    """
    for modified_page in modified_pages:
        page_id = modified_page['pageId']
        page_id_str = str(page_id)
        text_modified = modified_page.get('textModified', False)
        class_reset = modified_page.get('classReset', False)
        new_text_uri = modified_page.get('newTextUri')
        new_confidence_uri = modified_page.get('newConfidenceUri')
        
        logger.info(f"Processing page {page_id}: textModified={text_modified}, classReset={class_reset}")
        
        # Check if page exists in document
        if page_id_str not in document.pages:
            logger.warning(f"Page {page_id} not found in document, skipping")
            continue
            
        page = document.pages[page_id_str]
        
        # Handle class reset - removes sections containing this page
        if class_reset:
            logger.info(f"Resetting classification for page {page_id}")
            page.classification = None  # Reset to unclassified
            
            # Find and remove all sections containing this page
            sections_to_remove = []
            for section in document.sections:
                if page_id_str in section.page_ids:
                    sections_to_remove.append(section)
                    logger.info(f"Marking section {section.section_id} for removal (contains page {page_id})")
            
            # Clear extraction data and remove sections
            for section in sections_to_remove:
                if section.extraction_result_uri:
                    clear_extraction_data(section.extraction_result_uri)
                document.sections = [s for s in document.sections if s.section_id != section.section_id]
                logger.info(f"Removed section {section.section_id} due to page {page_id} class reset")
        
        # Handle text modification - clears extraction results but keeps sections
        elif text_modified:
            logger.info(f"Text modified for page {page_id}")
            
            # Update page URIs if provided
            if new_text_uri:
                page.text_uri = new_text_uri
                logger.info(f"Updated text URI for page {page_id}: {new_text_uri}")
            
            if new_confidence_uri:
                page.text_confidence_uri = new_confidence_uri
                logger.info(f"Updated confidence URI for page {page_id}: {new_confidence_uri}")
            
            # Find sections containing this page and clear their extraction results
            for section in document.sections:
                if page_id_str in section.page_ids:
                    if section.extraction_result_uri:
                        clear_extraction_data(section.extraction_result_uri)
                        section.extraction_result_uri = None
                        section.attributes = None
                        logger.info(f"Cleared extraction results for section {section.section_id} (page {page_id} text modified)")
                    
                    # Track section for reprocessing
                    if section.section_id not in modified_section_ids:
                        modified_section_ids.append(section.section_id)

def clear_extraction_data(s3_uri):
    """Clear extraction data from S3"""
    try:
        if not s3_uri or not s3_uri.startswith('s3://'):
            return
            
        # Parse S3 URI
        parts = s3_uri.replace('s3://', '').split('/', 1)
        if len(parts) != 2:
            return
            
        bucket, key = parts
        
        # Delete the object
        s3_client.delete_object(Bucket=bucket, Key=key)
        logger.info(f"Cleared extraction data: {s3_uri}")
        
    except Exception as e:
        logger.warning(f"Failed to clear extraction data {s3_uri}: {str(e)}")