# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0

notes: Criteria validation configuration for healthcare/insurance prior authorization
assessment:
  enabled: true
  validation_enabled: false
criteria_validation:
  model: us.anthropic.claude-3-5-sonnet-20240620-v1:0
  temperature: 0.0
  top_k: 20
  top_p: 0.01
  max_tokens: 4096
  semaphore: 3  # Number of concurrent API calls
  max_chunk_size: 180000  # Maximum tokens per chunk
  token_size: 4  # Average characters per token
  overlap_percentage: 10  # Chunk overlap percentage
  response_prefix: "<response>"
  
  # Main validation prompt
  system_prompt: |
    You are a specialized insurance evaluator tasked with determining the eligibility of insurance coverage based on a patient's user history and a set of criterias. 
    Each evaluation should be supported by precise reasoning, with citations from the user history where applicable.
  
  task_prompt: |
    Consider the patients user history inforamtion provided insided <user_history></user_history> XML tags.

    <user_history>
     <source_filepath>
     {source_filepath}
     </source_filepath>

     <content>
     {content}
     </content>
    </user_history>

    <criteria>
    <criteria_type>
    {criteria_type}
    </criteria_type>

    <question>
    {question}
    </question>
    </criteria>

    <instruction>
    Evaluate the patient's insurance eligibility for each question provided insided <question></question> XML tags  and patients user history information provided inside <user_history></user_history> XML tags. 

    Your Task:

    For each question, provide:

    Decision: Carefully review each requirement in the context of the patient's user history to decide if it is "Pass," "Fail," or "Information Not Found" and select one of the following options:

    {recommendation_options}

    Reasoning: Provide a brief explanation for the decision, highlighting any relevant details or absence of data.
    Citations: When applicable, cite specific sections of the user history (e.g., page numbers, sections, S3 URI) that support your decision.

    Json Response format:
    {{
     "criteria_type" : "question/criteria type mentioned inside <criteria_type></criteria_type> XML tags"
     "source_file" : ["list of source_filepath that supports the recommendation"]
     "question" : "question Description"
     "Recommendation" : "This should be one of the following: Pass, Fail, or Information Not Found"
     "Reasoning" : "Provide a thorough explanation, reasoning, and any citations from the source_file in a Single  paragraph explanation without line breaks"
    }}
    All fields must be included in the JSON response, even if some values are unavailable (leave them as empty strings if necessary).
    Ensure that the output is a valid JSON object and strictly adheres to the format provided above.
    criteria_type must be included as a field within the JSON and not as the primary key.
    The reasoning field must include detailed explanations and citations to support the decision.

    </instruction>

    Follow instructions provided inside <instruction></instruction> XML tags. 
    Provide the output in a Json format inside <response></response> XML tags. Do not include any space after <response> tag and before </response> tag.

  # Criteria definitions nested under criteria_validation
  criteria:
    administration_requirements:
      - "Will the immunotherapy be administered under the supervision of an appropriately trained physician?"
      - "Is the facility equipped to treat anaphylaxis?"
      - "Has the physician determined an appropriate dosage regimen and progression schedule?"
      - "Are there adequate safety protocols in place for infusion reactions?"
    medical_necessity:
      - "Has the patient failed standard treatments?"
      - "Is the proposed treatment medically necessary?"
      - "Does the patient meet the clinical criteria for this treatment?"
    dosage_requirements:
      - "Is the dosage appropriate for the patient's condition?"
      - "Has the physician documented the rationale for the selected dosage?"
      - "Are dosage adjustments based on patient response documented?"
    safety_requirements:
      - "Are safety protocols adequate for the proposed treatment?"
      - "Has the patient been screened for contraindications?"
      - "Are monitoring procedures in place during treatment?"
    
    # Recommendation options under criteria
    recommendation_options: |
      Pass: The requirement criteria are fully met.
      Fail: The requirement is partially met or requires additional information.
      Information Not Found: No relevant data exists in the user history.

# Summary configuration for multiple files
summary:
  system_prompt: |
    You are a specialized patient user insurance auth request evaluator
    You will assist in summarizing important patient user insurance auth request information determining the eligibility of insurance coverage based on a patient's user history and a set of questions/criterias. 
    You will be given a question (<question>) and the initial LLM responses (<initial_response>) that has information for one or more patient user history text documents.
    Your task is to evaluate and identify the important and relevant information from the related information to sufficiently answer the given question/criteria.
  
  task_prompt: |
    <criteria>
    <criteria_type>{criteria_type}</criteria_type>
    <question>{question}</question>
    </criteria

    <initial_response>
    {initial_response}
    </initial_response>

    <instructions>
    Analyze the provided criteria and initial response to generate a consolidated final evaluation, considering all available evidence to reach a single determination for each unique criterion.
    Multiple instances of the same question/criteria may appear within a category, often referencing different source files. 

    {recommendation_options}

    JSON RESPONSE FORMAT 

    {{
     "criteria_type" : "question/criteria type mentioned inside <criteria_type></criteria_type> XML tags"
     "source_file" : ["list of source_filepath that supports the recommendation"]
     "question" : "question Description"
     "Recommendation" : "This should be one of the following: Pass, Fail, or Information Not Found"
     "Reasoning" : "Provide a thorough explanation, reasoning, and any citations from the source_file in a Single paragraph explanation without line breaks"
    }}

    SOURCE FILE RULES:
    1. Include ONLY files that contain relevant information for the question/criteria
    2. If no information is found, source_file should be an empty list []
    3. For 'Fail' recommendations, include files that were reviewed and led to the failure decision
    4. For partial information, include only files containing the relevant documentation
    5. Cite specific evidence from each listed file in the reasoning

    RESPONSE RULES:
    1. Provide a single consolidated evaluation
    2. Include all source files that support the recommendation
    3. Keep reasoning concise and in a single paragraph
    4. Use only basic punctuation (. , : ;)
    5. Avoid special characters or line breaks in reasoning
    6. Base recommendation strictly on documented evidence
    7. All JSON fields must be included
    8. For 'Fail' or partial findings, explicitly state what documentation was missing or insufficient

    EVALUATION STEPS:
    1. Review initial response and evidence
    2. Identify files containing relevant information
    3. Determine final recommendation
          - If evidence clearly shows PASS, mark as PASS
          - If any evidence indicates FAIL, mark as FAIL
          - If mixed FAIL and "Information Not Found" for same criteria, use FAIL
          - Only use "Information Not Found" if no evidence at all is found
    4. List only files with supporting evidence
    5. Summarize reasoning with specific citations
    6. Format response as valid JSON

    </instruction>
    Follow instructions provided inside <instruction></instruction> XML tags. 
    Let's think step by step to analyze the question/criteria.
    Provide the output in a Json format inside <response></response> XML tags.

# Criteria types to validate
criteria_types:
  - administration_requirements
  - medical_necessity
  - dosage_requirements
  - safety_requirements

# S3 bucket configuration
request_bucket: criteria-validation-user-history
request_history_prefix: Prior-Auth
criteria_bucket: criteria-validation-criteria
output_bucket: criteria-validation-output
textract_page_tracker: criteria-validation-textract
cost_report_bucket: criteria-validation-cost-reports

discovery:
  output_format:
    sample_json: |-
      {
          "document_class" : "Form-1040",
          "document_description" : "Brief summary of the document",
          "groups" : [
              {
                  "name" : "PersonalInformation",
                  "description" : "Personal information of Tax payer",
                  "attributeType" : "group",
                  "groupAttributes" : [
                      {
                          "name": "FirstName",
                          "dataType" : "string",
                          "description" : "First Name of Taxpayer"
                      },
                      {
                          "name": "Age",
                          "dataType" : "number",
                          "description" : "Age of Taxpayer"
                      }
                  ]
              },
              {
                  "name" : "Dependents",
                  "description" : "Dependents of taxpayer",
                  "attributeType" : "list",
                  "listItemTemplate": {
                      "itemAttributes" : [
                          {
                              "name": "FirstName",
                              "dataType" : "string",
                              "description" : "Dependent first name"
                          },
                          {
                              "name": "Age",
                              "dataType" : "number",
                              "description" : "Dependent Age"
                          }
                      ]
                  }
              }
          ]
      }
  with_ground_truth:
    top_p: '0.1'
    temperature: '1.0'
    user_prompt: >-
      This image contains unstructured data. Analyze the data line by line using the provided ground truth as reference.                        
      <GROUND_TRUTH_REFERENCE>
      {ground_truth_json}
      </GROUND_TRUTH_REFERENCE>
      Ground truth reference JSON has the fields we are interested in extracting from the document/image. Use the ground truth to optimize field extraction. Match field names, data types, and groupings from the reference.
      Image may contain multiple pages, process all pages.
      Extract all field names including those without values.
      Do not change the group name and field name from ground truth in the extracted data json.
      Add field_description field for every field which will contain instruction to LLM to extract the field data from the image/document. Add data_type field for every field. 
      Add two fields document_class and document_description. 
      For document_class generate a short name based on the document content like W4, I-9, Paystub. 
      For document_description generate a description about the document in less than 50 words.
      If the group repeats and follows table format, update the attributeType as "list".                         
      Do not extract the values.
      Format the extracted data using the below JSON format:
      Format the extracted groups and fields using the below JSON format:

    model_id: us.amazon.nova-pro-v1:0
    system_prompt: >-
      You are an expert in processing forms. Extracting data from images and
      documents. Use provided ground truth data as reference to optimize field
      extraction and ensure consistency with expected document structure and
      field definitions.
    max_tokens: '10000'
  without_ground_truth:
    top_p: '0.1'
    temperature: '1.0'
    user_prompt: >-
      This image contains forms data. Analyze the form line by line.
      Image may contains multiple pages, process all the pages. 
      Form may contain multiple name value pair in one line. 
      Extract all the names in the form including the name value pair which doesn't have value. 
      Organize them into groups, extract field_name, data_type and field description
      Field_name should be less than 60 characters, should not have space use '-' instead of space.
      field_description is a brief description of the field and the location of the field like box number or line number in the form and section of the form.
      Field_name should be unique within the group.
      Add two fields document_class and document_description. 
      For document_class generate a short name based on the document content like W4, I-9, Paystub. 
      For document_description generate a description about the document in less than 50 words. 

      Group the fields based on the section they are grouped in the form. Group should have attributeType as "group".
      If the group repeats and follows table format, update the attributeType as "list".
      Do not extract the values.
      Return the extracted data in JSON format.
      Format the extracted data using the below JSON format:
      Format the extracted groups and fields using the below JSON format:
    model_id: us.amazon.nova-pro-v1:0
    system_prompt: >-
      You are an expert in processing forms. Extracting data from images and
      documents. Analyze forms line by line to identify field names, data types,
      and organizational structure. Focus on creating comprehensive blueprints
      for document processing without extracting actual values.
    max_tokens: '10000'

agents:
  error_analyzer:
    model_id: us.anthropic.claude-sonnet-4-20250514-v1:0

    system_prompt: |-
      You are an intelligent error analysis agent for the GenAI IDP system with access to specialized diagnostic tools.

      GENERAL TROUBLESHOOTING WORKFLOW:
      1. Identify document status from DynamoDB
      2. Find any errors reported during Step Function execution
      3. Collect relevant logs from CloudWatch
      4. Identify any performance issues from X-Ray traces
      5. Provide root cause analysis based on the collected information
      
      TOOL SELECTION STRATEGY:
      - If user provides a filename: Use cloudwatch_document_logs and dynamodb_status for document-specific analysis
      - For system-wide issues: Use cloudwatch_logs and dynamodb_query
      - For execution context: Use lambda_lookup or stepfunction_details
      - For distributed tracing: Use xray_trace or xray_performance_analysis
      
      ALWAYS format your response with exactly these three sections in this order:
      
      ## Root Cause
      Identify the specific underlying technical reason why the error occurred. Focus on the primary cause, not symptoms.

      ## Recommendations
      Provide specific, actionable steps to resolve the issue. Limit to top three recommendations only.

      <details>
      <summary><strong>Evidence</strong></summary>
      
      Format evidence with source information. Include relevant data from tool responses:
      
      **For CloudWatch logs:**
      **Log Group:** [full log_group name]
      **Log Stream:** [full log_stream name]
      ```
      [ERROR] timestamp message
      ```
      
      **For other sources (DynamoDB, Step Functions, X-Ray):**
      **Source:** [service name and resource]
      ```
      Relevant data from tool response
      ```

      </details>

      FORMATTING RULES:
      - Use the exact three-section structure above
      - Make Evidence section collapsible using HTML details tags
      - Include relevant data from all tool responses (CloudWatch, DynamoDB, Step Functions, X-Ray)
      - For CloudWatch: Show complete log group and log stream names without truncation
      - Present evidence data in code blocks with appropriate source labels
        
      ANALYSIS GUIDELINES:
      - Use multiple tools for comprehensive analysis when needed
      - Start with document-specific tools for targeted queries
      - Use system-wide tools for pattern analysis
      - Combine DynamoDB status with CloudWatch logs for complete picture
      - Leverage X-Ray for distributed system issues
      
      ROOT CAUSE DETERMINATION:
      1. Document Status: Check dynamodb_status first
      2. Execution Details: Use stepfunction_details for workflow failures
      3. Log Analysis: Use cloudwatch_document_logs or cloudwatch_logs for error details
      4. Distributed Tracing: Use xray_performance_analysis for service interaction issues
      5. Context: Use lambda_lookup for execution environment
      
      RECOMMENDATION GUIDELINES:
      For code-related issues or system bugs:
      - Do not suggest code modifications
      - Include error details, timestamps, and context

      For configuration-related issues:
      - Direct users to UI configuration panel
      - Specify exact configuration section and parameter names

      For operational issues:
      - Provide immediate troubleshooting steps
      - Include preventive measures

      TIME RANGE PARSING:
      - recent: 1 hour
      - last week: 168 hours  
      - last day: 24 hours
      - No time specified: 24 hours (default)
      
      IMPORTANT: Do not include any search quality reflections, search quality scores, or meta-analysis sections in your response. Only provide the three required sections: Root Cause, Recommendations, and Evidence.
    parameters:
      max_log_events: 5
      time_range_hours_default: 24

  chat_companion:
    model_id: us.anthropic.claude-sonnet-4-20250514-v1:0
# Pricing configuration for cost tracking
pricing:
  - name: textract/detect_document_text
    units:
      - name: first_million_pages
        price: 0.0015
      - name: over_million_pages
        price: 0.0006
  - name: bedrock/us.anthropic.claude-3-5-sonnet-20240620-v1:0
    units:
      - name: inputTokens
        price: 0.000003
      - name: outputTokens
        price: 0.000015
      - name: cacheReadInputTokens
        price: 0.0000003
      - name: cacheWriteInputTokens
        price: 0.00000375
  - name: bedrock/us.amazon.nova-pro-v1:0
    units:
      - name: inputTokens
        price: 0.0000008
      - name: outputTokens
        price: 0.0000032
      - name: cacheReadInputTokens
        price: 0.0000002
  # EU model pricing
  - name: bedrock/eu.amazon.nova-lite-v1:0
    units:
      - name: inputTokens
        price: '7.8E-8'
      - name: outputTokens
        price: '3.1E-7'
      - name: cacheReadInputTokens
        price: '1.9E-8'
      - name: cacheWriteInputTokens
        price: '7.8E-8'
  - name: bedrock/eu.amazon.nova-pro-v1:0
    units:
      - name: inputTokens
        price: '1.0E-6'
      - name: outputTokens
        price: '4.2E-6'
      - name: cacheReadInputTokens
        price: '2.6E-7'
      - name: cacheWriteInputTokens
        price: '1.0E-6'
  - name: bedrock/eu.anthropic.claude-3-haiku-20240307-v1:0
    units:
      - name: inputTokens
        price: '2.5E-7'
      - name: outputTokens
        price: '1.25E-6'
  - name: bedrock/eu.anthropic.claude-haiku-4-5-20251001-v1:0
    units:
      - name: inputTokens
        price: '1.1E-6'
      - name: outputTokens
        price: '5.5E-6'
      - name: cacheReadInputTokens
        price: '1.1E-7'
      - name: cacheWriteInputTokens
        price: '1.4E-6'
  - name: bedrock/eu.anthropic.claude-3-5-sonnet-20241022-v2:0
    units:
      - name: inputTokens
        price: '3.0E-6'
      - name: outputTokens
        price: '1.5E-5'
      - name: cacheReadInputTokens
        price: '3.0E-7'
      - name: cacheWriteInputTokens
        price: '3.75E-6'
  - name: bedrock/eu.anthropic.claude-3-7-sonnet-20250219-v1:0
    units:
      - name: inputTokens
        price: '3.0E-6'
      - name: outputTokens
        price: '1.5E-5'
      - name: cacheReadInputTokens
        price: '3.0E-7'
      - name: cacheWriteInputTokens
        price: '3.75E-6'
  - name: bedrock/eu.anthropic.claude-sonnet-4-20250514-v1:0
    units:
      - name: inputTokens
        price: '3.0E-6'
      - name: outputTokens
        price: '1.5E-5'
      - name: cacheReadInputTokens
        price: '3.0E-7'
      - name: cacheWriteInputTokens
        price: '3.75E-6'
  - name: bedrock/eu.anthropic.claude-sonnet-4-5-20250929-v1:0
    units:
      - name: inputTokens
        price: '3.3E-6'
      - name: outputTokens
        price: '1.65E-5'
      - name: cacheReadInputTokens
        price: '3.3E-7'
      - name: cacheWriteInputTokens
        price: '4.125E-6'
  - name: bedrock/eu.anthropic.claude-sonnet-4-5-20250929-v1:0:1m
    units:
      - name: inputTokens
        price: '6.6E-6'
      - name: outputTokens
        price: '2.475E-5'
      - name: cacheReadInputTokens
        price: '6.6E-7'
      - name: cacheWriteInputTokens
        price: '8.25E-6'
  # AWS Lambda pricing (US East - N. Virginia)
  - name: lambda/requests
    units:
      - name: invocations
        price: '2.0E-7'       # $0.0000002 per request ($0.20 per 1M requests)
  - name: lambda/duration  
    units:
      - name: gb_seconds
        price: '1.66667E-5'   # $0.0000166667 per GB-second ($16.67 per 1M GB-seconds)

