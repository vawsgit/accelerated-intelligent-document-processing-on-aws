notes: Default settings for RealKIE-FCC-Verified dataset - Pattern1 
summarization:
  enabled: false
  model: us.amazon.nova-premier-v1:0
  system_prompt: >-
    You are a document summarization expert who can analyze and summarize
    documents from various domains including medical, financial, legal, and
    general business documents. Your task is to create a summary that captures
    the key information, main points, and important details from the document.
    Your output must be in valid JSON format. \nSummarization Style:
    Balanced\\nCreate a balanced summary that provides a moderate level of
    detail. Include the main points and key supporting information, while
    maintaining the document's overall structure. Aim for a comprehensive yet
    concise summary.\n Your output MUST be in valid JSON format with markdown
    content. You MUST strictly adhere to the output format specified in the
    instructions.
  task_prompt: >-
    <document-text> {DOCUMENT_TEXT} </document-text>

    <extracted-attributes> {EXTRACTION_RESULTS} </extracted-attributes>

    Analyze the provided document (<document-text>) along with the extracted
    attributes (<extracted-attributes>) to create a comprehensive and accurate
    summary.

    CRITICAL INSTRUCTION: You MUST return your response as valid JSON with the
    EXACT structure shown at the end of these instructions. Do not include any
    explanations, notes, or text outside of the JSON structure.

    Create a summary that captures the essential information from the document.
    Your summary should:

    1. **Integrate Extracted Attributes**: Begin with a "Key Information"
    section that highlights the most important extracted attributes in a
    structured format (use tables or lists as appropriate)

    2. **Validate and Reference**: Cross-reference the document text with
    extracted values to ensure accuracy. When mentioning specific values, prefer
    the extracted attributes when they are available

    3. **Maintain Document Structure**: Preserve the original document's
    organizational structure where appropriate, using the extracted attributes
    to enhance each section

    4. **Highlight Critical Data**: Emphasize important extracted values such
    as:
       - Names, addresses, and identification numbers
       - Dates and time periods
       - Monetary amounts and financial figures
       - Status indicators and classifications
       - Any calculated or derived values

    5. **Use Markdown Formatting**: Apply markdown for better readability:
       - Use headers (##, ###) for sections
       - Create tables for structured data from extracted attributes
       - Use **bold** for important values and *italics* for emphasis
       - Create lists (bullet or numbered) for multiple items

    6. **Provide Context**: For each extracted value mentioned, provide brief
    context from the document text to explain its significance

    7. **Citation System**: Cite all facts using inline citations in the format
    [Cite-X, Page-Y] where X is a sequential citation number and Y is the page
    number. Format citations as markdown links: [[Cite-1,
    Page-3]](#cite-1-page-3)

    8. **Data Completeness**: If extracted attributes are missing or empty, note
    this in the summary and rely more heavily on the document text

    9. **References Section**: At the end, include a "References" section
    listing all citations with their exact text from the source document

    Structure your summary as follows: - **Key Information** (from extracted
    attributes) - **Document Overview** - **Detailed Sections** (based on
    document structure) - **Summary and Conclusions** - **References**

    Output Format:

    You MUST return ONLY valid JSON with the following structure and nothing
    else:

    ```json {
      "summary": "A comprehensive summary in markdown format that integrates extracted attributes with document text, includes inline citations linked to a references section at the bottom"
    } ```

    Do not include any text, explanations, or notes outside of this JSON
    structure. The JSON must be properly formatted and parseable.
  temperature: 0
  top_p: 0.1
  top_k: 5
  max_tokens: 4096
agents:
  error_analyzer:
    model_id: us.anthropic.claude-sonnet-4-20250514-v1:0
    error_patterns:
      - ERROR
      - CRITICAL
      - FATAL
      - Exception
      - Traceback
      - Failed
      - Timeout
      - AccessDenied
      - ThrottlingException
    system_prompt: >-
      You are an intelligent error analysis agent for the GenAI IDP system with
      access to specialized diagnostic tools.


      GENERAL TROUBLESHOOTING WORKFLOW:

      1. Identify document status from DynamoDB

      2. Find any errors reported during Step Function execution

      3. Collect relevant logs from CloudWatch

      4. Identify any performance issues from X-Ray traces

      5. Provide root cause analysis based on the collected information


      TOOL SELECTION STRATEGY:

      - If user provides a filename: Use cloudwatch_document_logs and
      dynamodb_status for document-specific analysis

      - For system-wide issues: Use cloudwatch_logs and dynamodb_query

      - For execution context: Use lambda_lookup or stepfunction_details

      - For distributed tracing: Use xray_trace or xray_performance_analysis


      ALWAYS format your response with exactly these three sections in this
      order:


      ## Root Cause

      Identify the specific underlying technical reason why the error occurred.
      Focus on the primary cause, not symptoms.


      ## Recommendations

      Provide specific, actionable steps to resolve the issue. Limit to top
      three recommendations only.


      <details>

      <summary><strong>Evidence</strong></summary>


      Format evidence with source information. Include relevant data from tool
      responses:


      **For CloudWatch logs:**

      **Log Group:** [full log_group name]

      **Log Stream:** [full log_stream name]

      ```

      [ERROR] timestamp message

      ```


      **For other sources (DynamoDB, Step Functions, X-Ray):**

      **Source:** [service name and resource]

      ```

      Relevant data from tool response

      ```


      </details>


      FORMATTING RULES:

      - Use the exact three-section structure above

      - Make Evidence section collapsible using HTML details tags

      - Include relevant data from all tool responses (CloudWatch, DynamoDB,
      Step Functions, X-Ray)

      - For CloudWatch: Show complete log group and log stream names without
      truncation

      - Present evidence data in code blocks with appropriate source labels
        
      ANALYSIS GUIDELINES:

      - Use multiple tools for comprehensive analysis when needed

      - Start with document-specific tools for targeted queries

      - Use system-wide tools for pattern analysis

      - Combine DynamoDB status with CloudWatch logs for complete picture

      - Leverage X-Ray for distributed system issues


      ROOT CAUSE DETERMINATION:

      1. Document Status: Check dynamodb_status first

      2. Execution Details: Use stepfunction_details for workflow failures

      3. Log Analysis: Use cloudwatch_document_logs or cloudwatch_logs for error
      details

      4. Distributed Tracing: Use xray_performance_analysis for service
      interaction issues

      5. Context: Use lambda_lookup for execution environment


      RECOMMENDATION GUIDELINES:

      For code-related issues or system bugs:

      - Do not suggest code modifications

      - Include error details, timestamps, and context


      For configuration-related issues:

      - Direct users to UI configuration panel

      - Specify exact configuration section and parameter names


      For operational issues:

      - Provide immediate troubleshooting steps

      - Include preventive measures


      TIME RANGE PARSING:

      - recent: 1 hour

      - last week: 168 hours  

      - last day: 24 hours

      - No time specified: 24 hours (default)


      IMPORTANT: Do not include any search quality reflections, search quality
      scores, or meta-analysis sections in your response. Only provide the three
      required sections: Root Cause, Recommendations, and Evidence.
    parameters:
      max_log_events: 5
      time_range_hours_default: 24
      max_log_message_length: 400
      max_events_per_log_group: 5
      max_log_groups: 20
      max_stepfunction_timeline_events: 3
      max_stepfunction_error_length: 400
      xray_slow_segment_threshold_ms: 5000
      xray_error_rate_threshold: 0.05
      xray_response_time_threshold_ms: 10000
  chat_companion:
    model_id: us.anthropic.claude-haiku-4-5-20251001-v1:0
    error_patterns:
      - ERROR
      - CRITICAL
      - FATAL
      - Exception
      - Traceback
      - Failed
      - Timeout
      - AccessDenied
      - ThrottlingException
    system_prompt: |2-

                  You are an intelligent error analysis agent for the GenAI IDP system with access to specialized diagnostic tools.

                    GENERAL TROUBLESHOOTING WORKFLOW:
                    1. Identify document status from DynamoDB
                        2. Find any errors reported during Step Function execution
                    3. Collect relevant logs from CloudWatch
                    4. Identify any performance issues from X-Ray traces
                5. Provide root cause analysis based on the collected information
                
                TOOL SELECTION STRATEGY:
                - If user provides a filename: Use cloudwatch_document_logs and dynamodb_status for document-specific analysis
                - For system-wide issues: Use cloudwatch_logs and dynamodb_query
                - For execution context: Use lambda_lookup or stepfunction_details
                - For distributed tracing: Use xray_trace or xray_performance_analysis
                
                ALWAYS format your response with exactly these three sections in this order:
                
                ## Root Cause
                Identify the specific underlying technical reason why the error occurred. Focus on the primary cause, not symptoms.

                ## Recommendations
                    Provide specific, actionable steps to resolve the issue. Limit to top three recommendations only.

                <details>
                    <summary><strong>Evidence</strong></summary>
                    
                    Format evidence with source information. Include relevant data from tool responses:
                    
                    **For CloudWatch logs:**
                        **Log Group:** [full log_group name]
                    **Log Stream:** [full log_stream name]
                        ```
                    [ERROR] timestamp message
                ```
                
                **For other sources (DynamoDB, Step Functions, X-Ray):**
                    **Source:** [service name and resource]
                    ```
                Relevant data from tool response
                    ```

                </details>

                    FORMATTING RULES:
                - Use the exact three-section structure above
                - Make Evidence section collapsible using HTML details tags
                - Include relevant data from all tool responses (CloudWatch, DynamoDB, Step Functions, X-Ray)
                - For CloudWatch: Show complete log group and log stream names without truncation
                - Present evidence data in code blocks with appropriate source labels
                      
                    ANALYSIS GUIDELINES:
                - Use multiple tools for comprehensive analysis when needed
                    - Start with document-specific tools for targeted queries
                    - Use system-wide tools for pattern analysis
                    - Combine DynamoDB status with CloudWatch logs for complete picture
                    - Leverage X-Ray for distributed system issues
                        
                        ROOT CAUSE DETERMINATION:
                        1. Document Status: Check dynamodb_status first
                    2. Execution Details: Use stepfunction_details for workflow failures
                    3. Log Analysis: Use cloudwatch_document_logs or cloudwatch_logs for error details
                    4. Distributed Tracing: Use xray_performance_analysis for service interaction issues
                    5. Context: Use lambda_lookup for execution environment
                    
                    RECOMMENDATION GUIDELINES:
                    For code-related issues or system bugs:
                        - Do not suggest code modifications
                    - Include error details, timestamps, and context

                    For configuration-related issues:
                        - Direct users to UI configuration panel
                            - Specify exact configuration section and parameter names

                            For operational issues:
                            - Provide immediate troubleshooting steps
                            - Include preventive measures

                            TIME RANGE PARSING:
                            - recent: 1 hour
                    - last week: 168 hours  
                            - last day: 24 hours
                            - No time specified: 24 hours (default)
                    
                    IMPORTANT: Do not include any search quality reflections, search quality scores, or meta-analysis sections in your response. Only provide the three required sections: Root Cause, Recommendations, and Evidence.
    parameters:
      max_log_events: 5
      time_range_hours_default: 24
      max_log_message_length: 400
      max_events_per_log_group: 5
      max_log_groups: 20
      max_stepfunction_timeline_events: 3
      max_stepfunction_error_length: 400
      xray_slow_segment_threshold_ms: 5000
      xray_error_rate_threshold: 0.05
      xray_response_time_threshold_ms: 10000
pricing:
  - name: textract/detect_document_text
    units:
      - name: pages
        price: 0.0015
  - name: textract/analyze_document-Layout
    units:
      - name: pages
        price: 0.004
  - name: textract/analyze_document-Signatures
    units:
      - name: pages
        price: 0.0035
  - name: textract/analyze_document-Forms
    units:
      - name: pages
        price: 0.05
  - name: textract/analyze_document-Tables
    units:
      - name: pages
        price: 0.015
  - name: textract/analyze_document-Tables+Forms
    units:
      - name: pages
        price: 0.065
  - name: bedrock/us.amazon.nova-lite-v1:0
    units:
      - name: inputTokens
        price: 6.e-8
      - name: outputTokens
        price: 2.4e-7
      - name: cacheReadInputTokens
        price: 1.5e-8
      - name: cacheWriteInputTokens
        price: 6.e-8
  - name: bedrock/us.amazon.nova-pro-v1:0
    units:
      - name: inputTokens
        price: 8.e-7
      - name: outputTokens
        price: 0.0000032
      - name: cacheReadInputTokens
        price: 2.e-7
      - name: cacheWriteInputTokens
        price: 8.e-7
  - name: bedrock/us.amazon.nova-premier-v1:0
    units:
      - name: inputTokens
        price: 0.0000025
      - name: outputTokens
        price: 0.0000125
  - name: bedrock/us.anthropic.claude-3-haiku-20240307-v1:0
    units:
      - name: inputTokens
        price: 2.5e-7
      - name: outputTokens
        price: 0.00000125
  - name: bedrock/us.anthropic.claude-3-5-haiku-20241022-v1:0
    units:
      - name: inputTokens
        price: 8.e-7
      - name: outputTokens
        price: 0.000004
      - name: cacheReadInputTokens
        price: 8.e-8
      - name: cacheWriteInputTokens
        price: 0.000001
  - name: bedrock/us.anthropic.claude-haiku-4-5-20251001-v1:0
    units:
      - name: inputTokens
        price: 0.0000011
      - name: outputTokens
        price: 0.0000055
      - name: cacheReadInputTokens
        price: 1.1e-7
      - name: cacheWriteInputTokens
        price: 0.0000014
  - name: bedrock/us.anthropic.claude-3-5-sonnet-20241022-v2:0
    units:
      - name: inputTokens
        price: 0.000003
      - name: outputTokens
        price: 0.000015
      - name: cacheReadInputTokens
        price: 3.e-7
      - name: cacheWriteInputTokens
        price: 0.00000375
  - name: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0
    units:
      - name: inputTokens
        price: 0.000003
      - name: outputTokens
        price: 0.000015
      - name: cacheReadInputTokens
        price: 3.e-7
      - name: cacheWriteInputTokens
        price: 0.00000375
  - name: bedrock/us.anthropic.claude-sonnet-4-20250514-v1:0
    units:
      - name: inputTokens
        price: 0.000003
      - name: outputTokens
        price: 0.000015
      - name: cacheReadInputTokens
        price: 3.e-7
      - name: cacheWriteInputTokens
        price: 0.00000375
  - name: bedrock/us.anthropic.claude-sonnet-4-20250514-v1:0:1m
    units:
      - name: inputTokens
        price: 0.000006
      - name: outputTokens
        price: 0.0000225
      - name: cacheReadInputTokens
        price: 6.e-7
      - name: cacheWriteInputTokens
        price: 0.0000075
  - name: bedrock/us.anthropic.claude-sonnet-4-5-20250929-v1:0
    units:
      - name: inputTokens
        price: 0.0000033
      - name: outputTokens
        price: 0.0000165
      - name: cacheReadInputTokens
        price: 3.3e-7
      - name: cacheWriteInputTokens
        price: 0.000004125
  - name: bedrock/us.anthropic.claude-sonnet-4-5-20250929-v1:0:1m
    units:
      - name: inputTokens
        price: 0.0000066
      - name: outputTokens
        price: 0.00002475
      - name: cacheReadInputTokens
        price: 6.6e-7
      - name: cacheWriteInputTokens
        price: 0.00000825
  - name: bedrock/us.anthropic.claude-opus-4-20250514-v1:0
    units:
      - name: inputTokens
        price: 0.000015
      - name: outputTokens
        price: 0.000075
      - name: cacheReadInputTokens
        price: 0.0000015
      - name: cacheWriteInputTokens
        price: 0.00001875
  - name: bedrock/us.anthropic.claude-opus-4-1-20250805-v1:0
    units:
      - name: inputTokens
        price: 0.000015
      - name: outputTokens
        price: 0.000075
      - name: cacheReadInputTokens
        price: 0.0000015
      - name: cacheWriteInputTokens
        price: 0.00001875
  - name: bedrock/eu.amazon.nova-lite-v1:0
    units:
      - name: inputTokens
        price: 7.8e-8
      - name: outputTokens
        price: 3.1e-7
      - name: cacheReadInputTokens
        price: 1.9e-8
      - name: cacheWriteInputTokens
        price: 7.8e-8
  - name: bedrock/eu.amazon.nova-pro-v1:0
    units:
      - name: inputTokens
        price: 0.000001
      - name: outputTokens
        price: 0.0000042
      - name: cacheReadInputTokens
        price: 2.6e-7
      - name: cacheWriteInputTokens
        price: 0.000001
  - name: bedrock/eu.anthropic.claude-3-haiku-20240307-v1:0
    units:
      - name: inputTokens
        price: 2.5e-7
      - name: outputTokens
        price: 0.00000125
  - name: bedrock/eu.anthropic.claude-haiku-4-5-20251001-v1:0
    units:
      - name: inputTokens
        price: 0.0000011
      - name: outputTokens
        price: 0.0000055
      - name: cacheReadInputTokens
        price: 1.1e-7
      - name: cacheWriteInputTokens
        price: 0.0000014
  - name: bedrock/eu.anthropic.claude-3-5-sonnet-20241022-v2:0
    units:
      - name: inputTokens
        price: 0.000003
      - name: outputTokens
        price: 0.000015
      - name: cacheReadInputTokens
        price: 3.e-7
      - name: cacheWriteInputTokens
        price: 0.00000375
  - name: bedrock/eu.anthropic.claude-3-7-sonnet-20250219-v1:0
    units:
      - name: inputTokens
        price: 0.000003
      - name: outputTokens
        price: 0.000015
      - name: cacheReadInputTokens
        price: 3.e-7
      - name: cacheWriteInputTokens
        price: 0.00000375
  - name: bedrock/eu.anthropic.claude-sonnet-4-20250514-v1:0
    units:
      - name: inputTokens
        price: 0.000003
      - name: outputTokens
        price: 0.000015
      - name: cacheReadInputTokens
        price: 3.e-7
      - name: cacheWriteInputTokens
        price: 0.00000375
  - name: bedrock/eu.anthropic.claude-sonnet-4-5-20250929-v1:0
    units:
      - name: inputTokens
        price: 0.0000033
      - name: outputTokens
        price: 0.0000165
      - name: cacheReadInputTokens
        price: 3.3e-7
      - name: cacheWriteInputTokens
        price: 0.000004125
  - name: bedrock/eu.anthropic.claude-sonnet-4-5-20250929-v1:0:1m
    units:
      - name: inputTokens
        price: 0.0000066
      - name: outputTokens
        price: 0.00002475
      - name: cacheReadInputTokens
        price: 6.6e-7
      - name: cacheWriteInputTokens
        price: 0.00000825
  - name: lambda/requests
    units:
      - name: invocations
        price: 2.e-7
  - name: lambda/duration
    units:
      - name: gb_seconds
        price: 0.0000166667
classes:
  - $schema: https://json-schema.org/draft/2020-12/schema
    $defs:
      LineItem:
        type: object
        properties:
          LineItemEndDate:
            default: 'null'
            x-aws-idp-confidence-threshold: '0.8'
            examples:
              - 11/06/2012
            data_type: string
            format: date
            description: End date for each line item (typically in MM/DD/YY format)
            type: string
            x-aws-idp-evaluation-method: LEVENSHTEIN
            x-aws-idp-evaluation-threshold: '0.7'
          LineItemDescription:
            data_type: string
            description: Description of the line item
            type: string
            x-aws-idp-evaluation-method: LEVENSHTEIN
            x-aws-idp-evaluation-threshold: '0.7'
          LineItemStartDate:
            default: 'null'
            x-aws-idp-confidence-threshold: '0.8'
            examples:
              - 11/06/2012
            data_type: string
            format: date
            description: Start date for each line item (typically in MM/DD/YY format)
            type: string
            x-aws-idp-evaluation-method: LEVENSHTEIN
            x-aws-idp-evaluation-threshold: '0.7'
          LineItemDays:
            maxItems: '7'
            x-aws-idp-confidence-threshold: '0.8'
            uniqueItems: true
            description: List of days of the week for the line item
            type: array
            items:
              type: string
              data_type: string
              enum:
                - M
                - T
                - W
                - Th
                - F
                - S
                - Su
            x-aws-idp-evaluation-method: EXACT
            x-aws-idp-evaluation-threshold: '0.7'
          LineItemRate:
            data_type: string
            description: Rate of the line item
            x-aws-idp-confidence-threshold: '0.8'
            type: number
            x-aws-idp-evaluation-method: NUMERIC_EXACT
    description: Invoice document
    type: object
    x-aws-idp-document-type: Invoice
    properties:
      LineItems:
        type: array
        description: List of line items in the invoice
        items:
          $ref: '#/$defs/LineItem'
      Agency:
        x-aws-idp-confidence-threshold: '0.8'
        data_type: string
        description: The advertising agency or station. May be labelled Agency, or Station.
        x-aws-idp-evaluation-weight: '2'
        type: string
        x-aws-idp-evaluation-method: LEVENSHTEIN
        x-aws-idp-evaluation-threshold: '0.7'
      Advertiser:
        x-aws-idp-confidence-threshold: '0.8'
        data_type: string
        description: The political advertiser or campaign purchasing the broadcast time
        x-aws-idp-evaluation-weight: '2'
        type: string
        x-aws-idp-evaluation-method: FUZZY
        x-aws-idp-evaluation-threshold: '0.8'
      GrossTotal:
        data_type: string
        description: >-
          The total gross amount for all line items before any discounts or
          adjustments
        x-aws-idp-evaluation-weight: '2'
        x-aws-idp-confidence-threshold: '0.8'
        type: number
        x-aws-idp-evaluation-method: NUMERIC_EXACT
      PaymentTerms:
        examples:
          - Net 30
        data_type: string
        description: Payment terms
        x-aws-idp-evaluation-weight: '0.2'
        type: string
        x-aws-idp-evaluation-method: FUZZY
        x-aws-idp-evaluation-threshold: '0.7'
      AgencyCommission:
        data_type: string
        description: Agency commission
        x-aws-idp-evaluation-weight: '0.2'
        x-aws-idp-confidence-threshold: '0.8'
        type: number
        x-aws-idp-evaluation-method: NUMERIC_EXACT
      NetAmountDue:
        data_type: string
        description: >-
          The final net amount due after any discounts or adjustments have been
          applied (stored as string with commas)
        x-aws-idp-evaluation-weight: '2'
        x-aws-idp-confidence-threshold: '0.8'
        type: number
        x-aws-idp-evaluation-method: NUMERIC_EXACT
    $id: Invoice
discovery:
  without_ground_truth:
    model_id: us.amazon.nova-pro-v1:0
    system_prompt: >-
      You are an expert in processing forms. Extracting data from images and
      documents. Analyze forms line by line to identify field names, data types,
      and organizational structure. Focus on creating comprehensive blueprints
      for document processing without extracting actual values.
    temperature: 1
    top_p: 0.1
    max_tokens: 10000
    user_prompt: >-
      This image contains forms data. Analyze the form line by line. Image may
      contains multiple pages, process all the pages.  Form may contain multiple
      name value pair in one line.  Extract all the names in the form including
      the name value pair which doesn't have value.  Organize them into groups,
      extract field_name, data_type and field description Field_name should be
      less than 60 characters, should not have space use '-' instead of space.
      field_description is a brief description of the field and the location of
      the field like box number or line number in the form and section of the
      form. Field_name should be unique within the group. Add two fields
      document_class and document_description.  For document_class generate a
      short name based on the document content like W4, I-9, Paystub.  For
      document_description generate a description about the document in less
      than 50 words. 

      Group the fields based on the section they are grouped in the form. Group
      should have attributeType as "group". If the group repeats and follows
      table format, update the attributeType as "list". Do not extract the
      values. Return the extracted data in JSON format. Format the extracted
      data using the below JSON format: Format the extracted groups and fields
      using the below JSON format:
  with_ground_truth:
    model_id: us.amazon.nova-pro-v1:0
    system_prompt: >-
      You are an expert in processing forms. Extracting data from images and
      documents. Use provided ground truth data as reference to optimize field
      extraction and ensure consistency with expected document structure and
      field definitions.
    temperature: 1
    top_p: 0.1
    max_tokens: 10000
    user_prompt: >-
      This image contains unstructured data. Analyze the data line by line using
      the provided ground truth as reference.                        
      <GROUND_TRUTH_REFERENCE> {ground_truth_json} </GROUND_TRUTH_REFERENCE>
      Ground truth reference JSON has the fields we are interested in extracting
      from the document/image. Use the ground truth to optimize field
      extraction. Match field names, data types, and groupings from the
      reference. Image may contain multiple pages, process all pages. Extract
      all field names including those without values. Do not change the group
      name and field name from ground truth in the extracted data json. Add
      field_description field for every field which will contain instruction to
      LLM to extract the field data from the image/document. Add data_type field
      for every field.  Add two fields document_class and document_description. 
      For document_class generate a short name based on the document content
      like W4, I-9, Paystub.  For document_description generate a description
      about the document in less than 50 words. If the group repeats and follows
      table format, update the attributeType as "list".                         
      Do not extract the values. Format the extracted data using the below JSON
      format: Format the extracted groups and fields using the below JSON
      format:
evaluation:
  enabled: true
  llm_method:
    top_p: 0.1
    max_tokens: 4096
    top_k: 5
    task_prompt: >-
      I need to evaluate attribute extraction for a document of class:
      {DOCUMENT_CLASS}.


      For the attribute named "{ATTRIBUTE_NAME}" described as
      "{ATTRIBUTE_DESCRIPTION}":

      - Expected value: {EXPECTED_VALUE}

      - Actual value: {ACTUAL_VALUE}


      Do these values match in meaning, taking into account formatting
      differences, word order, abbreviations, and semantic equivalence?

      Provide your assessment as a JSON with three fields:

      - "match": boolean (true if they match, false if not)

      - "score": number between 0 and 1 representing the confidence/similarity
      score

      - "reason": brief explanation of your decision


      Respond ONLY with the JSON and nothing else. Here's the exact format:

      {
        "match": true or false,
        "score": 0.0 to 1.0,
        "reason": "Your explanation here"
      }
    temperature: 0
    model: us.anthropic.claude-3-haiku-20240307-v1:0
    system_prompt: >-
      You are an evaluator that helps determine if the predicted and expected
      values match for document attribute extraction. You will consider the
      context and meaning rather than just exact string matching.
summary: null
criteria_types: null
request_bucket: null
request_history_prefix: null
criteria_bucket: null
output_bucket: null
textract_page_tracker: null
cost_report_bucket: null
