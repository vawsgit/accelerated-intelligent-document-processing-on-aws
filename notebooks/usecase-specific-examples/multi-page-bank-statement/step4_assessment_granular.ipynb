{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Assessment (Granular)\n",
    "\n",
    "This notebook demonstrates the **granular assessment** approach for evaluating extraction confidence using AWS Bedrock.\n",
    "\n",
    "**Key Features:**\n",
    "- Multiple focused inferences instead of single large inference\n",
    "- Prompt caching for cost optimization\n",
    "- Parallel processing for reduced latency\n",
    "- Better handling of complex documents with many attributes\n",
    "\n",
    "**Inputs:**\n",
    "- Document object with extraction results from Step 3\n",
    "- Granular assessment configuration\n",
    "- Document classes with confidence thresholds\n",
    "\n",
    "**Outputs:**\n",
    "- Document with enhanced assessment results\n",
    "- Detailed confidence scores and reasoning for each attribute\n",
    "- Performance metrics showing granular processing benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Previous Step Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "# Import IDP libraries\n",
    "from idp_common.models import Document, Status\n",
    "from idp_common import assessment\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger('idp_common.assessment.granular_service').setLevel(logging.INFO)\n",
    "logging.getLogger('idp_common.bedrock.client').setLevel(logging.INFO)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(\"Granular assessment logging enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document from previous step\n",
    "extraction_data_dir = Path(\".data/step3_extraction\")\n",
    "\n",
    "# Load document object from JSON\n",
    "document_path = extraction_data_dir / \"document.json\"\n",
    "with open(document_path, 'r') as f:\n",
    "    document = Document.from_json(f.read())\n",
    "\n",
    "# Load configuration directly from config files\n",
    "import yaml\n",
    "config_dir = Path(\"config\")\n",
    "CONFIG = {}\n",
    "\n",
    "# Load each configuration file\n",
    "config_files = [\n",
    "    \"assessment_granular.yaml\",  # Use granular config\n",
    "    \"classes.yaml\"\n",
    "]\n",
    "\n",
    "for config_file in config_files:\n",
    "    config_path = config_dir / config_file\n",
    "    if config_path.exists():\n",
    "        with open(config_path, 'r') as f:\n",
    "            file_config = yaml.safe_load(f)\n",
    "            CONFIG.update(file_config)\n",
    "        print(f\"Loaded {config_file}\")\n",
    "    else:\n",
    "        print(f\"Warning: {config_file} not found\")\n",
    "\n",
    "# Load environment info\n",
    "env_path = extraction_data_dir / \"environment.json\"\n",
    "with open(env_path, 'r') as f:\n",
    "    env_info = json.load(f)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['AWS_REGION'] = env_info['region']\n",
    "os.environ['METRIC_NAMESPACE'] = 'IDP-Modular-Pipeline'\n",
    "\n",
    "print(f\"Loaded document: {document.id}\")\n",
    "print(f\"Document status: {document.status.value}\")\n",
    "print(f\"Number of sections: {len(document.sections) if document.sections else 0}\")\n",
    "print(f\"Loaded configuration sections: {list(CONFIG.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Granular Assessment Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract assessment configuration\n",
    "assessment_config = CONFIG.get('assessment', {})\n",
    "granular_config = assessment_config.get('granular', {})\n",
    "\n",
    "print(\"=== Assessment Configuration ===\")\n",
    "print(f\"Model: {assessment_config.get('model')}\")\n",
    "print(f\"Temperature: {assessment_config.get('temperature')}\")\n",
    "print(f\"Max Tokens: {assessment_config.get('max_tokens')}\")\n",
    "print(f\"Default Confidence Threshold: {assessment_config.get('default_confidence_threshold')}\")\n",
    "\n",
    "print(\"\\n=== Granular Configuration ===\")\n",
    "print(f\"Enabled: {granular_config.get('enabled', False)}\")\n",
    "print(f\"Max Workers: {granular_config.get('max_workers', 4)}\")\n",
    "print(f\"Simple Batch Size: {granular_config.get('simple_batch_size', 3)}\")\n",
    "print(f\"List Batch Size: {granular_config.get('list_batch_size', 1)}\")\n",
    "print(f\"Enable Caching: {granular_config.get('enable_caching', True)}\")\n",
    "print(f\"Enable Parallel: {granular_config.get('enable_parallel', True)}\")\n",
    "\n",
    "print(\"\\n\" + \"*\"*50)\n",
    "print(f\"System Prompt:\\n{assessment_config.get('system_prompt')}\")\n",
    "print(\"*\"*50)\n",
    "print(f\"Task Prompt (first 500 chars):\\n{assessment_config.get('task_prompt', '')[:500]}...\")\n",
    "print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display document classes with confidence thresholds\n",
     "classes = CONFIG.get('classes', [])\n",
     "print(f\"\\nDocument Classes with Confidence Thresholds:\")\n",
     "for cls in classes:\n",
     "    # Get class name from JSON Schema format or legacy format\n",
     "    class_name = cls.get('x-aws-idp-document-type') or cls.get('$id') or cls.get('name', 'Unknown')\n",
     "    print(f\"\\n{class_name}:\")\n",
     "    \n",
     "    # JSON Schema format uses 'properties' instead of 'attributes'\n",
     "    properties = cls.get('properties', {})\n",
     "    if not properties:\n",
     "        # Fallback to legacy format for backwards compatibility\n",
     "        properties = {attr['name']: attr for attr in cls.get('attributes', [])}\n",
     "    \n",
     "    # Show first 5 properties\n",
     "    for idx, (attr_name, attr_schema) in enumerate(list(properties.items())[:5]):\n",
     "        threshold = attr_schema.get('x-aws-idp-confidence-threshold', 'default')\n",
     "        attr_type = attr_schema.get('type', 'string')\n",
     "        print(f\"  - {attr_name} ({attr_type}): threshold = {threshold}\")\n",
     "        \n",
     "        # Show nested properties for objects and arrays\n",
     "        if attr_type == 'object' and 'properties' in attr_schema:\n",
     "            for group_attr_name, group_attr in list(attr_schema['properties'].items())[:3]:\n",
     "                group_threshold = group_attr.get('x-aws-idp-confidence-threshold', 'default')\n",
     "                print(f\"    â€¢ {group_attr_name}: {group_threshold}\")\n",
     "        elif attr_type == 'array' and 'items' in attr_schema:\n",
     "            item_schema = attr_schema['items']\n",
     "            if item_schema.get('type') == 'object' and 'properties' in item_schema:\n",
     "                for item_attr_name, item_attr in list(item_schema['properties'].items())[:3]:\n",
     "                    item_threshold = item_attr.get('x-aws-idp-confidence-threshold', 'default')\n",
     "                    print(f\"    â€¢ {item_attr_name}: {item_threshold}\")\n",
     "                \n",
     "    if len(properties) > 5:\n",
     "        print(f\"  ... and {len(properties) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create assessment service - will automatically use granular if enabled\n",
    "assessment_service = assessment.AssessmentService(config=CONFIG)\n",
    "\n",
    "print(f\"Assessment service initialized: {type(assessment_service._service).__name__}\")\n",
    "print(f\"Service type: {'Granular' if 'Granular' in type(assessment_service._service).__name__ else 'Original'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assess Extraction Results with Granular Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to parse S3 URIs and load JSON\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "def load_json_from_s3(uri):\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket, key = parse_s3_uri(uri)\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    return json.loads(content)\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Assessing extraction confidence using granular approach...\")\n",
    "\n",
    "if not document.sections:\n",
    "    print(\"No sections found in document. Cannot proceed with assessment.\")\n",
    "else:\n",
    "    assessment_results = []\n",
    "    \n",
    "    # Process each section that has extraction results (limit to first 2 to save time)\n",
    "    sections_with_extractions = [s for s in document.sections if hasattr(s, 'extraction_result_uri') and s.extraction_result_uri]\n",
    "    n = min(2, len(sections_with_extractions))\n",
    "    \n",
    "    print(f\"Found {len(sections_with_extractions)} sections with extraction results\")\n",
    "    print(f\"Processing first {n} sections for granular assessment...\")\n",
    "    \n",
    "    for i, section in enumerate(sections_with_extractions[:n]):\n",
    "        print(f\"\\n--- Granular Assessment: Section {i+1}/{n} ---\")\n",
    "        print(f\"Section ID: {section.section_id}\")\n",
    "        print(f\"Classification: {section.classification}\")\n",
    "        print(f\"Extraction Result URI: {section.extraction_result_uri}\")\n",
    "        \n",
    "        # Load extraction results to show what will be assessed\n",
    "        try:\n",
    "            extraction_data = load_json_from_s3(section.extraction_result_uri)\n",
    "            extraction_results = extraction_data.get('inference_result', {})\n",
    "            print(f\"Attributes to assess: {list(extraction_results.keys())}\")\n",
    "            \n",
    "            # Show list attribute sizes\n",
    "            for attr_name, attr_value in extraction_results.items():\n",
    "                if isinstance(attr_value, list):\n",
    "                    print(f\"  - {attr_name}: {len(attr_value)} items\")\n",
    "                elif isinstance(attr_value, dict):\n",
    "                    print(f\"  - {attr_name}: {len(attr_value)} sub-attributes\")\n",
    "                else:\n",
    "                    print(f\"  - {attr_name}: simple value\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not preview extraction results: {e}\")\n",
    "        \n",
    "        # Process section assessment\n",
    "        start_time = time.time()\n",
    "        document = assessment_service.process_document_section(\n",
    "            document=document,\n",
    "            section_id=section.section_id\n",
    "        )\n",
    "        assessment_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Granular assessment completed in {assessment_time:.2f} seconds\")\n",
    "        \n",
    "        # Load updated results to show granular metadata\n",
    "        try:\n",
    "            updated_extraction_data = load_json_from_s3(section.extraction_result_uri)\n",
    "            metadata = updated_extraction_data.get('metadata', {})\n",
    "            \n",
    "            if metadata.get('granular_assessment_used'):\n",
    "                print(f\"âœ… Granular assessment confirmed\")\n",
    "                print(f\"ðŸ“Š Tasks created: {metadata.get('assessment_tasks_total', 'N/A')}\")\n",
    "                print(f\"âœ… Tasks successful: {metadata.get('assessment_tasks_successful', 'N/A')}\")\n",
    "                print(f\"âŒ Tasks failed: {metadata.get('assessment_tasks_failed', 'N/A')}\")\n",
    "                print(f\"â±ï¸  Assessment time: {metadata.get('assessment_time_seconds', 'N/A'):.2f}s\")\n",
    "            else:\n",
    "                print(\"âš ï¸  Original assessment used (granular not enabled)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load metadata: {e}\")\n",
    "        \n",
    "        # Record results\n",
    "        assessment_results.append({\n",
    "            'section_id': section.section_id,\n",
    "            'classification': section.classification,\n",
    "            'processing_time': assessment_time,\n",
    "            'extraction_result_uri': section.extraction_result_uri\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nGranular assessment complete for {n} sections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display Granular Assessment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_assessment_data(data, attr_name=\"\", indent=\"  \"):\n",
    "    \"\"\"\n",
    "    Recursively display assessment data supporting simple, group, and list attributes.\n",
    "    Enhanced to show confidence thresholds from granular assessment.\n",
    "    \n",
    "    Args:\n",
    "        data: Assessment data (can be dict with confidence, dict with nested attrs, or list)\n",
    "        attr_name: Name of the current attribute for display\n",
    "        indent: Current indentation level\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        # Check if this is a confidence assessment (has 'confidence' key)\n",
    "        if 'confidence' in data:\n",
    "            confidence = data.get('confidence', 'N/A')\n",
    "            threshold = data.get('confidence_threshold', 'N/A')\n",
    "            reason = data.get('confidence_reason')\n",
    "            \n",
    "            # Color coding based on confidence vs threshold\n",
    "            status = \"âœ…\" if isinstance(confidence, (int, float)) and isinstance(threshold, (int, float)) and confidence >= threshold else \"âš ï¸\"\n",
    "            \n",
    "            print(f\"{indent}{status} {attr_name}: {confidence:.3f} (threshold: {threshold})\")\n",
    "            if reason:\n",
    "                print(f\"{indent}   Reason: {reason}\")\n",
    "        else:\n",
    "            # This is a group attribute - iterate through sub-attributes\n",
    "            print(f\"{indent}{attr_name} (Group):\")\n",
    "            for sub_attr_name, sub_data in data.items():\n",
    "                display_assessment_data(sub_data, sub_attr_name, indent + \"  \")\n",
    "                \n",
    "    elif isinstance(data, list):\n",
    "        # This is a list attribute - display each item\n",
    "        print(f\"{indent}{attr_name} (List - {len(data)} items):\")\n",
    "        for i, item_data in enumerate(data[:3]):  # Show first 3 items\n",
    "            print(f\"{indent}  ðŸ“„ Item {i+1}:\")\n",
    "            if isinstance(item_data, dict):\n",
    "                for item_attr_name, item_assessment in item_data.items():\n",
    "                    display_assessment_data(item_assessment, item_attr_name, indent + \"    \")\n",
    "            else:\n",
    "                print(f\"{indent}    Unexpected item format: {type(item_data)}\")\n",
    "        \n",
    "        if len(data) > 3:\n",
    "            print(f\"{indent}  ... and {len(data) - 3} more items\")\n",
    "    else:\n",
    "        print(f\"{indent}{attr_name}: Unexpected data type {type(data)}\")\n",
    "\n",
    "print(\"Enhanced assessment display helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Granular Assessment Results ===\")\n",
    "\n",
    "if document.sections:\n",
    "    sections_with_extractions = [s for s in document.sections if hasattr(s, 'extraction_result_uri') and s.extraction_result_uri]\n",
    "    n = min(2, len(sections_with_extractions))\n",
    "    \n",
    "    for i, section in enumerate(sections_with_extractions[:n]):\n",
    "        print(f\"\\n--- Section {section.section_id} ({section.classification}) ---\")\n",
    "        \n",
    "        try:\n",
    "            # Load the updated extraction results with assessment\n",
    "            extraction_data = load_json_from_s3(section.extraction_result_uri)\n",
    "            \n",
    "            print(f\"Extraction Result URI: {section.extraction_result_uri}\")\n",
    "            \n",
    "            # Show granular assessment metadata\n",
    "            metadata = extraction_data.get('metadata', {})\n",
    "            if metadata.get('granular_assessment_used'):\n",
    "                print(f\"\\nðŸ“Š Granular Assessment Metrics:\")\n",
    "                print(f\"  â€¢ Total tasks: {metadata.get('assessment_tasks_total', 'N/A')}\")\n",
    "                print(f\"  â€¢ Successful: {metadata.get('assessment_tasks_successful', 'N/A')}\")\n",
    "                print(f\"  â€¢ Failed: {metadata.get('assessment_tasks_failed', 'N/A')}\")\n",
    "                print(f\"  â€¢ Processing time: {metadata.get('assessment_time_seconds', 'N/A'):.2f}s\")\n",
    "                \n",
    "            # Display the assessment results with support for nested structures\n",
    "            explainability_info = extraction_data.get('explainability_info', [])\n",
    "            if explainability_info:\n",
    "                print(\"\\nðŸŽ¯ Assessment Results:\")\n",
    "                # The explainability_info is a list, get the first item which contains the assessments\n",
    "                assessments = explainability_info[0] if isinstance(explainability_info, list) else explainability_info\n",
    "                \n",
    "                for attr_name, assessment_data in assessments.items():\n",
    "                    display_assessment_data(assessment_data, attr_name)\n",
    "            else:\n",
    "                print(\"\\nNo assessment results found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading assessment results: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "else:\n",
    "    print(\"No sections to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Performance Analysis ===\")\n",
    "\n",
    "if document.sections:\n",
    "    sections_with_extractions = [s for s in document.sections if hasattr(s, 'extraction_result_uri') and s.extraction_result_uri]\n",
    "    \n",
    "    total_tasks = 0\n",
    "    total_time = 0\n",
    "    total_attributes = 0\n",
    "    total_list_items = 0\n",
    "    \n",
    "    for section in sections_with_extractions[:2]:\n",
    "        try:\n",
    "            extraction_data = load_json_from_s3(section.extraction_result_uri)\n",
    "            metadata = extraction_data.get('metadata', {})\n",
    "            \n",
    "            if metadata.get('granular_assessment_used'):\n",
    "                tasks = metadata.get('assessment_tasks_total', 0)\n",
    "                time_taken = metadata.get('assessment_time_seconds', 0)\n",
    "                \n",
    "                total_tasks += tasks\n",
    "                total_time += time_taken\n",
    "                \n",
    "                # Count attributes and list items\n",
    "                extraction_results = extraction_data.get('inference_result', {})\n",
    "                for attr_name, attr_value in extraction_results.items():\n",
    "                    total_attributes += 1\n",
    "                    if isinstance(attr_value, list):\n",
    "                        total_list_items += len(attr_value)\n",
    "                \n",
    "                print(f\"\\nSection {section.section_id}:\")\n",
    "                print(f\"  â€¢ Assessment tasks: {tasks}\")\n",
    "                print(f\"  â€¢ Processing time: {time_taken:.2f}s\")\n",
    "                print(f\"  â€¢ Avg time per task: {time_taken/tasks:.3f}s\" if tasks > 0 else \"  â€¢ No tasks\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing section {section.section_id}: {e}\")\n",
    "    \n",
    "    if total_tasks > 0:\n",
    "        print(f\"\\nðŸ“ˆ Overall Performance:\")\n",
    "        print(f\"  â€¢ Total assessment tasks: {total_tasks}\")\n",
    "        print(f\"  â€¢ Total processing time: {total_time:.2f}s\")\n",
    "        print(f\"  â€¢ Average time per task: {total_time/total_tasks:.3f}s\")\n",
    "        print(f\"  â€¢ Total attributes assessed: {total_attributes}\")\n",
    "        print(f\"  â€¢ Total list items assessed: {total_list_items}\")\n",
    "        \n",
    "        print(f\"\\nðŸ’¡ Granular Benefits:\")\n",
    "        print(f\"  â€¢ Focused assessments: Each task handles 1-3 attributes\")\n",
    "        print(f\"  â€¢ Parallel processing: Multiple tasks run concurrently\")\n",
    "        print(f\"  â€¢ Prompt caching: Reduces token costs by 80-90%\")\n",
    "        print(f\"  â€¢ Better accuracy: Smaller prompts = better LLM performance\")\n",
    "else:\n",
    "    print(\"No sections available for performance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results for Next Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory for this step\n",
    "data_dir = Path(\".data/step4_assessment_granular\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save updated document object as JSON\n",
    "document_path = data_dir / \"document.json\"\n",
    "with open(document_path, 'w') as f:\n",
    "    f.write(document.to_json())\n",
    "\n",
    "# Save configuration (pass through)\n",
    "config_path = data_dir / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "# Save environment info (pass through)\n",
    "env_path = data_dir / \"environment.json\"\n",
    "with open(env_path, 'w') as f:\n",
    "    json.dump(env_info, f, indent=2)\n",
    "\n",
    "# Save granular assessment-specific results summary\n",
    "assessment_summary = {\n",
    "    'approach': 'granular',\n",
    "    'model_used': assessment_config.get('model'),\n",
    "    'default_confidence_threshold': assessment_config.get('default_confidence_threshold'),\n",
    "    'granular_config': granular_config,\n",
    "    'sections_assessed': len(assessment_results) if 'assessment_results' in locals() else 0,\n",
    "    'total_sections_with_extractions': len([s for s in (document.sections or []) if hasattr(s, 'extraction_result_uri') and s.extraction_result_uri]),\n",
    "    'assessment_results': assessment_results if 'assessment_results' in locals() else [],\n",
    "    'sections_status': [\n",
    "        {\n",
    "            'section_id': section.section_id,\n",
    "            'classification': section.classification,\n",
    "            'has_extraction': hasattr(section, 'extraction_result_uri') and section.extraction_result_uri is not None,\n",
    "            'extraction_result_uri': getattr(section, 'extraction_result_uri', None)\n",
    "        } for section in (document.sections or [])\n",
    "    ]\n",
    "}\n",
    "\n",
    "assessment_summary_path = data_dir / \"assessment_summary.json\"\n",
    "with open(assessment_summary_path, 'w') as f:\n",
    "    json.dump(assessment_summary, f, indent=2)\n",
    "\n",
    "print(f\"Saved document to: {document_path}\")\n",
    "print(f\"Saved configuration to: {config_path}\")\n",
    "print(f\"Saved environment info to: {env_path}\")\n",
    "print(f\"Saved granular assessment summary to: {assessment_summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_assessed = len(assessment_results) if 'assessment_results' in locals() else 0\n",
    "sections_with_extractions = len([s for s in (document.sections or []) if hasattr(s, 'extraction_result_uri') and s.extraction_result_uri])\n",
    "\n",
    "print(\"=== Step 4: Granular Assessment Complete ===\")\n",
    "print(f\"âœ… Document processed: {document.id}\")\n",
    "print(f\"âœ… Sections assessed: {sections_assessed} of {sections_with_extractions} with extractions\")\n",
    "print(f\"âœ… Total sections: {len(document.sections) if document.sections else 0}\")\n",
    "print(f\"âœ… Model used: {assessment_config.get('model')}\")\n",
    "print(f\"âœ… Default threshold: {assessment_config.get('default_confidence_threshold')}\")\n",
    "print(f\"âœ… Granular enabled: {granular_config.get('enabled', False)}\")\n",
    "print(f\"âœ… Data saved to: .data/step4_assessment_granular/\")\n",
    "meteringkey = f\"GranularAssessment/bedrock/{assessment_config.get('model')}\"\n",
    "print(f\"âœ… Token usage: {document.metering[meteringkey]}\")\n",
    "print(\"\\nðŸ“Œ Next step: Run step5_summarization.ipynb\")\n",
    "print(\"\\nðŸ“‹ Granular Assessment Features Demonstrated:\")\n",
    "print(\"  â€¢ Multiple focused inferences instead of single large inference\")\n",
    "print(\"  â€¢ Prompt caching for cost optimization (80-90% savings)\")\n",
    "print(\"  â€¢ Parallel processing for reduced latency\")\n",
    "print(\"  â€¢ Enhanced confidence threshold tracking\")\n",
    "print(\"  â€¢ Detailed performance metrics and task breakdown\")\n",
    "print(\"  â€¢ Better handling of complex documents with many attributes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
