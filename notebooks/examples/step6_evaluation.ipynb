{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Evaluation\n",
    "\n",
    "This notebook performs evaluation of the entire IDP pipeline results using the EvaluationService class to assess accuracy and generate comprehensive reports.\n",
    "\n",
    "**Inputs:**\n",
    "- Document object with all processing results from Step 5\n",
    "- Evaluation configuration\n",
    "- Optional ground truth data for accuracy assessment\n",
    "\n",
    "**Outputs:**\n",
    "- Comprehensive evaluation report (Markdown and JSON)\n",
    "- Accuracy metrics for each processing step\n",
    "- Performance analysis and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOTDIR=\"../..\"\n",
    "\n",
    "# First uninstall existing package (to ensure we get the latest version)\n",
    "%pip uninstall -y idp_common\n",
    "\n",
    "# Install the IDP common package with all components in development mode\n",
    "%pip install -q -e \"{ROOTDIR}/lib/idp_common_pkg[dev, all]\"\n",
    "\n",
    "%pip uninstall -y stickler-eval\n",
    "%pip install -q stickler-eval\n",
    "\n",
    "# Check installed version\n",
    "%pip show idp_common | grep -E \"Version|Location\"\n",
    "%pip show stickler-eval | grep -E \"Version|Location\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Previous Step Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Import IDP libraries\n",
    "from idp_common.models import Document, Status, Section\n",
    "from idp_common import evaluation\n",
    "\n",
    "import idp_common\n",
    "print(idp_common.__file__)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger('idp_common.evaluation.service').setLevel(logging.INFO)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document from previous step\n",
    "summarization_data_dir = Path(\".data/step5_summarization\")\n",
    "\n",
    "# Load document object from JSON\n",
    "document_path = summarization_data_dir / \"document.json\"\n",
    "with open(document_path, 'r') as f:\n",
    "    document = Document.from_json(f.read())\n",
    "\n",
    "# Load configuration directly from config files\n",
    "import yaml\n",
    "config_dir = Path(\"config\")\n",
    "CONFIG = {}\n",
    "\n",
    "# Load each configuration file\n",
    "config_files = [\n",
    "    \"evaluation.yaml\",\n",
    "    \"classes.yaml\"\n",
    "]\n",
    "\n",
    "for config_file in config_files:\n",
    "    config_path = config_dir / config_file\n",
    "    if config_path.exists():\n",
    "        with open(config_path, 'r') as f:\n",
    "            file_config = yaml.safe_load(f)\n",
    "            CONFIG.update(file_config)\n",
    "        print(f\"Loaded {config_file}\")\n",
    "    else:\n",
    "        print(f\"Warning: {config_file} not found\")\n",
    "\n",
    "# Load environment info\n",
    "env_path = summarization_data_dir / \"environment.json\"\n",
    "with open(env_path, 'r') as f:\n",
    "    env_info = json.load(f)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['AWS_REGION'] = env_info['region']\n",
    "os.environ['METRIC_NAMESPACE'] = 'IDP-Modular-Pipeline'\n",
    "\n",
    "print(f\"Loaded document: {document.id}\")\n",
    "print(f\"Document status: {document.status.value}\")\n",
    "print(f\"Number of sections: {len(document.sections) if document.sections else 0}\")\n",
    "print(f\"Loaded configuration sections: {list(CONFIG.keys())}\")\n",
    "print(f\"Processing complete - ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to parse S3 URIs and load JSON\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "def load_json_from_s3(uri):\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket, key = parse_s3_uri(uri)\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    return json.loads(content)\n",
    "\n",
    "def create_ground_truth_document(source_document, expected_results_dict):\n",
    "    \"\"\"Creates a ground truth document for evaluation from an existing document and expected results.\n",
    "    \n",
    "    Args:\n",
    "        source_document: The original document to copy structure from\n",
    "        expected_results_dict: Dictionary mapping section IDs to expected attribute values\n",
    "        \n",
    "    Returns:\n",
    "        Document: A document with the same structure but with expected results\n",
    "    \"\"\"\n",
    "    # Create a new document with the same core attributes\n",
    "    ground_truth = Document(\n",
    "        id=source_document.id,\n",
    "        input_bucket=source_document.input_bucket,\n",
    "        input_key=source_document.input_key,\n",
    "        output_bucket=source_document.output_bucket,\n",
    "        status=Status.COMPLETED\n",
    "    )\n",
    "    \n",
    "    # Copy sections and add expected result URIs\n",
    "    for section in source_document.sections:\n",
    "        # Create section with same structure\n",
    "        expected_section = Section(\n",
    "            section_id=section.section_id,\n",
    "            classification=section.classification,\n",
    "            confidence=1.0,\n",
    "            page_ids=section.page_ids.copy(),\n",
    "            extraction_result_uri=section.extraction_result_uri  # Copy the URI from actual document\n",
    "        )\n",
    "        ground_truth.sections.append(expected_section)\n",
    "    \n",
    "    # Copy pages\n",
    "    for page_id, page in source_document.pages.items():\n",
    "        ground_truth.pages[page_id] = page\n",
    "    \n",
    "    # Store expected results to S3 for sections that have extraction results\n",
    "    s3_client = boto3.client('s3')\n",
    "    for section_id, expected_data in expected_results_dict.items():\n",
    "        # Find the section in the document\n",
    "        for section in ground_truth.sections:\n",
    "            if section.section_id == section_id and section.extraction_result_uri:\n",
    "                # Load the original extraction result as template\n",
    "                uri = section.extraction_result_uri\n",
    "                bucket, key = parse_s3_uri(uri)\n",
    "                \n",
    "                try:\n",
    "                    # Get the original result structure\n",
    "                    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "                    result_data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "                    \n",
    "                    # Replace the inference_result with our expected data\n",
    "                    if \"inference_result\" in result_data:\n",
    "                        result_data[\"inference_result\"] = expected_data\n",
    "                    else:\n",
    "                        # Or just replace the entire content if no inference_result key\n",
    "                        result_data = expected_data\n",
    "                    \n",
    "                    # Write back to S3 with a different key for expected values\n",
    "                    expected_key = key.replace(\"/result.json\", \"/expected.json\")\n",
    "                    s3_client.put_object(\n",
    "                        Bucket=bucket,\n",
    "                        Key=expected_key,\n",
    "                        Body=json.dumps(result_data).encode('utf-8')\n",
    "                    )\n",
    "                    \n",
    "                    # Update the section's extraction URI to point to our expected data\n",
    "                    section.extraction_result_uri = f\"s3://{bucket}/{expected_key}\"\n",
    "                    print(f\"Stored expected results for section {section_id} at {section.extraction_result_uri}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error storing expected results for section {section_id}: {e}\")\n",
    "    \n",
    "    return ground_truth\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Evaluation Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract evaluation configuration\n",
    "evaluation_config = CONFIG.get('evaluation', {}).get('llm_method', {})\n",
    "print(\"Evaluation Configuration:\")\n",
    "print(\"Summarization Configuration:\")\n",
    "print(f\"Model: {evaluation_config.get('model')}\")\n",
    "print(f\"Temperature: {evaluation_config.get('temperature')}\")\n",
    "print(f\"Max Tokens: {evaluation_config.get('max_tokens')}\")\n",
    "print(f\"Default Confidence Threshold: {evaluation_config.get('default_confidence_threshold')}\")\n",
    "print(\"*\"*50)\n",
    "print(f\"System Prompt:\\n{evaluation_config.get('system_prompt')}\")\n",
    "print(\"*\"*50)\n",
    "print(f\"Task Prompt:\\n{evaluation_config.get('task_prompt')}\")\n",
    "print(\"*\"*50)\n",
    "\n",
    "# Initialize evaluation service\n",
    "evaluation_service = evaluation.EvaluationService(config=CONFIG)\n",
    "print(\"\\n‚úÖ EvaluationService initialized successfully\")\n",
    "print(f\"Service configured with: {evaluation_config.get('llm_method', {}).get('model', 'default model')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Ground Truth Data (Optional)\n",
    "\n",
    "For demonstration purposes, we'll create sample ground truth data. In a real scenario, you would load actual ground truth data from your test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have extraction results to evaluate\n",
    "sections_with_extractions = [section for section in document.sections if hasattr(section, 'extraction_result_uri') and section.extraction_result_uri]\n",
    "\n",
    "print(f\"Found {len(sections_with_extractions)} sections with extraction results\")\n",
    "\n",
    "# Create sample ground truth data if we have extractions\n",
    "if sections_with_extractions:\n",
    "    # Load actual extraction results to create realistic ground truth\n",
    "    print(\"\\nCreating sample ground truth based on actual extractions...\")\n",
    "    \n",
    "    expected_results = {}\n",
    "    \n",
    "    for section in sections_with_extractions[:3]:  # Limit to first 3 sections\n",
    "        try:\n",
    "            # Load actual extraction result\n",
    "            extraction_data = load_json_from_s3(section.extraction_result_uri)\n",
    "            \n",
    "            if 'inference_result' in extraction_data:\n",
    "                actual_result = extraction_data['inference_result']\n",
    "                \n",
    "                import copy\n",
    "                expected_result = copy.deepcopy(actual_result) if isinstance(actual_result, dict) else {}\n",
    "                \n",
    "                # Create specific targeted variations based on document type\n",
    "                if section.classification == \"Payslip\":\n",
    "                    # Test NUMERIC_EXACT with small difference\n",
    "                    if 'CurrentGrossPay' in expected_result:\n",
    "                        expected_result['CurrentGrossPay'] = 450.00  # Off by ~2.43\n",
    "                        \n",
    "                    # Test nested object comparison (LLM comparator)\n",
    "                    if 'EmployeeName' in expected_result and isinstance(expected_result['EmployeeName'], dict):\n",
    "                        expected_result['EmployeeName']['LastName'] = 'Smith'  # Changed from 'Stiles'\n",
    "                        \n",
    "                    # Test array comparison (LLM comparator for structured lists)\n",
    "                    if 'FederalTaxes' in expected_result and isinstance(expected_result['FederalTaxes'], list):\n",
    "                        if len(expected_result['FederalTaxes']) > 0:\n",
    "                            expected_result['FederalTaxes'][0]['Period'] = '42.00'  # Off by ~1.40\n",
    "                            \n",
    "                elif section.classification == \"US-drivers-licenses\":\n",
    "                    # Test nested object with EXACT fields\n",
    "                    if 'NAME_DETAILS' in expected_result and isinstance(expected_result['NAME_DETAILS'], dict):\n",
    "                        expected_result['NAME_DETAILS']['FirstName'] = 'JON'  # Typo: JOHN ‚Üí JON\n",
    "                        \n",
    "                    # Test nested object in ADDRESS_DETAILS\n",
    "                    if 'ADDRESS_DETAILS' in expected_result and isinstance(expected_result['ADDRESS_DETAILS'], dict):\n",
    "                        expected_result['ADDRESS_DETAILS']['ZipCode'] = '92128'  # Wrong zip\n",
    "                        \n",
    "                elif section.classification == \"Bank-checks\":\n",
    "                    # Test EXACT string comparison\n",
    "                    if 'payee_name' in expected_result:\n",
    "                        expected_result['payee_name'] = 'JOHN J STILES'  # Added middle initial\n",
    "                        \n",
    "                    # Test NUMERIC_EXACT\n",
    "                    if 'dollar_amount' in expected_result:\n",
    "                        expected_result['dollar_amount'] = 292.00  # Off by 0.10\n",
    "                \n",
    "                expected_results[section.section_id] = expected_result\n",
    "                print(f\"  Created ground truth with targeted variations for {section.classification}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error creating ground truth for section {section.section_id}: {e}\")\n",
    "    \n",
    "    print(f\"\\nCreated ground truth for {len(expected_results)} sections\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo extraction results found - skipping ground truth creation\")\n",
    "    expected_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation Using EvaluationService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_service and expected_results:\n",
    "    print(\"=== Running Document Evaluation ===\")\n",
    "    \n",
    "    # Create ground truth document\n",
    "    print(\"Creating ground truth document...\")\n",
    "    expected_document = create_ground_truth_document(document, expected_results)\n",
    "    \n",
    "    # Run evaluation using EvaluationService\n",
    "    print(\"\\nRunning evaluation with EvaluationService...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"CONFIG: {json.dumps(CONFIG)}\")\n",
    "    print(f\"ACTUAL: {document.to_json()}\")\n",
    "    print(f\"EXPECTED: {expected_document.to_json()}\")\n",
    "\n",
    "    try:\n",
    "        document = evaluation_service.evaluate_document(\n",
    "            actual_document=document,\n",
    "            expected_document=expected_document\n",
    "        )\n",
    "        evaluation_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚úÖ Evaluation completed in {evaluation_time:.2f} seconds\")        \n",
    "        evaluation_completed = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running evaluation: {e}\")\n",
    "        evaluation_completed = False\n",
    "        \n",
    "elif not evaluation_service:\n",
    "    print(\"‚ö†Ô∏è EvaluationService not available - skipping evaluation\")\n",
    "    evaluation_completed = False\n",
    "    \n",
    "elif not expected_results:\n",
    "    print(\"‚ö†Ô∏è No ground truth data available - skipping evaluation\")\n",
    "    evaluation_completed = False\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Missing requirements for evaluation\")\n",
    "    evaluation_completed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Display Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_completed and hasattr(document, 'evaluation_report_uri') and document.evaluation_report_uri:\n",
    "    print(\"=== Evaluation Results ===\")\n",
    "\n",
    "    print(f\"üìä Evaluation report URI: {document.evaluation_report_uri}\")\n",
    "    \n",
    "    # Show evaluation result summary\n",
    "    if hasattr(document, 'evaluation_result') and document.evaluation_result:\n",
    "        eval_result = document.evaluation_result\n",
    "        print(f\"\\nüìà Evaluation Summary - Overall:\")\n",
    "        print(f\"{eval_result.overall_metrics}\")\n",
    "        print(f\"\\nüìà Evaluation Summary - Per Section:\")\n",
    "        for section_result in eval_result.section_results:\n",
    "            print(f\"\\nüìà Evaluation Summary - Section {section_result.section_id}:\")\n",
    "            print(f\"{section_result.metrics}\")\n",
    "    else:\n",
    "        print(\"‚ùå No evaluation result found in document\")\n",
    "\n",
    "    try:\n",
    "        # Read the evaluation report from S3\n",
    "        print(\"Reading evaluation report from S3...\")\n",
    "        bucket, key = parse_s3_uri(document.evaluation_report_uri)\n",
    "        s3_client = boto3.client('s3')\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "        report_content = response['Body'].read().decode('utf-8')\n",
    "        \n",
    "        print(f\"üìÑ Successfully loaded report from {document.evaluation_report_uri}\")\n",
    "               \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìã EVALUATION REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Display the markdown report\n",
    "        display(Markdown(report_content))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading evaluation report: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory for this step\n",
    "data_dir = Path(\".data/step6_evaluation\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Update document status to completed\n",
    "document.status = Status.COMPLETED\n",
    "\n",
    "# Save final document object as JSON\n",
    "document_path = data_dir / \"document.json\"\n",
    "with open(document_path, 'w') as f:\n",
    "    f.write(document.to_json())\n",
    "\n",
    "# Save configuration (pass through)\n",
    "config_path = data_dir / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "# Save environment info (pass through)\n",
    "env_path = data_dir / \"environment.json\"\n",
    "with open(env_path, 'w') as f:\n",
    "    json.dump(env_info, f, indent=2)\n",
    "\n",
    "print(f\"Saved final document to: {document_path}\")\n",
    "print(f\"Saved configuration to: {config_path}\")\n",
    "print(f\"Saved environment info to: {env_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Step 6: Evaluation Complete ===\")\n",
    "print(f\"‚úÖ Document processed: {document.id}\")\n",
    "print(f\"‚úÖ Pages processed: {getattr(document, 'num_pages', 0)}\")\n",
    "print(f\"‚úÖ Sections identified: {len(document.sections) if document.sections else 0}\")\n",
    "\n",
    "if 'evaluation_completed' in locals() and evaluation_completed:\n",
    "    print(f\"‚úÖ EvaluationService analysis completed successfully\")\n",
    "    if hasattr(document, 'evaluation_report_uri') and document.evaluation_report_uri:\n",
    "        print(f\"‚úÖ Evaluation report generated: {document.evaluation_report_uri}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Performance analysis completed (no ground truth evaluation)\")\n",
    "\n",
    "print(f\"‚úÖ Results saved to: .data/step6_evaluation/\")\n",
    "\n",
    "print(\"\\n=== üéâ MODULAR IDP PIPELINE COMPLETE! üéâ ===\")\n",
    "print(\"\\nAll steps have been successfully executed:\")\n",
    "print(\"  0Ô∏è‚É£ Setup - Environment and document initialization\")\n",
    "print(\"  1Ô∏è‚É£ OCR - Text and image extraction from PDF\")\n",
    "print(\"  2Ô∏è‚É£ Classification - Document type identification\")\n",
    "print(\"  3Ô∏è‚É£ Extraction - Structured data extraction\")\n",
    "print(\"  4Ô∏è‚É£ Assessment - Confidence evaluation\")\n",
    "print(\"  5Ô∏è‚É£ Summarization - Content summarization\")\n",
    "print(\"  6Ô∏è‚É£ Evaluation - Final analysis and reporting\")\n",
    "\n",
    "print(\"\\nüìä Key Benefits of Modular Approach:\")\n",
    "print(\"  ‚Ä¢ Independent step execution and testing\")\n",
    "print(\"  ‚Ä¢ Modular configuration management\")\n",
    "print(\"  ‚Ä¢ Step-by-step result persistence\")\n",
    "print(\"  ‚Ä¢ Easy experimentation with different configurations\")\n",
    "print(\"  ‚Ä¢ Comprehensive evaluation and reporting\")\n",
    "print(\"  ‚Ä¢ Professional-grade EvaluationService integration\")\n",
    "\n",
    "print(\"\\nüîß Next Steps for Experimentation:\")\n",
    "print(\"  ‚Ä¢ Modify config files to try different models or parameters\")\n",
    "print(\"  ‚Ä¢ Add new document classes in classes.yaml\")\n",
    "print(\"  ‚Ä¢ Run individual steps with different configurations\")\n",
    "print(\"  ‚Ä¢ Compare results across different pipeline runs\")\n",
    "print(\"  ‚Ä¢ Experiment with different confidence thresholds\")\n",
    "print(\"  ‚Ä¢ Provide real ground truth data for accuracy evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiic-idp-accelerator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
