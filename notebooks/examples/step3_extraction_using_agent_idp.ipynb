{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Information Extraction Using Agentic Methods\n",
    "\n",
    "\n",
    "This notebook performs information extraction from classified document sections using AWS Bedrock.\n",
    "\n",
    "This extraction method utilises Strands Agent for a self correcting mechanism working against pydantic models internally to ensure schema adherence.\n",
    "\n",
    "**Important**\n",
    "\n",
    "This method only works with higher intelligence models that support tool use. Recommended model family is Anthropic Claude.\n",
    "OpenAI models should perform well too but they are limited to text input only inside bedrock.\n",
    "For Amazon models you should use Nova Premier.\n",
    "\n",
    "**Inputs:**\n",
    "- Document object with classification results from Step 2\n",
    "- Extraction configuration\n",
    "- Document classes with attributes definition\n",
    "\n",
    "**Outputs:**\n",
    "- Document with extraction results for each section\n",
    "- Structured data extracted based on document class attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Previous Step Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOTDIR=\"../..\"\n",
    "%pip install -q -e \"{ROOTDIR}/lib/idp_common_pkg[agents]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "from idp_common import extraction\n",
    "\n",
    "# Import IDP libraries\n",
    "from idp_common.models import Document, Status\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger('idp_common.extraction').setLevel(logging.INFO)\n",
    "logging.getLogger('idp_common.bedrock.client').setLevel(logging.INFO)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document from previous step\n",
    "classification_data_dir = Path(\".data/step2_classification\")\n",
    "\n",
    "# Load document object from JSON\n",
    "document_path = classification_data_dir / \"document.json\"\n",
    "with open(document_path, 'r') as f:\n",
    "    document = Document.from_json(f.read())\n",
    "\n",
    "# Load configuration directly from config files\n",
    "import yaml\n",
    "\n",
    "config_dir = Path(\"config\")\n",
    "CONFIG = {}\n",
    "\n",
    "# Load each configuration file\n",
    "config_files = [\n",
    "    \"extraction.yaml\",\n",
    "    \"classes.yaml\"\n",
    "]\n",
    "\n",
    "for config_file in config_files:\n",
    "    config_path = config_dir / config_file\n",
    "    if config_path.exists():\n",
    "        with open(config_path, 'r') as f:\n",
    "            file_config = yaml.safe_load(f)\n",
    "            CONFIG.update(file_config)\n",
    "        print(f\"Loaded {config_file}\")\n",
    "    else:\n",
    "        print(f\"Warning: {config_file} not found\")\n",
    "\n",
    "# Load environment info\n",
    "env_path = classification_data_dir / \"environment.json\"\n",
    "with open(env_path, 'r') as f:\n",
    "    env_info = json.load(f)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['AWS_REGION'] = env_info['region']\n",
    "os.environ['METRIC_NAMESPACE'] = 'IDP-Modular-Pipeline'\n",
    "\n",
    "print(f\"Loaded document: {document.id}\")\n",
    "print(f\"Document status: {document.status.value}\")\n",
    "print(f\"Number of sections: {len(document.sections) if document.sections else 0}\")\n",
    "print(f\"Loaded configuration sections: {list(CONFIG.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Extraction Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure Extraction Service - with Agentic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract extraction configuration\n",
    "CONFIG[\"extraction\"][\"agentic\"] = {\"enabled\":True}\n",
    "# Using Sonnet model - recommended for agentic extraction\n",
    "CONFIG[\"extraction\"][\"model\"] = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "# Alternative: CONFIG[\"extraction\"][\"model\"] = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "\n",
    "\n",
    "extraction_config = CONFIG.get('extraction', {})\n",
    "print(\"Extraction Configuration:\")\n",
    "print(f\"Model: {extraction_config.get('model')}\")\n",
    "print(f\"Temperature: {extraction_config.get('temperature')}\")\n",
    "print(f\"Max Tokens: {extraction_config.get('max_tokens')}\")\n",
    "print(\"*\"*50)\n",
    "\n",
    "print(f\"System Prompt:\\n{extraction_config.get('system_prompt')}\")\n",
    "print(\"*\"*50)\n",
    "print(f\"Task Prompt:\\n{extraction_config.get('task_prompt')}\")\n",
    "print(\"*\"*50)\n",
    "\n",
    "# Display available document classes and their attributes\n",
    "classes = CONFIG.get('classes', [])\n",
    "print(\"\\nDocument Classes and Attributes:\")\n",
    "for cls in classes:\n",
    "    print(f\"\\n{cls['$id']} ({len(cls.get('attributes', []))} attributes):\")\n",
    "    for attr in cls.get('attributes', [])[:3]:  # Show first 3 attributes\n",
    "        print(f\"  - {attr['$id']}: {attr['description'][:100]}...\")\n",
    "    if len(cls.get('attributes', [])) > 3:\n",
    "        print(f\"  ... and {len(cls.get('attributes', [])) - 3} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extraction service with Bedrock\n",
    "extraction_service = extraction.ExtractionService(config=CONFIG)\n",
    "\n",
    "print(\"Extraction service initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract Information from Document Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to parse S3 URIs and load JSON\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "def load_json_from_s3(uri):\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket, key = parse_s3_uri(uri)\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    return json.loads(content)\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting information from document sections...\")\n",
    "\n",
    "if not document.sections:\n",
    "    print(\"No sections found in document. Cannot proceed with extraction.\")\n",
    "else:\n",
    "    extraction_results = []\n",
    "    \n",
    "    # Process each section (limit to first 3 to save time in demo)\n",
    "    n = min(3, len(document.sections))\n",
    "    print(f\"Processing first {n} of {len(document.sections)} sections...\")\n",
    "    \n",
    "    for i, section in enumerate(document.sections[:n]):\n",
    "        print(f\"\\n--- Processing Section {i+1}/{n} ---\")\n",
    "        print(f\"Section ID: {section.section_id}\")\n",
    "        print(f\"Classification: {section.classification}\")\n",
    "        print(f\"Pages: {section.page_ids}\")\n",
    "        \n",
    "        # Process section extraction\n",
    "        start_time = time.time()\n",
    "        document = extraction_service.process_document_section(\n",
    "            document=document,\n",
    "            section_id=section.section_id\n",
    "        )\n",
    "        extraction_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Extraction completed in {extraction_time:.2f} seconds\")\n",
    "        \n",
    "        # Record results\n",
    "        extraction_results.append({\n",
    "            'section_id': section.section_id,\n",
    "            'classification': section.classification,\n",
    "            'processing_time': extraction_time,\n",
    "            'extraction_result_uri': getattr(section, 'extraction_result_uri', None)\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nExtraction complete for {n} sections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Display Extraction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Extraction Results ===\")\n",
    "\n",
    "if document.sections:\n",
    "    for i, section in enumerate(document.sections[:n]):\n",
    "        print(f\"\\n--- Section {section.section_id} ({section.classification}) ---\")\n",
    "        \n",
    "        if hasattr(section, 'extraction_result_uri') and section.extraction_result_uri:\n",
    "            try:\n",
    "                # Load extraction results from S3\n",
    "                extraction_data = load_json_from_s3(section.extraction_result_uri)\n",
    "                \n",
    "                print(f\"Extraction Result URI: {section.extraction_result_uri}\")\n",
    "                \n",
    "                # Display inference results\n",
    "                if 'inference_result' in extraction_data:\n",
    "                    inference_result = extraction_data['inference_result']\n",
    "                    print(\"Extracted Data:\")\n",
    "                    for attr_name, attr_value in inference_result.items():\n",
    "                        if attr_value is not None:\n",
    "                            # Truncate long values for display\n",
    "                            display_value = str(attr_value)[:1000] + \"...\" if len(str(attr_value)) > 1000 else attr_value\n",
    "                            print(f\"  {attr_name}: {display_value}\")\n",
    "                        else:\n",
    "                            print(f\"  {attr_name}: null\")\n",
    "                else:\n",
    "                    print(\"No inference results found\")\n",
    "                    \n",
    "                # Display metadata if available\n",
    "                if 'metadata' in extraction_data:\n",
    "                    metadata = extraction_data['metadata']\n",
    "                    print(f\"Processing time: {metadata.get('extraction_time_seconds', 'N/A')} seconds\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading extraction results: {e}\")\n",
    "        else:\n",
    "            print(\"No extraction results available\")\n",
    "else:\n",
    "    print(\"No sections to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save Results for Next Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory for this step\n",
    "data_dir = Path(\".data/step3_extraction\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save updated document object as JSON\n",
    "document_path = data_dir / \"document.json\"\n",
    "with open(document_path, 'w') as f:\n",
    "    f.write(document.to_json())\n",
    "\n",
    "# Save configuration (pass through)\n",
    "config_path = data_dir / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "# Save environment info (pass through)\n",
    "env_path = data_dir / \"environment.json\"\n",
    "with open(env_path, 'w') as f:\n",
    "    json.dump(env_info, f, indent=2)\n",
    "\n",
    "# Save extraction-specific results summary\n",
    "extraction_summary = {\n",
    "    'model_used': extraction_config.get('model'),\n",
    "    'sections_processed': len(extraction_results) if 'extraction_results' in locals() else 0,\n",
    "    'total_sections': len(document.sections) if document.sections else 0,\n",
    "    'section_results': extraction_results if 'extraction_results' in locals() else [],\n",
    "    'sections_with_extractions': [\n",
    "        {\n",
    "            'section_id': section.section_id,\n",
    "            'classification': section.classification,\n",
    "            'extraction_result_uri': getattr(section, 'extraction_result_uri', None),\n",
    "            'has_results': hasattr(section, 'extraction_result_uri') and section.extraction_result_uri is not None\n",
    "        } for section in (document.sections or [])\n",
    "    ]\n",
    "}\n",
    "\n",
    "extraction_summary_path = data_dir / \"extraction_summary.json\"\n",
    "with open(extraction_summary_path, 'w') as f:\n",
    "    json.dump(extraction_summary, f, indent=2)\n",
    "\n",
    "print(f\"Saved document to: {document_path}\")\n",
    "print(f\"Saved configuration to: {config_path}\")\n",
    "print(f\"Saved environment info to: {env_path}\")\n",
    "print(f\"Saved extraction summary to: {extraction_summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_processed = len(extraction_results) if 'extraction_results' in locals() else 0\n",
    "sections_with_results = sum(1 for section in (document.sections or []) if hasattr(section, 'extraction_result_uri') and section.extraction_result_uri)\n",
    "\n",
    "print(\"=== Step 3: Extraction Complete ===\")\n",
    "print(f\"‚úÖ Document processed: {document.id}\")\n",
    "print(f\"‚úÖ Sections processed: {sections_processed} of {len(document.sections) if document.sections else 0}\")\n",
    "print(f\"‚úÖ Sections with results: {sections_with_results}\")\n",
    "print(f\"‚úÖ Model used: {extraction_config.get('model')}\")\n",
    "print(\"‚úÖ Data saved to: .data/step3_extraction/\")\n",
    "print(\"\\nüìå Next step: Run step4_assessment.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure Extraction without Agentic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract extraction configuration\n",
    "CONFIG[\"extraction\"][\"agentic\"] = {\"enabled\":False}\n",
    "# For traditional extraction, can use simpler models\n",
    "CONFIG[\"extraction\"][\"model\"] = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "\n",
    "\n",
    "extraction_config = CONFIG.get('extraction', {})\n",
    "print(\"Extraction Configuration:\")\n",
    "print(f\"Model: {extraction_config.get('model')}\")\n",
    "print(f\"Temperature: {extraction_config.get('temperature')}\")\n",
    "print(f\"Max Tokens: {extraction_config.get('max_tokens')}\")\n",
    "print(\"*\"*50)\n",
    "\n",
    "print(f\"System Prompt:\\n{extraction_config.get('system_prompt')}\")\n",
    "print(\"*\"*50)\n",
    "print(f\"Task Prompt:\\n{extraction_config.get('task_prompt')}\")\n",
    "print(\"*\"*50)\n",
    "\n",
    "# Display available document classes and their attributes\n",
    "classes = CONFIG.get('classes', [])\n",
    "print(\"\\nDocument Classes and Attributes:\")\n",
    "for cls in classes:\n",
    "    print(f\"\\n{cls['name']} ({len(cls.get('attributes', []))} attributes):\")\n",
    "    for attr in cls.get('attributes', [])[:3]:  # Show first 3 attributes\n",
    "        print(f\"  - {attr['name']}: {attr['description'][:100]}...\")\n",
    "    if len(cls.get('attributes', [])) > 3:\n",
    "        print(f\"  ... and {len(cls.get('attributes', [])) - 3} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extraction service with Bedrock\n",
    "extraction_service = extraction.ExtractionService(config=CONFIG)\n",
    "\n",
    "print(\"Extraction service initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract Information from Document Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to parse S3 URIs and load JSON\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "def load_json_from_s3(uri):\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket, key = parse_s3_uri(uri)\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    return json.loads(content)\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting information from document sections...\")\n",
    "\n",
    "if not document.sections:\n",
    "    print(\"No sections found in document. Cannot proceed with extraction.\")\n",
    "else:\n",
    "    extraction_results = []\n",
    "    \n",
    "    # Process each section (limit to first 3 to save time in demo)\n",
    "    n = min(3, len(document.sections))\n",
    "    print(f\"Processing first {n} of {len(document.sections)} sections...\")\n",
    "    \n",
    "    for i, section in enumerate(document.sections[:n]):\n",
    "        print(f\"\\n--- Processing Section {i+1}/{n} ---\")\n",
    "        print(f\"Section ID: {section.section_id}\")\n",
    "        print(f\"Classification: {section.classification}\")\n",
    "        print(f\"Pages: {section.page_ids}\")\n",
    "        \n",
    "        # Process section extraction\n",
    "        start_time = time.time()\n",
    "        document = extraction_service.process_document_section(\n",
    "            document=document,\n",
    "            section_id=section.section_id\n",
    "        )\n",
    "        extraction_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Extraction completed in {extraction_time:.2f} seconds\")\n",
    "        \n",
    "        # Record results\n",
    "        extraction_results.append({\n",
    "            'section_id': section.section_id,\n",
    "            'classification': section.classification,\n",
    "            'processing_time': extraction_time,\n",
    "            'extraction_result_uri': getattr(section, 'extraction_result_uri', None)\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nExtraction complete for {n} sections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Display Extraction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Extraction Results ===\")\n",
    "\n",
    "if document.sections:\n",
    "    for i, section in enumerate(document.sections[:n]):\n",
    "        print(f\"\\n--- Section {section.section_id} ({section.classification}) ---\")\n",
    "        \n",
    "        if hasattr(section, 'extraction_result_uri') and section.extraction_result_uri:\n",
    "            try:\n",
    "                # Load extraction results from S3\n",
    "                extraction_data = load_json_from_s3(section.extraction_result_uri)\n",
    "                \n",
    "                print(f\"Extraction Result URI: {section.extraction_result_uri}\")\n",
    "                \n",
    "                # Display inference results\n",
    "                if 'inference_result' in extraction_data:\n",
    "                    inference_result = extraction_data['inference_result']\n",
    "                    print(\"Extracted Data:\")\n",
    "                    for attr_name, attr_value in inference_result.items():\n",
    "                        if attr_value is not None:\n",
    "                            # Truncate long values for display\n",
    "                            display_value = str(attr_value)[:1000] + \"...\" if len(str(attr_value)) > 1000 else attr_value\n",
    "                            print(f\"  {attr_name}: {display_value}\")\n",
    "                        else:\n",
    "                            print(f\"  {attr_name}: null\")\n",
    "                else:\n",
    "                    print(\"No inference results found\")\n",
    "                    \n",
    "                # Display metadata if available\n",
    "                if 'metadata' in extraction_data:\n",
    "                    metadata = extraction_data['metadata']\n",
    "                    print(f\"Processing time: {metadata.get('extraction_time_seconds', 'N/A')} seconds\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading extraction results: {e}\")\n",
    "        else:\n",
    "            print(\"No extraction results available\")\n",
    "else:\n",
    "    print(\"No sections to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save Results for Next Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory for this step\n",
    "data_dir = Path(\".data/step3_extraction\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save updated document object as JSON\n",
    "document_path = data_dir / \"document.json\"\n",
    "with open(document_path, 'w') as f:\n",
    "    f.write(document.to_json())\n",
    "\n",
    "# Save configuration (pass through)\n",
    "config_path = data_dir / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "# Save environment info (pass through)\n",
    "env_path = data_dir / \"environment.json\"\n",
    "with open(env_path, 'w') as f:\n",
    "    json.dump(env_info, f, indent=2)\n",
    "\n",
    "# Save extraction-specific results summary\n",
    "extraction_summary = {\n",
    "    'model_used': extraction_config.get('model'),\n",
    "    'sections_processed': len(extraction_results) if 'extraction_results' in locals() else 0,\n",
    "    'total_sections': len(document.sections) if document.sections else 0,\n",
    "    'section_results': extraction_results if 'extraction_results' in locals() else [],\n",
    "    'sections_with_extractions': [\n",
    "        {\n",
    "            'section_id': section.section_id,\n",
    "            'classification': section.classification,\n",
    "            'extraction_result_uri': getattr(section, 'extraction_result_uri', None),\n",
    "            'has_results': hasattr(section, 'extraction_result_uri') and section.extraction_result_uri is not None\n",
    "        } for section in (document.sections or [])\n",
    "    ]\n",
    "}\n",
    "\n",
    "extraction_summary_path = data_dir / \"extraction_summary.json\"\n",
    "with open(extraction_summary_path, 'w') as f:\n",
    "    json.dump(extraction_summary, f, indent=2)\n",
    "\n",
    "print(f\"Saved document to: {document_path}\")\n",
    "print(f\"Saved configuration to: {config_path}\")\n",
    "print(f\"Saved environment info to: {env_path}\")\n",
    "print(f\"Saved extraction summary to: {extraction_summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_processed = len(extraction_results) if 'extraction_results' in locals() else 0\n",
    "sections_with_results = sum(1 for section in (document.sections or []) if hasattr(section, 'extraction_result_uri') and section.extraction_result_uri)\n",
    "\n",
    "print(\"=== Step 3: Extraction Complete ===\")\n",
    "print(f\"‚úÖ Document processed: {document.id}\")\n",
    "print(f\"‚úÖ Sections processed: {sections_processed} of {len(document.sections) if document.sections else 0}\")\n",
    "print(f\"‚úÖ Sections with results: {sections_with_results}\")\n",
    "print(f\"‚úÖ Model used: {extraction_config.get('model')}\")\n",
    "print(\"‚úÖ Data saved to: .data/step3_extraction/\")\n",
    "print(\"\\nüìå Next step: Run step4_assessment.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Comparison: Same Document, Both Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract from the same section using both methods for direct comparison\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "if document.sections and len(document.sections) > 0:\n",
    "    test_section = document.sections[0]  # Use first section\n",
    "    \n",
    "    print(f\"üìã Testing with Section: {test_section.classification}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Track metrics\n",
    "    comparison_results = {}\n",
    "    \n",
    "    # Method 1: Traditional Extraction\n",
    "    print(\"\\nüî¥ METHOD 1: TRADITIONAL EXTRACTION\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    CONFIG_TRAD = CONFIG.copy()\n",
    "    CONFIG_TRAD[\"extraction\"][\"agentic\"] = {\"enabled\": False}\n",
    "    \n",
    "    extraction_service_traditional = extraction.ExtractionService(config=CONFIG_TRAD)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        document_trad = extraction_service_traditional.process_document_section(\n",
    "            document=deepcopy(document),\n",
    "            section_id=test_section.section_id\n",
    "        )\n",
    "        trad_time = time.time() - start_time\n",
    "        \n",
    "        # Load results\n",
    "        trad_section = document_trad.sections[0]\n",
    "        if trad_section.extraction_result_uri:\n",
    "            trad_data = load_json_from_s3(trad_section.extraction_result_uri)\n",
    "            trad_result = trad_data.get('inference_result', {})\n",
    "            \n",
    "            comparison_results['traditional'] = {\n",
    "                'time': trad_time,\n",
    "                'fields_extracted': len([k for k, v in trad_result.items() if v is not None]),\n",
    "                'total_fields': len(trad_result),\n",
    "                'has_nested': any(isinstance(v, dict) for v in trad_result.values()),\n",
    "                'has_arrays': any(isinstance(v, list) for v in trad_result.values())\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Completed in {trad_time:.2f} seconds\")\n",
    "            print(f\"   Fields: {comparison_results['traditional']['fields_extracted']}/{comparison_results['traditional']['total_fields']}\")\n",
    "            print(f\"   Complex structures: Nested={comparison_results['traditional']['has_nested']}, Arrays={comparison_results['traditional']['has_arrays']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "        comparison_results['traditional'] = {'error': str(e)}\n",
    "    \n",
    "    # Method 2: Agentic Extraction\n",
    "    print(\"\\nüü¢ METHOD 2: AGENTIC EXTRACTION\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    CONFIG_AGENT = CONFIG.copy()\n",
    "    CONFIG_AGENT[\"extraction\"][\"agentic\"] = {\"enabled\": True}\n",
    "    \n",
    "    extraction_service_agentic = extraction.ExtractionService(config=CONFIG_AGENT)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        document_agent = extraction_service_agentic.process_document_section(\n",
    "            document=deepcopy(document),\n",
    "            section_id=test_section.section_id\n",
    "        )\n",
    "        agent_time = time.time() - start_time\n",
    "        \n",
    "        # Load results\n",
    "        agent_section = document_agent.sections[0]\n",
    "        if agent_section.extraction_result_uri:\n",
    "            agent_data = load_json_from_s3(agent_section.extraction_result_uri)\n",
    "            agent_result = agent_data.get('inference_result', {})\n",
    "            \n",
    "            comparison_results['agentic'] = {\n",
    "                'time': agent_time,\n",
    "                'fields_extracted': len([k for k, v in agent_result.items() if v is not None]),\n",
    "                'total_fields': len(agent_result),\n",
    "                'has_nested': any(isinstance(v, dict) for v in agent_result.values()),\n",
    "                'has_arrays': any(isinstance(v, list) for v in agent_result.values())\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Completed in {agent_time:.2f} seconds\")\n",
    "            print(f\"   Fields: {comparison_results['agentic']['fields_extracted']}/{comparison_results['agentic']['total_fields']}\")\n",
    "            print(f\"   Complex structures: Nested={comparison_results['agentic']['has_nested']}, Arrays={comparison_results['agentic']['has_arrays']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "        comparison_results['agentic'] = {'error': str(e)}\n",
    "    \n",
    "    # Comparison Summary\n",
    "    print(\"\\nüìä COMPARISON SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'traditional' in comparison_results and 'agentic' in comparison_results:\n",
    "        if 'error' not in comparison_results['traditional'] and 'error' not in comparison_results['agentic']:\n",
    "            trad = comparison_results['traditional']\n",
    "            agent = comparison_results['agentic']\n",
    "            \n",
    "            speed_diff = ((trad['time'] - agent['time']) / trad['time']) * 100\n",
    "            field_diff = agent['fields_extracted'] - trad['fields_extracted']\n",
    "            \n",
    "            print(f\"‚è±Ô∏è  Speed: Agentic is {speed_diff:.1f}% {'faster' if speed_diff > 0 else 'slower'}\")\n",
    "            print(f\"    Traditional: {trad['time']:.2f}s | Agentic: {agent['time']:.2f}s\")\n",
    "            print()\n",
    "            print(f\"üìù Field Extraction: Agentic extracted {field_diff:+d} more fields\")\n",
    "            print(f\"    Traditional: {trad['fields_extracted']}/{trad['total_fields']} | Agentic: {agent['fields_extracted']}/{agent['total_fields']}\")\n",
    "            print()\n",
    "            print(f\"üèóÔ∏è  Structure Handling:\")\n",
    "            print(f\"    Nested Objects: Traditional={trad['has_nested']} | Agentic={agent['has_nested']}\")\n",
    "            print(f\"    Arrays: Traditional={trad['has_arrays']} | Agentic={agent['has_arrays']}\")\n",
    "            \n",
    "            print(\"\\n‚ú® KEY ADVANTAGES OF AGENTIC EXTRACTION:\")\n",
    "            advantages = []\n",
    "            if speed_diff > 10:\n",
    "                advantages.append(f\"‚Ä¢ {speed_diff:.0f}% faster processing\")\n",
    "            if field_diff > 0:\n",
    "                advantages.append(f\"‚Ä¢ Better field coverage (+{field_diff} fields)\")\n",
    "            if agent['has_nested'] or agent['has_arrays']:\n",
    "                advantages.append(\"‚Ä¢ Handles complex nested structures\")\n",
    "            advantages.append(\"‚Ä¢ Self-correcting with schema validation\")\n",
    "            advantages.append(\"‚Ä¢ Consistent output format guaranteed\")\n",
    "            \n",
    "            for adv in advantages:\n",
    "                print(adv)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Comparison could not be completed due to errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
