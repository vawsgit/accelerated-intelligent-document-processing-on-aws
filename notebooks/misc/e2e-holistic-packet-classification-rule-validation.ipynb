{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holistic Packet Classification with Extraction and Rule Validation\n",
    "\n",
    "This notebook demonstrates how to use the holistic packet classification capability of the IDP Common Package to classify multi-document packets, where each document might span multiple pages. The holistic approach examines the document as a whole to identify boundaries between different document types within the packet.\n",
    "\n",
    "**Key Benefits of Holistic Packet Classification:**\n",
    "1. Properly handles multi-page documents within a packet\n",
    "2. Detects logical document boundaries\n",
    "3. Identifies document types in context of the whole document\n",
    "4. Handles documents where individual pages may not be clearly classifiable on their own\n",
    "\n",
    "The notebook demonstrates how to process a document with:\n",
    "\n",
    "1. OCR Service - Convert a PDF document to text using AWS Textract\n",
    "2. Classification Service - Classify document pages into sections using Bedrock using the multi-model page based method.\n",
    "3. Extraction Service - Extract structured information from classified sections using Bedrock\n",
    "4. Rule Validation Service - Validate documents against business rules using LLMs with extraction results as context\n",
    "5. Evaluation Service - Evaluate accuracy of extracted information\n",
    "\n",
    "Each step uses the unified Document object model for data flow and consistency.\n",
    "\n",
    "> **Note**: This notebook uses AWS services including S3, Textract, and Bedrock. You need valid AWS credentials with appropriate permissions to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "The IDP common package supports granular installation through extras. You can install:\n",
    "- `[core]` - Just core functionality \n",
    "- `[ocr]` - OCR service with Textract dependencies\n",
    "- `[classification]` - Classification service dependencies\n",
    "- `[extraction]` - Extraction service dependencies\n",
    "- `[evaluation]` - Evaluation service dependencies\n",
    "- `[all]` - All of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure that modules are autoreloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "ROOTDIR=\"../..\"\n",
    "# First uninstall existing package (to ensure we get the latest version)\n",
    "%pip uninstall -y idp_common\n",
    "\n",
    "# Install the IDP common package with all components in development mode\n",
    "%pip install -q -e \"{ROOTDIR}/lib/idp_common_pkg[dev, all]\"\n",
    "\n",
    "# Note: We can also install specific components like:\n",
    "# %pip install -q -e \"{ROOTDIR}/lib/idp_common_pkg[ocr,classification,extraction,evaluation]\"\n",
    "\n",
    "# Check installed version\n",
    "%pip show idp_common | grep -E \"Version|Location\"\n",
    "\n",
    "# Optionally use a .env file for environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  \n",
    "except ImportError:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "# Import base libraries\n",
    "from idp_common.models import Document, Status, Section, Page\n",
    "from idp_common import ocr, classification, extraction, evaluation, summarization\n",
    "from idp_common import s3\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging \n",
    "logging.basicConfig(level=logging.WARNING)  # Set root logger to WARNING (less verbose)\n",
    "logging.getLogger('idp_common.ocr.service').setLevel(logging.INFO)  # Focus on service logs\n",
    "logging.getLogger('textractor').setLevel(logging.WARNING)  # Suppress textractor logs\n",
    "logging.getLogger('idp_common.evaluation.service').setLevel(logging.INFO)  # Enable evaluation logs\n",
    "logging.getLogger('idp_common.bedrock.client').setLevel(logging.INFO)  # show prompts\n",
    "logging.getLogger('idp_common.rule_validation.service').setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['METRIC_NAMESPACE'] = 'IDP-Notebook-Example'\n",
    "os.environ['AWS_REGION'] = boto3.session.Session().region_name or 'us-east-1'\n",
    "\n",
    "# Get AWS account ID for unique bucket names\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = os.environ['AWS_REGION']\n",
    "\n",
    "# Define sample PDF path \n",
    "SAMPLE_PDF_PATH = f\"{ROOTDIR}/samples/rule-validation/respiratory_pa_packet.pdf\"\n",
    "\n",
    "# Create unique bucket names based on account ID and region\n",
    "input_bucket_name =  os.getenv(\"IDP_INPUT_BUCKET_NAME\", f\"idp-notebook-input-{account_id}-{region}\")\n",
    "output_bucket_name = os.getenv(\"IDP_OUTPUT_BUCKET_NAME\", f\"idp-notebook-output-{account_id}-{region}\")\n",
    "\n",
    "# Helper function to parse S3 URIs\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "# Helper function to load JSON from S3\n",
    "def load_json_from_s3(uri):\n",
    "    bucket, key = parse_s3_uri(uri)\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    return json.loads(content)\n",
    "\n",
    "print(\"Environment setup:\")\n",
    "print(f\"METRIC_NAMESPACE: {os.environ.get('METRIC_NAMESPACE')}\")\n",
    "print(f\"AWS_REGION: {os.environ.get('AWS_REGION')}\")\n",
    "print(f\"Input bucket: {input_bucket_name}\")\n",
    "print(f\"Output bucket: {output_bucket_name}\")\n",
    "print(f\"SAMPLE_PDF_PATH: {SAMPLE_PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up S3 Buckets and Upload Sample File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Function to create a bucket if it doesn't exist\n",
    "def ensure_bucket_exists(bucket_name):\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            if region == 'us-east-1':\n",
    "                s3_client.create_bucket(Bucket=bucket_name)\n",
    "            else:\n",
    "                s3_client.create_bucket(\n",
    "                    Bucket=bucket_name,\n",
    "                    CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                )\n",
    "            print(f\"Created bucket: {bucket_name}\")\n",
    "            \n",
    "            # Wait for bucket to be accessible\n",
    "            waiter = s3_client.get_waiter('bucket_exists')\n",
    "            waiter.wait(Bucket=bucket_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating bucket {bucket_name}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Ensure both buckets exist\n",
    "ensure_bucket_exists(input_bucket_name)\n",
    "ensure_bucket_exists(output_bucket_name)\n",
    "\n",
    "# Upload the sample file to S3\n",
    "sample_file_key = \"sample-\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".pdf\"\n",
    "with open(SAMPLE_PDF_PATH, 'rb') as file_data:\n",
    "    s3_client.upload_fileobj(file_data, input_bucket_name, sample_file_key)\n",
    "\n",
    "print(f\"Uploaded sample file to: s3://{input_bucket_name}/{sample_file_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(f\"{ROOTDIR}/config_library/pattern-2/rule-validation/config.yaml\", 'r') as file:\n",
    "    # \"../../config_library/pattern-2/rule-validation/config.yaml\"\n",
    "    # genaiic-idp-accelerator/config_library/pattern-2/rvl-cdip-package-sample/config.yaml\n",
    "    CONFIG = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Document with OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new Document\n",
    "document = Document(\n",
    "    id=\"respiratory_pa_packet\",\n",
    "    input_bucket=input_bucket_name,\n",
    "    input_key=sample_file_key,\n",
    "    output_bucket=output_bucket_name,\n",
    "    status=Status.QUEUED\n",
    ")\n",
    "\n",
    "print(f\"Created document with ID: {document.id}\")\n",
    "print(f\"Status: {document.status.value}\")\n",
    "\n",
    "# Create OCR service with Textract\n",
    "# Valid features are 'LAYOUT', 'FORMS', 'SIGNATURES', 'TABLES' (uses analyze_document API)\n",
    "# or leave it empty (to use basic detect_document_text API)\n",
    "ocr_service = ocr.OcrService(\n",
    "    region=region,\n",
    "    enhanced_features=['LAYOUT']\n",
    ")\n",
    "\n",
    "# Process document with OCR\n",
    "print(\"\\nProcessing document with OCR...\")\n",
    "start_time = time.time()\n",
    "document = ocr_service.process_document(document)\n",
    "ocr_time = time.time() - start_time\n",
    "\n",
    "print(f\"OCR processing completed in {ocr_time:.2f} seconds\")\n",
    "print(f\"Document status: {document.status.value}\")\n",
    "print(f\"Number of pages processed: {document.num_pages}\")\n",
    "\n",
    "# Show pages information\n",
    "print(\"\\nProcessed pages:\")\n",
    "for page_id, page in document.pages.items():\n",
    "    print(f\"Page {page_id}:\")\n",
    "    print(f\"  Image URI: {page.image_uri}\")\n",
    "    print(f\"  Raw Text URI: {page.raw_text_uri}\")\n",
    "    print(f\"  Parsed Text URI: {page.parsed_text_uri}\")\n",
    "print(\"\\nMetering:\")\n",
    "print(json.dumps(document.metering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classify the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that Config specifies => \"classificationMethod\": \"textbasedHolisticClassification\"\n",
    "print(\"*****************************************************************\")\n",
    "print(f'CONFIG classificationMethod: {CONFIG[\"classification\"].get(\"classificationMethod\")}')\n",
    "print(\"*****************************************************************\")\n",
    "\n",
    "# Create classification service with Bedrock backend\n",
    "# The classification method is set in the config\n",
    "classification_service = classification.ClassificationService(\n",
    "    config=CONFIG, \n",
    "    backend=\"bedrock\" \n",
    ")\n",
    "\n",
    "# Classify the document\n",
    "print(\"\\nClassifying document...\")\n",
    "start_time = time.time()\n",
    "document = classification_service.classify_document(document)\n",
    "classification_time = time.time() - start_time\n",
    "print(f\"Classification completed in {classification_time:.2f} seconds\")\n",
    "print(f\"Document status: {document.status.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if document.sections:\n",
    "    print(\"\\nDetected sections:\")\n",
    "    for section in document.sections:\n",
    "        print(f\"Section {section.section_id}: {section.classification}\")\n",
    "        print(f\"  Pages: {section.page_ids}\")\n",
    "else:\n",
    "    print(\"\\nNo sections detected\")\n",
    "\n",
    "# Show page classification\n",
    "print(\"\\nPage-level classifications:\")\n",
    "for page_id, page in sorted(document.pages.items()):\n",
    "    print(f\"Page {page_id}: {page.classification}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extract Information from Classified Sections\n",
    "\n",
    "Now that we have classified the document into sections, we'll extract structured information from each section using the extraction service. This step is crucial for rule validation as it provides structured data that can be referenced during the validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extraction service\n",
    "extraction_service = extraction.ExtractionService(config=CONFIG)\n",
    "\n",
    "# Extract information from each section\n",
    "print(\"\\nExtracting information from classified sections...\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not document.sections:\n",
    "    print(\"No sections found in document. Cannot proceed with extraction.\")\n",
    "else:\n",
    "    print(f\"Processing {len(document.sections)} sections...\")\n",
    "    \n",
    "    for i, section in enumerate(document.sections):\n",
    "        print(f\"\\n--- Processing Section {i+1}/{len(document.sections)} ---\")\n",
    "        print(f\"Section ID: {section.section_id}\")\n",
    "        print(f\"Classification: {section.classification}\")\n",
    "        print(f\"Pages: {section.page_ids}\")\n",
    "        \n",
    "        # Process section extraction\n",
    "        section_start_time = time.time()\n",
    "        document = extraction_service.process_document_section(\n",
    "            document=document,\n",
    "            section_id=section.section_id\n",
    "        )\n",
    "        section_time = time.time() - section_start_time\n",
    "        print(f\"Section extraction completed in {section_time:.2f} seconds\")\n",
    "\n",
    "extraction_time = time.time() - start_time\n",
    "print(f\"\\nTotal extraction completed in {extraction_time:.2f} seconds\")\n",
    "print(f\"Document status: {document.status.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show extraction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExtraction results by section:\")\n",
    "for section in document.sections:\n",
    "    print(f\"\\nSection {section.section_id} ({section.classification}):\")\n",
    "    if section.extraction_result_uri:\n",
    "        try:\n",
    "            extraction_data = s3.get_json_content(section.extraction_result_uri)\n",
    "            extraction_results = extraction_data.get('inference_result', {})\n",
    "            print(f\"  Extracted {len(extraction_results)} fields\")\n",
    "            # Show first few fields as preview\n",
    "            for i, (key, value) in enumerate(extraction_results.items()):\n",
    "                if i < 3:  # Show first 3 fields\n",
    "                    print(f\"    {key}: {str(value)[:100]}{'...' if len(str(value)) > 100 else ''}\")\n",
    "                elif i == 3:\n",
    "                    print(f\"    ... and {len(extraction_results) - 3} more fields\")\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading extraction results: {e}\")\n",
    "    else:\n",
    "        print(\"  No extraction results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Rule-Validation Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from idp_common.rule_validation import RuleValidationService\n",
    "\n",
    "\n",
    "# Load configuration from YAML file\n",
    "config_path = \"../../config_library/pattern-2/rule-validation/config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "print(f\"✅ Loaded configuration from: {config_path}\")\n",
    "print(f\"Fact Extraction Model: {config_data['rule_validation']['fact_extraction']['model']}\")\n",
    "\n",
    "rule_validation_service = RuleValidationService(\n",
    "            region=region,\n",
    "            config=config_data\n",
    "        )       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each section individually (following new architecture)\n",
    "section_results = []\n",
    "\n",
    "for section in document.sections:\n",
    "    print(f\"Processing section {section.section_id} (classification: {section.classification})\")\n",
    "    \n",
    "    # Create a document with only this section\n",
    "    section_document = Document(\n",
    "            id=document.id,\n",
    "            input_key=document.input_key,\n",
    "            input_bucket=document.input_bucket,\n",
    "            output_bucket=document.output_bucket,\n",
    "            pages=document.pages,\n",
    "            sections=[section],  # Only include this section\n",
    "            status=document.status,\n",
    "            metering=document.metering.copy() if document.metering else {},  # Copy existing metering\n",
    "            rule_validation_result=document.rule_validation_result,  # Copy existing result\n",
    "            # Add other fields as needed\n",
    "        )\n",
    "    \n",
    "    # Create fresh service instance for each section (avoids asyncio semaphore issues in notebooks)\n",
    "    section_rule_validation_service = RuleValidationService(\n",
    "        region=region,\n",
    "        config=config_data\n",
    "    )\n",
    "    \n",
    "    # Process the single section\n",
    "    section_result = section_rule_validation_service.validate_document(section_document)\n",
    "    section_results.append(section_result)\n",
    "    \n",
    "    print(f\"✅ Completed section {section.section_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_results[0].rule_validation_result.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate results (this would normally load from S3 files)\n",
    "# For notebook demo, we'll show the individual section results\n",
    "print(\"Consolidating section results...\")\n",
    "\n",
    "for i, section_result in enumerate(section_results):\n",
    "    section_id = document.sections[i].section_id\n",
    "    if hasattr(section_result, 'rule_validation_result') and section_result.rule_validation_result:\n",
    "        rv_result = section_result.rule_validation_result\n",
    "        # The actual result is stored in the S3 URI\n",
    "        section_uri = rv_result.metadata.get('section_output_uri')\n",
    "        if section_uri:\n",
    "            print(f\"Section {section_id}: Results saved to {section_uri}\")\n",
    "        else:\n",
    "            print(f\"Section {section_id}: Rule validation completed (check summary)\")\n",
    "    else:\n",
    "        print(f\"Section {section_id}: No rule validation results\")\n",
    "\n",
    "print(\"Section processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Orchestration service\n",
    "from idp_common.rule_validation import RuleValidationOrchestratorService\n",
    "\n",
    "orchestration_service = RuleValidationOrchestratorService(\n",
    "    config=config_data\n",
    ")\n",
    "\n",
    "print(\"Orchestration service created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = orchestration_service.consolidate_and_save(\n",
    "    document=document,\n",
    "    config=config_data,\n",
    "    multiple_sections=True\n",
    ")\n",
    "\n",
    "print(f\"Consolidated summary URI: {document.rule_validation_result.output_uri}\")\n",
    "print(f\"Rule type files: {document.rule_validation_result.summary.get('rule_type_uris', [])}\")\n",
    "print(f\"Sections processed: {document.rule_validation_result.metadata.get('sections_processed', 0)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-idp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
