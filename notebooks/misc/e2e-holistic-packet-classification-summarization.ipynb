{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holistic Packet Classification with IDP Common Package\n",
    "\n",
    "This notebook demonstrates how to use the holistic packet classification capability of the IDP Common Package to classify multi-document packets, where each document might span multiple pages. The holistic approach examines the document as a whole to identify boundaries between different document types within the packet.\n",
    "\n",
    "**Key Benefits of Holistic Packet Classification:**\n",
    "1. Properly handles multi-page documents within a packet\n",
    "2. Detects logical document boundaries\n",
    "3. Identifies document types in context of the whole document\n",
    "4. Handles documents where individual pages may not be clearly classifiable on their own\n",
    "\n",
    "The notebook demonstrates how to process a document with:\n",
    "\n",
    "1. OCR Service - Convert a PDF document to text using AWS Textract\n",
    "2. Classification Service - Classify document pages into sections using Bedrock using the multi-model page based method.\n",
    "3. Extraction Service - Extract structured information from sections using Bedrock\n",
    "4. Evaluation Service - Evaluate accuracy of extracted information\n",
    "\n",
    "Each step uses the unified Document object model for data flow and consistency.\n",
    "\n",
    "> **Note**: This notebook uses AWS services including S3, Textract, and Bedrock. You need valid AWS credentials with appropriate permissions to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "The IDP common package supports granular installation through extras. You can install:\n",
    "- `[core]` - Just core functionality \n",
    "- `[ocr]` - OCR service with Textract dependencies\n",
    "- `[classification]` - Classification service dependencies\n",
    "- `[extraction]` - Extraction service dependencies\n",
    "- `[evaluation]` - Evaluation service dependencies\n",
    "- `[all]` - All of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure that modules are autoreloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "ROOTDIR=\"../..\"\n",
    "# First uninstall existing package (to ensure we get the latest version)\n",
    "%pip uninstall -y idp_common\n",
    "\n",
    "# Install the IDP common package with all components in development mode\n",
    "%pip install -q -e \"{ROOTDIR}/lib/idp_common_pkg[dev, all]\"\n",
    "\n",
    "# Note: We can also install specific components like:\n",
    "# %pip install -q -e \"{ROOTDIR}/lib/idp_common_pkg[ocr,classification,extraction,evaluation]\"\n",
    "\n",
    "# Check installed version\n",
    "%pip show idp_common | grep -E \"Version|Location\"\n",
    "\n",
    "# Optionally use a .env file for environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  \n",
    "except ImportError:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "# Import base libraries\n",
    "from idp_common.models import Document, Status, Section, Page\n",
    "from idp_common import ocr, classification, extraction, evaluation, summarization\n",
    "from idp_common import s3\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging \n",
    "logging.basicConfig(level=logging.WARNING)  # Set root logger to WARNING (less verbose)\n",
    "logging.getLogger('idp_common.ocr.service').setLevel(logging.INFO)  # Focus on service logs\n",
    "logging.getLogger('textractor').setLevel(logging.WARNING)  # Suppress textractor logs\n",
    "logging.getLogger('idp_common.evaluation.service').setLevel(logging.INFO)  # Enable evaluation logs\n",
    "logging.getLogger('idp_common.bedrock.client').setLevel(logging.INFO)  # show prompts\n",
    "\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['METRIC_NAMESPACE'] = 'IDP-Notebook-Example'\n",
    "os.environ['AWS_REGION'] = boto3.session.Session().region_name or 'us-east-1'\n",
    "\n",
    "# Get AWS account ID for unique bucket names\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = os.environ['AWS_REGION']\n",
    "\n",
    "# Define sample PDF path \n",
    "SAMPLE_PDF_PATH = f\"{ROOTDIR}/samples/insurance_package.pdf\"\n",
    "\n",
    "# Create unique bucket names based on account ID and region\n",
    "input_bucket_name =  os.getenv(\"IDP_INPUT_BUCKET_NAME\", f\"idp-notebook-input-{account_id}-{region}\")\n",
    "output_bucket_name = os.getenv(\"IDP_OUTPUT_BUCKET_NAME\", f\"idp-notebook-output-{account_id}-{region}\")\n",
    "\n",
    "# Helper function to parse S3 URIs\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "# Helper function to load JSON from S3\n",
    "def load_json_from_s3(uri):\n",
    "    bucket, key = parse_s3_uri(uri)\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    return json.loads(content)\n",
    "\n",
    "print(\"Environment setup:\")\n",
    "print(f\"METRIC_NAMESPACE: {os.environ.get('METRIC_NAMESPACE')}\")\n",
    "print(f\"AWS_REGION: {os.environ.get('AWS_REGION')}\")\n",
    "print(f\"Input bucket: {input_bucket_name}\")\n",
    "print(f\"Output bucket: {output_bucket_name}\")\n",
    "print(f\"SAMPLE_PDF_PATH: {SAMPLE_PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up S3 Buckets and Upload Sample File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Function to create a bucket if it doesn't exist\n",
    "def ensure_bucket_exists(bucket_name):\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            if region == 'us-east-1':\n",
    "                s3_client.create_bucket(Bucket=bucket_name)\n",
    "            else:\n",
    "                s3_client.create_bucket(\n",
    "                    Bucket=bucket_name,\n",
    "                    CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                )\n",
    "            print(f\"Created bucket: {bucket_name}\")\n",
    "            \n",
    "            # Wait for bucket to be accessible\n",
    "            waiter = s3_client.get_waiter('bucket_exists')\n",
    "            waiter.wait(Bucket=bucket_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating bucket {bucket_name}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Ensure both buckets exist\n",
    "ensure_bucket_exists(input_bucket_name)\n",
    "ensure_bucket_exists(output_bucket_name)\n",
    "\n",
    "# Upload the sample file to S3\n",
    "sample_file_key = \"sample-\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".pdf\"\n",
    "with open(SAMPLE_PDF_PATH, 'rb') as file_data:\n",
    "    s3_client.upload_fileobj(file_data, input_bucket_name, sample_file_key)\n",
    "\n",
    "print(f\"Uploaded sample file to: s3://{input_bucket_name}/{sample_file_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(f\"{ROOTDIR}/config_library/pattern-2/rvl-cdip-package-sample/config.yaml\", 'r') as file:\n",
    "    # \"../../config_library/pattern-2/criteria-validation/config.yaml\"\n",
    "    # genaiic-idp-accelerator/config_library/pattern-2/rvl-cdip-package-sample/config.yaml\n",
    "    CONFIG = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Document with OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new Document\n",
    "document = Document(\n",
    "    id=\"rvl-cdip-package\",\n",
    "    input_bucket=input_bucket_name,\n",
    "    input_key=sample_file_key,\n",
    "    output_bucket=output_bucket_name,\n",
    "    status=Status.QUEUED\n",
    ")\n",
    "\n",
    "print(f\"Created document with ID: {document.id}\")\n",
    "print(f\"Status: {document.status.value}\")\n",
    "\n",
    "# Create OCR service with Textract\n",
    "# Valid features are 'LAYOUT', 'FORMS', 'SIGNATURES', 'TABLES' (uses analyze_document API)\n",
    "# or leave it empty (to use basic detect_document_text API)\n",
    "ocr_service = ocr.OcrService(\n",
    "    region=region,\n",
    "    enhanced_features=['LAYOUT']\n",
    ")\n",
    "\n",
    "# Process document with OCR\n",
    "print(\"\\nProcessing document with OCR...\")\n",
    "start_time = time.time()\n",
    "document = ocr_service.process_document(document)\n",
    "ocr_time = time.time() - start_time\n",
    "\n",
    "print(f\"OCR processing completed in {ocr_time:.2f} seconds\")\n",
    "print(f\"Document status: {document.status.value}\")\n",
    "print(f\"Number of pages processed: {document.num_pages}\")\n",
    "\n",
    "# Show pages information\n",
    "print(\"\\nProcessed pages:\")\n",
    "for page_id, page in document.pages.items():\n",
    "    print(f\"Page {page_id}:\")\n",
    "    print(f\"  Image URI: {page.image_uri}\")\n",
    "    print(f\"  Raw Text URI: {page.raw_text_uri}\")\n",
    "    print(f\"  Parsed Text URI: {page.parsed_text_uri}\")\n",
    "print(\"\\nMetering:\")\n",
    "print(json.dumps(document.metering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classify the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that Config specifies => \"classificationMethod\": \"textbasedHolisticClassification\"\n",
    "print(\"*****************************************************************\")\n",
    "print(f'CONFIG classificationMethod: {CONFIG[\"classification\"].get(\"classificationMethod\")}')\n",
    "print(\"*****************************************************************\")\n",
    "\n",
    "# Create classification service with Bedrock backend\n",
    "# The classification method is set in the config\n",
    "classification_service = classification.ClassificationService(\n",
    "    config=CONFIG, \n",
    "    backend=\"bedrock\" \n",
    ")\n",
    "\n",
    "# Classify the document\n",
    "print(\"\\nClassifying document...\")\n",
    "start_time = time.time()\n",
    "document = classification_service.classify_document(document)\n",
    "classification_time = time.time() - start_time\n",
    "print(f\"Classification completed in {classification_time:.2f} seconds\")\n",
    "print(f\"Document status: {document.status.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if document.sections:\n",
    "    print(\"\\nDetected sections:\")\n",
    "    for section in document.sections:\n",
    "        print(f\"Section {section.section_id}: {section.classification}\")\n",
    "        print(f\"  Pages: {section.page_ids}\")\n",
    "else:\n",
    "    print(\"\\nNo sections detected\")\n",
    "\n",
    "# Show page classification\n",
    "print(\"\\nPage-level classifications:\")\n",
    "for page_id, page in sorted(document.pages.items()):\n",
    "    print(f\"Page {page_id}: {page.classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior-Auth Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from idp_common.criteria_validation import CriteriaValidationService, CriteriaValidationResult\n",
    "\n",
    "\n",
    "# Load configuration from YAML file\n",
    "config_path = \"../../config_library/pattern-2/criteria-validation/config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "print(f\"âœ… Loaded configuration from: {config_path}\")\n",
    "print(f\"Available criteria types: {config_data.get('criteria_types', [])}\")\n",
    "print(f\"Model: {config_data['criteria_validation']['model']}\")    \n",
    "\n",
    "criteria_validation_service = CriteriaValidationService(\n",
    "            region=region,\n",
    "            config=config_data\n",
    "        )       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = criteria_validation_service.validate_document(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.summary_report_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Processing Individual Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_service = summarization.SummarizationService(config=CONFIG)\n",
    "\n",
    "print(\"=== PART 1: Processing Individual Sections ===\")\n",
    "n = 3  # Only process first 3 sections to save time\n",
    "# Process each section directly using the section_id\n",
    "for section in document.sections[:n]:  \n",
    "    print(f\"\\nProcessing section {section.section_id} (class: {section.classification})\")\n",
    "    \n",
    "    # Process section directly with the original document\n",
    "    start_time = time.time()\n",
    "    document, section_metering = summarization_service.process_document_section(\n",
    "        document=document,\n",
    "        section_id=section.section_id\n",
    "    )\n",
    "    summarization_time = time.time() - start_time\n",
    "    print(f\"Summarization for section {section.section_id} completed in {summarization_time:.2f} seconds\")\n",
    "    \n",
    "    # Print the summary content if available\n",
    "    if section.attributes and 'summary_uri' in section.attributes:\n",
    "        summary_uri = section.attributes['summary_uri']\n",
    "        summary_md_uri = section.attributes.get('summary_md_uri')\n",
    "        \n",
    "        print(f\"\\nJSON Summary URI: {summary_uri}\")\n",
    "        if summary_md_uri:\n",
    "            print(f\"Markdown Summary URI: {summary_md_uri}\")\n",
    "        \n",
    "        # Get and display JSON summary\n",
    "        try:\n",
    "            # Get the JSON summary content from S3\n",
    "            summary_content = s3.get_json_content(summary_uri)\n",
    "            print(\"\\nJSON Summary Content:\")\n",
    "            \n",
    "            # Check if there's a specific summary field in the content\n",
    "            if isinstance(summary_content, dict):\n",
    "                if 'summary' in summary_content:\n",
    "                    print(\"Summary field found in JSON:\")\n",
    "                    print(summary_content['summary'][:300] + \"...\" if len(summary_content['summary']) > 300 else summary_content['summary'])\n",
    "                elif 'content' in summary_content:\n",
    "                    print(\"Content field found in JSON:\")\n",
    "                    print(summary_content['content'][:300] + \"...\" if len(str(summary_content['content'])) > 300 else summary_content['content'])\n",
    "                else:\n",
    "                    # Print the whole content if no specific summary field\n",
    "                    print(\"Full JSON content (truncated):\")\n",
    "                    print(json.dumps(summary_content, indent=2)[:300] + \"...\" if len(json.dumps(summary_content)) > 300 else json.dumps(summary_content, indent=2))\n",
    "            else:\n",
    "                print(summary_content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving JSON summary: {e}\")\n",
    "            \n",
    "        # Get and display Markdown summary if available\n",
    "        if summary_md_uri:\n",
    "            try:\n",
    "                # Get the markdown summary content from S3\n",
    "                markdown_content = s3.get_text_content(summary_md_uri)\n",
    "                print(\"\\nMarkdown Summary Content (first 300 chars):\")\n",
    "                print(markdown_content[:300] + \"...\" if len(markdown_content) > 300 else markdown_content)\n",
    "                \n",
    "                # Display the rendered markdown\n",
    "                from IPython.display import Markdown, display\n",
    "                print(\"\\nRendered Markdown Summary:\")\n",
    "                display(Markdown(markdown_content))\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving markdown summary: {e}\")\n",
    "    else:\n",
    "        print(\"No summary available for this section\")\n",
    "    \n",
    "print(f\"\\nSummarization for first {n} sections complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Processing Document with Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_with_sections = copy.deepcopy(document)\n",
    "\n",
    "# Process the entire document using the section-based approach\n",
    "start_time = time.time()\n",
    "document_with_sections = summarization_service.process_document(\n",
    "    document=document_with_sections,\n",
    "    store_results=True\n",
    ")\n",
    "summarization_time = time.time() - start_time\n",
    "print(f\"Document summarization with sections completed in {summarization_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the combined summary report URI\n",
    "if document_with_sections.summary_report_uri:\n",
    "    print(f\"\\nCombined Summary Report URI: {document_with_sections.summary_report_uri}\")\n",
    "    \n",
    "    # Try to get and display the markdown summary\n",
    "    try:\n",
    "        # Extract bucket and key from the s3 URI\n",
    "        uri_parts = document_with_sections.summary_report_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "        bucket = uri_parts[0]\n",
    "        key = uri_parts[1]\n",
    "        \n",
    "        # Use boto3 to get the object directly\n",
    "        s3_client = boto3.client('s3')\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "        markdown_content = response['Body'].read().decode('utf-8')\n",
    "        \n",
    "        # Display a preview of the summary\n",
    "        print(\"\\nSummary Preview (first 500 chars):\")\n",
    "        print(markdown_content[:500] + \"...\" if len(markdown_content) > 500 else markdown_content)\n",
    "        \n",
    "        # Display the full markdown summary in a rendered cell\n",
    "        from IPython.display import Markdown, display\n",
    "        print(\"\\nFull Rendered Summary:\")\n",
    "        display(Markdown(markdown_content))\n",
    "        \n",
    "        # Also check if JSON summary exists\n",
    "        json_key = key.replace(\"summary.md\", \"summary.json\")\n",
    "        try:\n",
    "            json_response = s3_client.get_object(Bucket=bucket, Key=json_key)\n",
    "            summary_json = json.loads(json_response['Body'].read().decode('utf-8'))\n",
    "            # print(\"\\nJSON Summary Structure:\")\n",
    "            # print(f\"Keys: {list(summary_json.keys())}\")\n",
    "            \n",
    "            # Check for section summaries\n",
    "            if 'metadata' in summary_json and 'section_summaries' in summary_json['metadata']:\n",
    "                print(f\"\\nSection Summaries: {list(summary_json['metadata']['section_summaries'].keys())}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Note: JSON summary not found or couldn't be parsed: {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving summary: {e}\")\n",
    "else:\n",
    "    print(\"No summary available\")\n",
    "\n",
    "# Check individual section summaries if available\n",
    "# if document_with_sections.sections:\n",
    "#     print(\"\\nIndividual Section Summaries:\")\n",
    "#     for section in document_with_sections.sections:\n",
    "#         if section.attributes and 'summary_md_uri' in section.attributes:\n",
    "#             # print(f\"Section {section.section_id} ({section.classification}) Summary: {section.attributes['summary_md_uri']}\")\n",
    "#             print(f\"{section.attributes['summary_md_uri']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Processing Document without Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the document without sections to demonstrate the fallback approach\n",
    "# document_without_sections = copy.deepcopy(document)\n",
    "# document_without_sections.sections = []  # Remove all sections\n",
    "\n",
    "# # Process the document without sections (should use the fallback approach)\n",
    "# start_time = time.time()\n",
    "# document_without_sections = summarization_service.process_document(\n",
    "#     document=document_without_sections,\n",
    "#     store_results=True\n",
    "# )\n",
    "# summarization_time = time.time() - start_time\n",
    "# print(f\"Document summarization without sections completed in {summarization_time:.2f} seconds\")\n",
    "\n",
    "# # Print the summary report URI\n",
    "# if document_without_sections.summary_report_uri:\n",
    "#     print(f\"\\nWhole Document Summary Report URI: {document_without_sections.summary_report_uri}\")\n",
    "    \n",
    "#     # Try to get and display the markdown summary\n",
    "#     try:\n",
    "#         # Extract bucket and key from the s3 URI\n",
    "#         uri_parts = document_without_sections.summary_report_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "#         bucket = uri_parts[0]\n",
    "#         key = uri_parts[1]\n",
    "        \n",
    "#         # Use boto3 to get the object directly\n",
    "#         s3_client = boto3.client('s3')\n",
    "#         response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "#         summary_md = response['Body'].read().decode('utf-8')\n",
    "        \n",
    "#         print(\"\\nWhole Document Summary (first 500 chars):\")\n",
    "#         print(summary_md[:500] + \"...\" if len(summary_md) > 500 else summary_md)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error retrieving whole document summary: {e}\")\n",
    "# else:\n",
    "#     print(\"No whole document summary available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"# CURRICULUM VITAE \\n\\nSURNAME: Kalina BIRTHDATE: January 21, 1938 \\n\\nFIRST NAME: Moshe \\n\\nEDUCATION BACKGROUND \\n\\n\\n\\nFrom-To\\tInstitution\\tArea of specialization\\tDegree\\n1958-1961\\tHebrew University of Jerusalem\\tAgriculture\\tB.Sc\\n1961-1964\\tHebrew University of Jerusalem\\tBiochemistry\\tM.Sc.\\n1964-1967\\tUniv. of London, King's College\\tCytochemistry\\tPh D.\\n\\n\\n\\nMajor research interest: The surfactant system: cell biological approach \\n\\n## EMPLOYMENT (start with present position) \\n\\n\\n\\nFrom-To\\tInstitution\\tResearch area\\tTitle\\n1978-present\\tDept. of Histology, Tel Aviv University\\tThe surfactant system\\tAssoc Prof.\\n1972-1978\\tDept. of Histology & Cell Biology, in\\tCytotoxic lymphocyte\\tSen Lectur.\\n1967-1978\\tDept. of Histology&Cell Biology, 11\\tHistochemistry\\tLecturer\\n1990-1991\\tNational Jewish Hospital, Denver\\tThe surfactant system\\tVisit. Ass Pro\\n1984-1985\\tPostgraduate Medical School, London\\tImmunocytochem-endocrineVist Ass. Prof\\nsystem\\n1976-1977\\tDept. of Anatomy, UCLA\\tThe surfactant system\\tVist Ass Prof\\n1972-1973\\tJohns Hopkins University\\tE.M. cytochemistry\\tVist.Ass. Prof\\nList grants and contracts on related or other subjects currently received by investigator from BSF and other sources.\\nFrom-To\\tTitle of project\\tSources\\t% Time/\\tproject\\tTotal grant\\n\\n\\n\\n50601911\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (idpacc)",
   "language": "python",
   "name": "idpacc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
